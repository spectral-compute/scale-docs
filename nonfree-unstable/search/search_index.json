{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SCALE by Spectral Compute","text":""},{"location":"#what-is-scale","title":"What is SCALE?","text":"<p>SCALE is a GPGPU programming toolkit that allows CUDA applications to be natively compiled for AMD GPUs.</p> <p>SCALE does not require the CUDA program or its build system to be modified.</p> <p>Support for more GPU vendors and CUDA APIs is in development.</p> <p>To get started:</p> <ul> <li>See the tutorial.</li> <li>Review the examples.</li> <li>Check out the FAQ</li> <li>Contact us for help.</li> </ul>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>SCALE has several key innovations compared to other cross-platform GPGPU solutions:</p> <ul> <li>SCALE accepts CUDA programs as-is. No need to port them to another    language. This is true even if your program uses inline PTX <code>asm</code>.</li> <li>The SCALE compiler accepts the same command-line options and CUDA dialect   as <code>nvcc</code>, serving as a drop-in replacement.</li> <li>\"Impersonates\" an installation of the NVIDIA CUDA Toolkit, so existing    build tools and scripts like <code>cmake</code> just work.</li> </ul>"},{"location":"#what-projects-have-been-tested","title":"What projects have been tested?","text":"<p>We validate SCALE by compiling open-source CUDA projects and running their tests.</p> <p>The following open-source projects are currently part of our nightly automated  tests and pass fully:</p> Project Version Tested alien <code>v4.12</code> AMGX <code>v2.4.0</code> Blender Cycles <code>v4.2.0</code> faiss <code>v1.9.0</code> FLAMEGPU2 <code>345e0ae</code> GOMC <code>9fc85fb</code> GPUJPEG <code>3e045d1</code> gpu_jpeg2k <code>ee715e9</code> hashcat <code>6716447dfc</code> llama-cpp <code>b1500</code> NVIDIA Thrust <code>756c5af</code> stdgpu <code>2588168</code> xgboost <code>v2.1.0</code> <p>The scripts we use to build and test these projects (and others that do not  yet entirely work) are available on github. You can use these to reproduce our results (and find bugs!).</p>"},{"location":"#which-gpus-are-supported","title":"Which GPUs are supported?","text":"<p>The following GPU targets are supported, and are covered by our nightly tests:</p> <ul> <li>AMD <code>gfx900</code> (Vega 10, GCN 5.0)</li> <li>AMD <code>gfx1030</code> (Navi 21, RDNA 2.0)</li> <li>AMD <code>gfx1100</code> (Navi 31, RDNA 3.0)</li> </ul> <p>The following GPU targets have undergone ad-hoc manual testing and \"seem to work\":</p> <ul> <li>AMD <code>gfx1010</code></li> <li>AMD <code>gfx1101</code></li> <li>AMD <code>gfx1102</code></li> </ul> <p>Contact us if you want us to expedite support for a particular AMD GPU architecture.</p>"},{"location":"#what-are-the-components-of-scale","title":"What are the components of SCALE?","text":"<p>SCALE consists of:</p> <ul> <li>An <code>nvcc</code>-compatible compiler capable of compiling nvcc-dialect CUDA for AMD   GPUs, including PTX asm.</li> <li>Implementations of the CUDA runtime and driver APIs for AMD GPUs.</li> <li>Open-source wrapper libraries providing the \"CUDA-X\" APIs by delegating to the   corresponding ROCm libraries.   This is how libraries such as <code>cuBLAS</code> and <code>cuSOLVER</code> are handled.</li> </ul>"},{"location":"#what-are-the-differences-between-scale-and-other-solutions","title":"What are the differences between SCALE and other solutions?","text":"<p>Instead of providing a new way to write GPGPU  software, SCALE allows programs written using the widely-popular CUDA language to be directly compiled for AMD GPUs.</p> <p>SCALE aims to be fully compatible with NVIDIA CUDA. We believe that users  should not have to maintain multiple codebases or compromise on performance to support multiple GPU vendors.</p> <p>SCALE's language is a superset of NVIDIA CUDA, offering some opt-in language extensions that can make writing GPU code easier and more efficient for users who wish to move away from <code>nvcc</code>.</p> <p>SCALE is a work in progress. If there is a missing API that is blocking your attempt to use SCALE, please contact us so we can prioritise its development.</p>"},{"location":"#contact-us","title":"Contact us","text":"<p>There are multiple ways to get in touch with us:</p> <ul> <li>Join our Discord</li> <li>Send us an e-mail at hello@spectralcompute.co.uk</li> </ul>"},{"location":"licensing/","title":"Free Edition License","text":"<p>The Free Edition of SCALE has the following licensing terms:</p> Permissions Conditions Limitations  Commercial use  License and copyright notice  Liability  Distribution  No reverse engineering  Warranty  Modification  No endorsement  Private Use  Comply with 3rd party licenses <p>This table is intended as an overview, for detailed terms, please see the full license text below.</p>"},{"location":"licensing/#full-license-text","title":"Full License Text","text":"<p>SCALE Free Edition Public License v1</p> <p>Published: 10/7-2024</p> <p>Copyright (C) 2024 Spectral Compute Ltd</p> <p>License</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <p>Attribution</p> <ul> <li>Redistributions of source code must retain the above copyright notice, this list of conditions, and the following disclaimer.</li> <li>Redistributions in binary form must reproduce the above copyright notice, this list of conditions, and the following disclaimer in the documentation and/or other materials provided with the distribution.</li> <li>Neither the name of Spectral Compute nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</li> </ul> <p>Modifications</p> <ul> <li>Any modifications made to this software must be clearly documented, stating the nature of the changes, the date of the changes, and the identity of the person or entity making the changes.</li> </ul> <p>Prohibition on Reverse Engineering</p> <ul> <li>You may not reverse engineer, decompile, or disassemble the software, except and to only the extent that such activity is expressly permitted by applicable law notwithstanding this limitation.</li> </ul> <p>Third-Party Software</p> <ul> <li>The software may include third-party software components. You agree to comply with all applicable third-party terms and conditions, which will be included in the NOTICES file accompanying the software. The NOTICES file will also contain attributions for any third-party software components included in this software.</li> </ul> <p>Disclaimer of Warranty</p> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>Limitation of Liability</p> <p>IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF, OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Governing Law</p> <p>This License shall be governed by and construed in accordance with the laws of England, United Kingdom.</p> <p>Contact</p> <p>If you have any questions regarding this license, please contact us at legal@spectralcompute.co.uk.</p>"},{"location":"notices/","title":"Third party software","text":"<p>SCALE uses a number of third-party software tools, libraries and content.</p> <p>The file (gratefully) attributes the authors of those works, the licences under which they  are available, and indicates the terms of each license.</p> <p>============================================================================== Thrust is under the Apache Licence v2.0, with some specific exceptions listed below LLVM and libcu++ is under the Apache License v2.0 with LLVM Exceptions: ==============================================================================                                 Apache License                            Version 2.0, January 2004                         http://www.apache.org/licenses/</p> <pre><code>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n  \"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n  \"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n  \"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n  \"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n  \"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n  \"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n  \"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n  \"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n  \"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n  \"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\n  this License, each Contributor hereby grants to You a perpetual,\n  worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n  copyright license to reproduce, prepare Derivative Works of,\n  publicly display, publicly perform, sublicense, and distribute the\n  Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\n  this License, each Contributor hereby grants to You a perpetual,\n  worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n  (except as stated in this section) patent license to make, have made,\n  use, offer to sell, sell, import, and otherwise transfer the Work,\n  where such license applies only to those patent claims licensable\n  by such Contributor that are necessarily infringed by their\n  Contribution(s) alone or by combination of their Contribution(s)\n  with the Work to which such Contribution(s) was submitted. If You\n  institute patent litigation against any entity (including a\n  cross-claim or counterclaim in a lawsuit) alleging that the Work\n  or a Contribution incorporated within the Work constitutes direct\n  or contributory patent infringement, then any patent licenses\n  granted to You under this License for that Work shall terminate\n  as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\n  Work or Derivative Works thereof in any medium, with or without\n  modifications, and in Source or Object form, provided that You\n  meet the following conditions:\n\n  (a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n  (b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n  (c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n  (d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n  You may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\n  any Contribution intentionally submitted for inclusion in the Work\n  by You to the Licensor shall be under the terms and conditions of\n  this License, without any additional terms or conditions.\n  Notwithstanding the above, nothing herein shall supersede or modify\n  the terms of any separate license agreement you may have executed\n  with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\n  names, trademarks, service marks, or product names of the Licensor,\n  except as required for reasonable and customary use in describing the\n  origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\n  agreed to in writing, Licensor provides the Work (and each\n  Contributor provides its Contributions) on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n  implied, including, without limitation, any warranties or conditions\n  of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n  PARTICULAR PURPOSE. You are solely responsible for determining the\n  appropriateness of using or redistributing the Work and assume any\n  risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\n  whether in tort (including negligence), contract, or otherwise,\n  unless required by applicable law (such as deliberate and grossly\n  negligent acts) or agreed to in writing, shall any Contributor be\n  liable to You for damages, including any direct, indirect, special,\n  incidental, or consequential damages of any character arising as a\n  result of this License or out of the use or inability to use the\n  Work (including but not limited to damages for loss of goodwill,\n  work stoppage, computer failure or malfunction, or any and all\n  other commercial damages or losses), even if such Contributor\n  has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\n  the Work or Derivative Works thereof, You may choose to offer,\n  and charge a fee for, acceptance of support, warranty, indemnity,\n  or other liability obligations and/or rights consistent with this\n  License. However, in accepting such obligations, You may act only\n  on Your own behalf and on Your sole responsibility, not on behalf\n  of any other Contributor, and only if You agree to indemnify,\n  defend, and hold each Contributor harmless for any liability\n  incurred by, or claims asserted against, such Contributor by reason\n  of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre> <p>============================================================================== Legacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy): ============================================================================== ---- LLVM Exceptions to the Apache 2.0 License ----</p> <p>As an exception, if, as a result of your compiling your source code, portions of this Software are embedded into an Object form of such source code, you may redistribute such embedded portions in such Object form without complying with the conditions of Sections 4(a), 4(b) and 4(d) of the License.</p> <p>In addition, if you combine or link compiled forms of this Software with software that is licensed under the GPLv2 (\"Combined Software\") and if a court of competent jurisdiction determines that the patent provision (Section 3), the indemnity provision (Section 9) or other Section of the License conflicts with the conditions of the GPLv2, you may retroactively and prospectively choose to deem waived or otherwise exclude such Section(s) of the License, but only in their entirety and only with respect to the Combined Software.</p> <p>============================================================================== Software from third parties included in the LLVM Project: ============================================================================== The LLVM Project contains third party software which is under different license terms. All such code will be identified clearly using at least one of two mechanisms: 1) It will be in a separate directory tree with its own <code>LICENSE.txt</code> or    <code>LICENSE</code> file at the top containing the specific license and restrictions    which apply to that software, or 2) It will contain specific license and restriction terms at the top of every    file.</p> <p>============================================================================== Legacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy): ==============================================================================</p> <p>The libc++ library is dual licensed under both the University of Illinois \"BSD-Like\" license and the MIT license.  As a user of this code you may choose to use it under either license.  As a contributor, you agree to allow your code to be used under both.</p> <p>Full text of the relevant licenses is included below.</p> <p>==============================================================================</p> <p>University of Illinois/NCSA Open Source License</p> <p>Copyright (c) 2009-2019 by the contributors listed in CREDITS.TXT</p> <p>All rights reserved.</p> <p>Developed by:</p> <pre><code>LLVM Team\n\nUniversity of Illinois at Urbana-Champaign\n\nhttp://llvm.org\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <pre><code>* Redistributions of source code must retain the above copyright notice,\n  this list of conditions and the following disclaimers.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimers in the\n  documentation and/or other materials provided with the distribution.\n\n* Neither the names of the LLVM Team, University of Illinois at\n  Urbana-Champaign, nor the names of its contributors may be used to\n  endorse or promote products derived from this Software without specific\n  prior written permission.\n</code></pre> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.</p> <p>==============================================================================</p> <p>Copyright (c) 2009-2014 by the contributors listed in CREDITS.TXT</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>============================================================================== Some libcudacxx components not shipped with this distribution are covered by the below license. Each source file indicates which license it is under.</p> <p>If you find a file in our distribution with the following license, please let us know at legal@spectralcompute.co.uk immediately. ==============================================================================</p> <p>NVIDIA SOFTWARE LICENSE</p> <p>This license is a legal agreement between you and NVIDIA Corporation (\"NVIDIA\") and governs your use of the NVIDIA/CUDA C++ Library software and materials provided hereunder (\u201cSOFTWARE\u201d).</p> <p>This license can be accepted only by an adult of legal age of majority in the country in which the SOFTWARE is used. If you are under the legal age of majority, you must ask your parent or legal guardian to consent to this license. By taking delivery of the SOFTWARE, you affirm that you have reached the legal age of majority, you accept the terms of this license, and you take legal and financial responsibility for the actions of your permitted users.</p> <p>You agree to use the SOFTWARE only for purposes that are permitted by (a) this license, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.</p> <ol> <li> <p>LICENSE. Subject to the terms of this license, NVIDIA grants you a non-exclusive limited license to: (a) install and use the SOFTWARE, and (b) distribute the SOFTWARE subject to the distribution requirements described in this license. NVIDIA reserves all rights, title and interest in and to the SOFTWARE not expressly granted to you under this license.</p> </li> <li> <p>DISTRIBUTION REQUIREMENTS. These are the distribution requirements for you to exercise the distribution grant: a.  The terms under which you distribute the SOFTWARE must be consistent with the terms of this license, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA\u2019s intellectual property rights. b.  You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SOFTWARE not in compliance with the requirements of this license, and to enforce the terms of your agreements with respect to distributed SOFTWARE.</p> </li> <li> <p>LIMITATIONS. Your license to use the SOFTWARE is restricted as follows: a.  The SOFTWARE is licensed for you to develop applications only for use in systems with NVIDIA GPUs. b.  You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SOFTWARE or copies of the SOFTWARE. c.  You may not modify or create derivative works of any portion of the SOFTWARE. d.  You may not bypass, disable, or circumvent any technical measure, encryption, security, digital rights management or authentication mechanism in the SOFTWARE. e.  You may not use the SOFTWARE in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SOFTWARE be (i) disclosed or distributed in source code form; (ii) licensed for the purpose of making derivative works; or (iii) redistributable at no charge. f.  Unless you have an agreement with NVIDIA for this purpose, you may not use the SOFTWARE with any system or application where the use or failure of the system or application can reasonably be expected to threaten or result in personal injury, death, or catastrophic loss. Examples include use in avionics, navigation, military, medical, life support or other life critical applications. NVIDIA does not design, test or manufacture the SOFTWARE for these critical uses and NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. g.  You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney\u2019s fees and costs incident to establishing the right of indemnification) arising out of or related to use of the SOFTWARE outside of the scope of this Agreement, or not in compliance with its terms.</p> </li> <li> <p>PRE-RELEASE. SOFTWARE versions identified as alpha, beta, preview, early access or otherwise as pre-release may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. You may use a pre-release SOFTWARE version at your own risk, understanding that these versions are not intended for use in production or business-critical systems.</p> </li> <li> <p>OWNERSHIP. The SOFTWARE and the related intellectual property rights therein are and will remain the sole and exclusive property of NVIDIA or its licensors. The SOFTWARE is copyrighted and protected by the laws of the United States and other countries, and international treaty provisions. NVIDIA may make changes to the SOFTWARE, at any time without notice, but is not obligated to support or update the SOFTWARE.</p> </li> <li> <p>COMPONENTS UNDER OTHER LICENSES. The SOFTWARE may include NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SOFTWARE. If and to the extent there is a conflict between the terms in this license and the license terms associated with a component, the license terms associated with the components control only to the extent necessary to resolve the conflict.</p> </li> <li> <p>FEEDBACK. You may, but don\u2019t have to, provide to NVIDIA any Feedback. \u201cFeedback\u201d means any suggestions, bug fixes, enhancements, modifications, feature requests or other feedback regarding the SOFTWARE. For any Feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) the Feedback without the payment of any royalties or fees to you. NVIDIA will use Feedback at its choice.</p> </li> <li> <p>NO WARRANTIES. THE SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. NVIDIA DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT THE OPERATION THEREOF WILL BE UNINTERRUPTED OR ERROR-FREE, OR THAT ALL ERRORS WILL BE CORRECTED.</p> </li> <li> <p>LIMITATIONS OF LIABILITY. TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR ANY LOST PROFITS, PROJECT DELAYS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS LICENSE OR THE USE OR PERFORMANCE OF THE SOFTWARE, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY, EVEN IF NVIDIA HAS PREVIOUSLY BEEN ADVISED OF, OR COULD REASONABLY HAVE FORESEEN, THE POSSIBILITY OF SUCH DAMAGES. IN NO EVENT WILL NVIDIA\u2019S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS LICENSE EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.</p> </li> <li> <p>TERMINATION. Your rights under this license will terminate automatically without notice from NVIDIA if you fail to comply with any term and condition of this license or if you commence or participate in any legal proceeding against NVIDIA with respect to the SOFTWARE. NVIDIA may terminate this license with advance written notice to you if NVIDIA decides to no longer provide the SOFTWARE in a country or, in NVIDIA\u2019s sole discretion, the continued use of it is no longer commercially viable. Upon any termination of this license, you agree to promptly discontinue use of the SOFTWARE and destroy all copies in your possession or control. Your prior distributions in accordance with this license are not affected by the termination of this license. All provisions of this license will survive termination, except for the license granted to you.</p> </li> <li> <p>APPLICABLE LAW. This license will be governed in all respects by the laws of the United States and of the State of Delaware as those laws are applied to contracts entered into and performed entirely within Delaware by Delaware residents, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language. The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this license. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.</p> </li> <li> <p>NO ASSIGNMENT. This license and your rights and obligations thereunder may not be assigned by you by any means or operation of law without NVIDIA\u2019s permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect.</p> </li> <li> <p>EXPORT. The SOFTWARE is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SOFTWARE into any country, or use the SOFTWARE in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasury\u2019s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this license, you confirm that you are not a resident or citizen of any country currently embargoed by the U.S. and that you are not otherwise prohibited from receiving the SOFTWARE.</p> </li> <li> <p>GOVERNMENT USE. The SOFTWARE has been developed entirely at private expense and is \u201ccommercial items\u201d consisting of \u201ccommercial computer software\u201d and \u201ccommercial computer software documentation\u201d provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this license pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (b)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.</p> </li> <li> <p>ENTIRE AGREEMENT. This license is the final, complete and exclusive agreement between the parties relating to the subject matter of this license and supersedes all prior or contemporaneous understandings and agreements relating to this subject matter, whether oral or written. If any court of competent jurisdiction determines that any provision of this license is illegal, invalid or unenforceable, the remaining provisions will remain in full force and effect. This license may only be modified in a writing signed by an authorized representative of each party.</p> </li> </ol> <p>(v. August 20, 2021)</p> <p>================================================================================ Some portions of Thrust may be licensed under other compatible open-source licenses. Any divergence from the Apache 2 license will be noted in the source code where applicable. Portions under other terms include, but are not limited to: ================================================================================</p> <p>Various C++ utility classes in Thrust are based on the Boost Iterator, Tuple, System, and Random Number libraries, which are provided under the Boost Software License:</p> <pre><code>Boost Software License - Version 1.0 - August 17th, 2003\n\nPermission is hereby granted, free of charge, to any person or organization\nobtaining a copy of the software and accompanying documentation covered by\nthis license (the \"Software\") to use, reproduce, display, distribute,\nexecute, and transmit the Software, and to prepare derivative works of the\nSoftware, and to permit third-parties to whom the Software is furnished to\ndo so, all subject to the following:\n\nThe copyright notices in the Software and this entire statement, including\nthe above license grant, this restriction and the following disclaimer,\nmust be included in all copies of the Software, in whole or in part, and\nall derivative works of the Software, unless such copies or derivative\nworks are solely in the form of machine-executable object code generated by\na source language processor.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT\nSHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE\nFOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE.\n</code></pre> <p>================================================================================</p> <p>Portions of the thrust::complex implementation are derived from FreeBSD with the following terms:</p> <p>================================================================================</p> <pre><code>Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n\n1. Redistributions of source code must retain the above copyright\n   notice[1] unmodified, this list of conditions, and the following\n   disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\nOF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\nIN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\nINCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\nNOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\nTHIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre> <p>[1] Individual copyright notices from the original authors are included in     the relevant source files.</p> <p>============================================================================== CUB's source code is released under the BSD 3-Clause license: ============================================================================== Copyright (c) 2010-2011, Duane Merrill.  All rights reserved. Copyright (c) 2011-2023, NVIDIA CORPORATION.  All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:    *  Redistributions of source code must retain the above copyright       notice, this list of conditions and the following disclaimer.    *  Redistributions in binary form must reproduce the above copyright       notice, this list of conditions and the following disclaimer in the       documentation and/or other materials provided with the distribution.    *  Neither the name of the NVIDIA CORPORATION nor the       names of its contributors may be used to endorse or promote products       derived from this software without specific prior written permission.</p> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>============================================================================== The ROCm project is distributed under the following license ==============================================================================</p> <p>MIT License</p> <p>Copyright \u00a9 2023 - 2024 Advanced Micro Devices, Inc. All rights reserved.</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>============================================================================== The ROCm ROCT-Thunk-Interface is distributed under the following license ==============================================================================</p> <p>ROCT-Thunk Interface LICENSE</p> <p>Copyright (c) 2016 Advanced Micro Devices, Inc. All rights reserved.</p> <p>MIT LICENSE: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>This product contains software provided by Nginx, Inc. and its contributors.</p> <p>Copyright (C) 2002-2018 Igor Sysoev Copyright (C) 2011-2018 Nginx, Inc. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <p>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>============================================================================== The ROCm ROCR-Runtime project is distributed under the following license ==============================================================================</p> <p>The University of Illinois/NCSA Open Source License (NCSA)</p> <p>Copyright (c) 2024, Advanced Micro Devices, Inc. All rights reserved.</p> <p>Developed by:</p> <pre><code>            AMD Research and AMD HSA Software Development\n\n            Advanced Micro Devices, Inc.\n\n            www.amd.com\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <ul> <li>Redistributions of source code must retain the above copyright notice,    this list of conditions and the following disclaimers.</li> <li>Redistributions in binary form must reproduce the above copyright    notice, this list of conditions and the following disclaimers in    the documentation and/or other materials provided with the distribution.</li> <li>Neither the names of Advanced Micro Devices, Inc,    nor the names of its contributors may be used to endorse or promote    products derived from this Software without specific prior written    permission.</li> </ul> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.</p> <p>============================================================================== The erfinvf() function was implemented from scratch with inspiration from  the following algorithm:</p> <p>\u201cApproximating the erfinv function.\" - M. Giles https://people.maths.ox.ac.uk/gilesm/files/gems_erfinv.pdf ==============================================================================</p> <p>============================================================================== The erfcinvf() function's source code is released under the following license: ============================================================================== /*   Copyright 2023, Norbert Juffa</p> <p>Redistribution and use in source and binary forms, with or without   modification, are permitted provided that the following conditions   are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright      notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright      notice, this list of conditions and the following disclaimer in the      documentation and/or other materials provided with the distribution.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS   \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT   HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */</p> <p>============================================================================== Blender Cycles is released under the Apache 2.0 license: ==============================================================================</p> <pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p> <p>============================================================================== The docopt.cpp project is released under the following license: ==============================================================================</p> <p>Copyright (c) 2012 Vladimir Keleshev, vladimir@keleshev.com</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"use_of_trademarks/","title":"Use of Trademarks","text":"<p>All uses of trademarks on this website are purely for nominative and/or descriptive purposes. We do not claim any affiliations, partnerships, licensing agreements or any other kind of association with Nvidia Corporation, Advanced Micro Devices, Inc. or Intel Corporation.</p> <ul> <li>CUDA is a registered trademark of the Nvidia Corporation.</li> <li>AMD ROCm is a registered trademark of Advanced Micro Devices, Inc.</li> <li>SCALE is a registered trademark of Spectral Compute Ltd.</li> </ul> <p>Please contact us at legal@spectralcompute.co.uk for any inquiries or corrections regarding our use of trademarks.</p>"},{"location":"contact/report-a-bug/","title":"Report a Bug","text":"<p>SCALE is still in active development, so you may encounter bugs. If you run  into problems, contact us by:</p> <ul> <li>Joining our Discord</li> <li>Creating a ticket</li> <li>Sending us an e-mail at hello@spectralcompute.co.uk</li> </ul> <p>The remainder of this page provides information about how to make your  report as helpful as possible.</p>"},{"location":"contact/report-a-bug/#no-such-function-cudasomethingsomething","title":"\"No such function: cudaSomethingSomething()\"","text":"<p>If your project fails to compile due to a missing CUDA Runtime or Driver API function, get in touch: this helps us prioritise work by fixing the holes that have the most demand first.</p>"},{"location":"contact/report-a-bug/#no-such-function-cublascufftcusolversomethingsomething","title":"\"No such function: cuBlas/cuFFt/cuSolverSomethingSomething()\"","text":"<p>If your project needs a missing \"CUDA-X\" API (cuBLAS, cuFFT, cuSOLVER and friends), this is most likely something you can fix yourself by submitting a patch to the open-source library wrapper project. So long as an equivalent function is available in a ROCm library, the wrapper code is trivial.</p>"},{"location":"contact/report-a-bug/#compiler-crash","title":"Compiler crash","text":"<p>When the compiler crashes, it creates temporary files containing a reproducer for the compiler crash, like this:</p> <pre><code>********************\n\nPLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:\nPreprocessed source(s) and associated run script(s) are located at:\nclang++: note: diagnostic msg: /tmp/a-02f191.cpp\nclang++: note: diagnostic msg: /tmp/a-02f191.sh\nclang++: note: diagnostic msg:\n\n********************\n</code></pre> <p>These files will contain the preprocessed version of the source file that broke the compiler, among other things. If you are able to share this with us, it will significantly increase the usefulness of the bug report.</p> <p>If the source file contains sensitive/proprietary information, this could be destroyed by reducing the testcase using cvise. Alternatively, a bug report consisting of just the compiler output is still helpful - especially if it relates to PTX.</p>"},{"location":"contact/report-a-bug/#gpu-crash","title":"GPU Crash","text":"<p>If your GPU code crashes with SCALE but not with NVIDIA's compiler, more useful information can be harvested by enabling some environment variables that dump extra information. If you are able, sharing the output obtained from reproducing the crash with one or both of these enabled can be helpful:</p> <ul> <li><code>REDSCALE_CRASH_REPORT_DETAILED=1</code> will dump extra information from the   GPU trap handler. This includes register state and some symbol names, so   it is unlikely to contain any sensitive/proprietary information from your code.</li> <li><code>REDSCALE_CRASH_DUMP=somefilename</code> will write the crashing machine code to   a file. This makes it easier to investigate the problem, but it means that you're   sharing the compiled version of the crasing GPU kernel with us.</li> </ul>"},{"location":"contact/report-a-bug/#something-else","title":"Something else","text":"<p>It will be helpful if you provide the output of the following commands along with your report:</p> <pre><code>lspci | grep VGA\nredscaleinfo\nhsainfo\nhsakmtinfo\n</code></pre> <p>Running your program with the environment variable <code>SCALE_EXCEPTIONS=1</code> set might give a more detailed error that would be helpful to us too.</p>"},{"location":"examples/","title":"SCALE Example Programs","text":"<p>These example programs are simple CUDA programs demonstrating the  capabilities of SCALE.</p> <p>SCALE is capable of much more, but these small demonstrations serve as a  proof of concept of CUDA compatibility, as well as a starting point for  users wishing to get into GPGPU programming.</p>"},{"location":"examples/#list-of-examples","title":"List of examples","text":"<p>Here is the list of examples that are currently available:</p> Example What it is about Basic Usage in its simplest form PTX Using PTX Assembly BLAS Using BLAS maths wrapper"},{"location":"examples/#accessing-the-examples","title":"Accessing the examples","text":"<p>The examples are hosted in the public github repository with the rest of this manual.</p> <pre><code>git clone https://github.com/spectral-compute/scale-docs.git\ncd scale-docs/examples\n</code></pre>"},{"location":"examples/#using-the-examples","title":"Using the examples","text":"<p>To build an example: - Install SCALE - Decide on a GPU target - Build the example using cmake</p> <p>The example repository includes a helper script, <code>example.sh</code> that can fully  automate the process. Pass your SCALE target directory as the first argument, and the example you want to build/run as the second:</p> <pre><code># You should be in the `examples` directory of the `scale-docs` repository\n./example.sh /opt/scale/targets/gfx1030 basic\n</code></pre> <p>For the specified example, this will:</p> <ol> <li>Remove its build directory if it already exists.</li> <li>Configure CMake for that example in a freshly-created build directory.</li> <li>Build the example in that directory using Make.</li> <li>Set the <code>SCALE_EXCEPTIONS=1</code> environment variable for better     error reporting.</li> <li>Run the example.</li> </ol>"},{"location":"examples/basic/","title":"Basic example","text":"<p>This is simple vector-sum kernel using CUDA. </p> <p>The example:</p> <ul> <li>Generates test data on the host</li> <li>Sends data to the device</li> <li>Launches a kernel on the device</li> <li>Receives data back from the device</li> <li>Checks that the data is correct</li> </ul> <p>Build and run the example by following the general instructions.</p>"},{"location":"examples/basic/#example-source-code","title":"Example source code","text":"<pre><code>#include &lt;vector&gt;\n#include &lt;iostream&gt;\n\n\n// The kernel we are going to launch\n__global__ void basicSum(const int * a, const int * b, size_t n, int * out) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx &lt; n)\n    {\n        out[idx] = a[idx] + b[idx];\n    }\n}\n\n\n// A generic helper function to simplify error handling.\nvoid check(cudaError_t error, const char * file, size_t line) {\n    if (error != cudaSuccess)\n    {\n        std::cout &lt;&lt; \"cuda error: \" &lt;&lt; cudaGetErrorString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\n// A wrapper for the helper function above to include the filename and line number\n// where the error occurs into the output.\n#define CHECK(error) check(error, __FILE__, __LINE__)\n\n\nint main(int argc, char ** argv) {\n\n    const size_t N = 4096;\n    const size_t BYTES = N * sizeof(int);\n\n    std::vector&lt;int&gt; a(N);\n    std::vector&lt;int&gt; b(N);\n    std::vector&lt;int&gt; out(N);\n\n    // Generate input data\n    for (size_t i = 0; i &lt; N; i++) {\n        a[i] = i * 2;\n        b[i] = N - i;\n    }\n\n    int * devA;\n    int * devB;\n    int * devOut;\n\n    // Allocate memory for the inputs and the output\n    CHECK(cudaMalloc(&amp;devA, BYTES));\n    CHECK(cudaMalloc(&amp;devB, BYTES));\n    CHECK(cudaMalloc(&amp;devOut, BYTES));\n\n    // Copy the input data to the device\n    CHECK(cudaMemcpy(devA, a.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devB, b.data(), BYTES, cudaMemcpyHostToDevice));\n\n    // Launch the kernel\n    basicSum&lt;&lt;&lt;N / 256 + 1, 256&gt;&gt;&gt;(devA, devB, N, devOut);\n    CHECK(cudaDeviceSynchronize());\n    CHECK(cudaGetLastError());\n\n    // Copy the output data back to host\n    CHECK(cudaMemcpy(out.data(), devOut, BYTES, cudaMemcpyDeviceToHost));\n\n    // Free up the memory we allocated for the inputs and the output\n    CHECK(cudaFree(devA));\n    CHECK(cudaFree(devB));\n    CHECK(cudaFree(devOut));\n\n    // Test that the output matches our expectations\n    for (size_t i = 0; i &lt; N; i++) {\n        if (a[i] + b[i] != out[i]) {\n            std::cout &lt;&lt; \"Incorrect sum: \" &lt;&lt; a[i] &lt;&lt; \" + \" &lt;&lt; b[i] &lt;&lt; \" = \" &lt;&lt; out[i] &lt;&lt; \" ?\\n\";\n        }\n    }\n\n    std::cout &lt;&lt; \"Example finished\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"examples/basic/#cmakeliststxt-used","title":"<code>CMakeLists.txt</code> used","text":"<pre><code>cmake_minimum_required(VERSION 3.17 FATAL_ERROR)\nproject(example_basic LANGUAGES CUDA)\n\nadd_executable(example_basic basic.cu)\n</code></pre>"},{"location":"examples/blas/","title":"BLAS example","text":"<p>This example demonstrates SCALE's compatibility with cuBLAS APIs by using  cuBLAS to perform a double-precision dot-product on an AMD GPU.</p> <p>cuBLAS APIs are forwarded to use the relevant ROCm APIs. Note that the example links to <code>cublas</code> in its <code>CMakeLists.txt</code>.</p>"},{"location":"examples/blas/#example-source-code","title":"Example source code","text":"<pre><code>#include &lt;vector&gt;\n#include &lt;iostream&gt;\n\n#include &lt;cublas_v2.h&gt;\n\n\nvoid check(cudaError_t error, const char * file, size_t line) {\n    if (error != cudaSuccess)\n    {\n        std::cout &lt;&lt; \"cuda error: \" &lt;&lt; cudaGetErrorString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\nvoid checkCublas(cublasStatus_t error, const char * file, size_t line) {\n    if (error != CUBLAS_STATUS_SUCCESS) {\n        std::cout &lt;&lt; \"cublas error: \" &lt;&lt; cublasGetStatusString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\n#define CHECK(error) check(error, __FILE__, __LINE__)\n#define CHECK_CUBLAS(error) checkCublas(error, __FILE__, __LINE__)\n\n\nint main(int argc, char ** argv) {\n    cublasHandle_t handle;\n    CHECK_CUBLAS(cublasCreate(&amp;handle));\n\n    const size_t N = 10;\n    const size_t BYTES = N * sizeof(double);\n    const double E = 1e-5;\n\n    /* Prepare the data */\n\n    std::vector&lt;double&gt; A(N);\n    std::vector&lt;double&gt; B(N);\n\n    for (size_t i = 0; i &lt; N; i++) {\n        A[i] = i;\n        B[i] = i + N;\n    }\n\n    /* Send the data */\n\n    double * devA;\n    double * devB;\n\n    CHECK(cudaMalloc(&amp;devA, BYTES));\n    CHECK(cudaMalloc(&amp;devB, BYTES));\n\n    CHECK(cudaMemcpy(devA, A.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devB, B.data(), BYTES, cudaMemcpyHostToDevice));\n\n    /* Calculate */\n\n    const int strideA = 1;\n    const int strideB = 1;\n    double result = 0;\n\n    CHECK_CUBLAS(cublasDdot(handle, A.size(), devA, strideA, devB, strideB, &amp;result));\n\n    CHECK(cudaDeviceSynchronize());\n\n    double expected = 0;\n    for (size_t i = 0; i &lt; N; i++) {\n        expected += A[i] * B[i];\n    }\n\n    if (std::abs(result - expected) &gt; E) {\n        std::cout &lt;&lt; \"Result \" &lt;&lt; result &lt;&lt; \" is different from expected \" &lt;&lt; expected &lt;&lt; std::endl;\n    }\n\n    CHECK_CUBLAS(cublasDestroy(handle));\n\n    std::cout &lt;&lt; \"Example finished.\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"examples/blas/#cmakeliststxt-used","title":"<code>CMakeLists.txt</code> used","text":"<pre><code>cmake_minimum_required(VERSION 3.17 FATAL_ERROR)\nproject(example_blas LANGUAGES CUDA)\n\nadd_executable(example_blas blas.cu)\ntarget_link_libraries(example_blas PRIVATE cublas redscale)\n</code></pre>"},{"location":"examples/ptx/","title":"PTX example","text":"<p>This example demonstrates SCALE's support for inline PTX. A lot of real-world  CUDA code uses inline PTX asm blocks, which are inherently NVIDIA-only. There is no  need to rewrite those when using SCALE: the compiler just digests them and  outputs AMD machine code.</p> <p>This example uses C++ templates to access the functionality of the PTX  <code>lop3</code> instruction, used in various ways throughout the kernel.</p> <p>Build and run the example by following the general instructions.</p>"},{"location":"examples/ptx/#extra-info","title":"Extra info","text":"<ul> <li>Using inline PTX Assembly</li> <li>PTX ISA reference</li> </ul> <p>PTX instructions used:</p> <ul> <li><code>add</code></li> <li><code>lop3</code></li> </ul>"},{"location":"examples/ptx/#example-source-code","title":"Example source code","text":"<pre><code>#include &lt;bitset&gt;\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;cstdint&gt;\n\n\n__device__ inline uint32_t ptx_add(uint32_t x, uint32_t y) {\n    // Calculate a sum of `x` and `y`, put the result into `x`\n    asm(\n        \"add.u32 %0, %0, %1;\"\n        : \"+r\"(x)\n        : \"r\"(y)\n    );\n    return x;\n}\n\n\n__global__ void kernelAdd(const uint32_t * a, const uint32_t * b, size_t n, uint32_t * out) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx &lt; n)\n    {\n        out[idx] = ptx_add(a[idx], b[idx]);\n    }\n}\n\n\ntemplate&lt;uint8_t Op&gt;\n__device__ inline uint32_t ptx_lop3(uint32_t x, uint32_t y, uint32_t z) {\n    // Compute operator `Op` on `x`, `y`, `z`, put the result into `x`\n\n    asm(\n        \"lop3.b32 %0, %0, %1, %2, %3;\"\n        : \"+r\"(x)\n        : \"r\"(y), \"r\"(z), \"n\"(Op)\n    );\n    return x;\n}\n\n\ntemplate&lt;uint8_t Op&gt;\n__global__ void kernelLop3(const uint32_t * a, const uint32_t * b, const uint32_t * c, size_t n, uint32_t * out) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx &lt; n)\n    {\n        out[idx] = ptx_lop3&lt;Op&gt;(a[idx], b[idx], c[idx]);\n    }\n}\n\n\nvoid check(cudaError_t error, const char * file, size_t line) {\n    if (error != cudaSuccess)\n    {\n        std::cout &lt;&lt; \"cuda error: \" &lt;&lt; cudaGetErrorString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\n#define CHECK(error) check(error, __FILE__, __LINE__)\n\n\ntemplate&lt;typename T&gt;\nconstexpr T lop3op(T a, T b, T c) {\n    return a &amp; b ^ (~c);\n}\n\n\nint main(int argc, char ** argv) {\n\n    const size_t N = 4096;\n    const size_t BYTES = N * sizeof(uint32_t);\n\n    std::vector&lt;uint32_t&gt; a(N);\n    std::vector&lt;uint32_t&gt; b(N);\n    std::vector&lt;uint32_t&gt; c(N);\n    std::vector&lt;uint32_t&gt; out(N);\n\n    for (size_t i = 0; i &lt; N; i++) {\n        a[i] = i * 2;\n        b[i] = N - i;\n        c[i] = i * i;\n    }\n\n    uint32_t * devA;\n    uint32_t * devB;\n    uint32_t * devC;\n    uint32_t * devOut;\n\n    CHECK(cudaMalloc(&amp;devA, BYTES));\n    CHECK(cudaMalloc(&amp;devB, BYTES));\n    CHECK(cudaMalloc(&amp;devC, BYTES));\n    CHECK(cudaMalloc(&amp;devOut, BYTES));\n\n    CHECK(cudaMemcpy(devA, a.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devB, b.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devC, c.data(), BYTES, cudaMemcpyHostToDevice));\n\n    // Test \"add\"\n\n    kernelAdd&lt;&lt;&lt;N / 256 + 1, 256&gt;&gt;&gt;(devA, devB, N, devOut);\n    CHECK(cudaDeviceSynchronize());\n    CHECK(cudaGetLastError());\n\n    CHECK(cudaMemcpy(out.data(), devOut, BYTES, cudaMemcpyDeviceToHost));\n\n    for (size_t i = 0; i &lt; N; i++) {\n        if (a[i] + b[i] != out[i]) {\n            std::cout &lt;&lt; \"Incorrect add: \" &lt;&lt; a[i] &lt;&lt; \" + \" &lt;&lt; b[i] &lt;&lt; \" = \" &lt;&lt; out[i] &lt;&lt; \" ?\\n\";\n        }\n    }\n\n    // Test \"lop3\"\n\n    constexpr uint8_t TA = 0xF0;\n    constexpr uint8_t TB = 0xCC;\n    constexpr uint8_t TC = 0xAA;\n    constexpr uint8_t Op = lop3op(TA, TB, TC);\n\n    kernelLop3&lt;Op&gt;&lt;&lt;&lt;N / 256 + 1, 256&gt;&gt;&gt;(devA, devB, devC, N, devOut);\n    CHECK(cudaDeviceSynchronize());\n    CHECK(cudaGetLastError());\n\n    CHECK(cudaMemcpy(out.data(), devOut, BYTES, cudaMemcpyDeviceToHost));\n\n    for (size_t i = 0; i &lt; N; i++) {\n        if (lop3op(a[i], b[i], c[i]) != out[i]) {\n            std::cout &lt;&lt; \"Incorrect lop3: \\n\"\n                &lt;&lt; \"    \" &lt;&lt; std::bitset&lt;32&gt;{a[i]} &lt;&lt; \"\\n\"\n                &lt;&lt; \" &amp;  \" &lt;&lt; std::bitset&lt;32&gt;{b[i]} &lt;&lt; \"\\n\"\n                &lt;&lt; \" ^ ~\" &lt;&lt; std::bitset&lt;32&gt;{c[i]} &lt;&lt; \"\\n\"\n                &lt;&lt; \" =  \" &lt;&lt; std::bitset&lt;32&gt;{out[i]} &lt;&lt; \" ?\\n\\n\";\n        }\n    }\n\n    CHECK(cudaFree(devA));\n    CHECK(cudaFree(devB));\n    CHECK(cudaFree(devC));\n    CHECK(cudaFree(devOut));\n\n    // Finish\n\n    std::cout &lt;&lt; \"Example finished\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"examples/ptx/#cmakeliststxt-used","title":"<code>CMakeLists.txt</code> used","text":"<pre><code>cmake_minimum_required(VERSION 3.17 FATAL_ERROR)\nproject(example_ptx LANGUAGES CUDA)\n\nadd_executable(example_ptx ptx.cu)\n</code></pre>"},{"location":"manual/CHANGELOG/","title":"What's new?","text":""},{"location":"manual/CHANGELOG/#unstable-20250404","title":"Unstable-2025.04.04","text":""},{"location":"manual/CHANGELOG/#platform","title":"Platform","text":"<ul> <li>Packages for Rocky9 are now available.</li> <li>Added <code>scaleenv</code>, a new and much easier way to use SCALE.</li> </ul>"},{"location":"manual/CHANGELOG/#library","title":"Library","text":"<ul> <li>Added most of cuFFT.</li> <li>Lazy-initialisation of primary contexts now works properly, fixing some    subtle lifecycle issues.</li> <li>Added some missing undocumented headers like <code>texture_types.h</code>.</li> <li><code>SCALE_EXCEPTIONS</code> now supports a non-fatal, print-only mode for projects    that create exceptions intentionally.</li> <li>Fixed even more multigpu/IPC bugs, especially when combining P2P and IPC    memory APIs.</li> <li>Added <code>cuMemcpyPeer</code>/<code>cuMemcpyPeerAsync</code>.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler","title":"Compiler","text":"<ul> <li><code>--device-c</code> no longer inappropriately triggers the linker.</li> <li>Support for PTX <code>bar.warp.sync</code></li> <li>Newly-supported <code>nvcc</code> flags:<ul> <li><code>-arch=native</code></li> <li><code>-jump-table-density</code></li> <li><code>-compress-mode</code></li> <li><code>-split-compile-extended</code> (ignored)</li> </ul> </li> <li>Denorm-flushing optimisations are no longer skipped when they aren't   supposed to be.</li> <li>Compiler diagnostic to catch some undefined behaviour patterns with CUDA    atomics.</li> </ul>"},{"location":"manual/CHANGELOG/#unstable-20250324","title":"Unstable-2025.03.24","text":""},{"location":"manual/CHANGELOG/#platform_1","title":"Platform","text":"<ul> <li>Upgraded from llvm18.1.8 to llvm19.1.7. Much zoominess ensues.</li> <li>Added faux \"stubs\" libraries to placate non-cmake buildsystems</li> <li>Added some fake symbols to satisfy ancient/insane buildsystems.</li> <li>Added <code>SCALE_CUDA_VERSION</code> environment variable to tell SCALE to impersonate a   specific version of CUDA.</li> </ul>"},{"location":"manual/CHANGELOG/#library-enhancements","title":"Library Enhancements","text":"<ul> <li>Fix a crash when initialising SCALE with many GPUs with huge amounts of memory.</li> <li>Faster startup times, especially on many-GPU machines.</li> <li>Added CUDA IPC APIs. Among other things, this enables CUDA-MPI    applications to work, including AMGX's distributed mode.</li> <li>Fixed lots of multi-GPU brokenness.</li> <li>Added lots more of cuSolver and cuSPARSE</li> <li>Filled in some missing NVTX APIs</li> <li>Implemented the <code>CU_CTX_SYNC_MEMOPS</code> context flag.</li> <li>Fixed accuracy issues in some of the CUDA Math APIs.</li> <li>fp16 headers no longer produce warnings for projects that include them without   <code>-isystem</code>.</li> <li>Improved performance and correctness of cudaMemcpy/memset by finishing the    move away from hsa's buggy implementation.</li> <li>Fix some wave64 issues with <code>cooperative_groups.h</code>.</li> <li>Support for <code>grid_sync()</code>.</li> <li>Fix subtle issues with pointer attribute APIs.</li> <li>Implicit context initialisation rules now more closely match NVIDIA's, fixing   some projects that depended on checking primary context state.</li> <li>Improvements to C89 compatibility of headers.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler-enhancements","title":"Compiler Enhancements","text":"<ul> <li><code>__launch_bounds__</code> now works correctly, significantly improving performance.</li> <li>Added the nvcc <code>-prec-sqrt</code> and <code>-prec-div</code> flags.</li> <li>Corrected the behaviour of the nvcc <code>-odir</code> flag in during dependency generation.</li> <li><code>use_fast_math</code> now matches nivida's behaviour, instead of mapping to clang's   <code>-ffast-math</code>, which does too much.</li> <li>Support broken template code in more situations in nvcc mode.</li> <li>Allow invalid const-correctness in unexpanded template code in nvcc mode.</li> <li>Allow trailing commas in template argument lists in nvcc mode.</li> <li>Fix a parser crash when explicitly calling <code>operator&lt;&lt;&lt;int&gt;()</code> in CUDA mode.</li> </ul>"},{"location":"manual/CHANGELOG/#thirdparty-project-demos","title":"Thirdparty Project demos","text":"<p>Things that now appear to work include:</p> <ul> <li>CUDA-aware MPI</li> <li>MAGMA</li> </ul>"},{"location":"manual/CHANGELOG/#unstable-20250219","title":"Unstable-2025.02.19","text":""},{"location":"manual/CHANGELOG/#platform_2","title":"Platform","text":"<ul> <li>Support for simulating a warp size of 32 even on wave64 platforms, fixing    many projects on such platforms.</li> <li>Support for <code>bfloat16</code>.</li> <li>Upgraded from llvm17 to llvm18.1.8.</li> <li>Support for rocm 6.3.1</li> <li>Availabiliy of an Ubuntu package repo to simplify installation/upgrades.</li> </ul>"},{"location":"manual/CHANGELOG/#library-enhancements_1","title":"Library Enhancements","text":"<ul> <li>Added software emulated WMMA APIs, and <code>wmma</code>/<code>mma</code> PTX instructions.    Hardware-accelerated versions are in development.</li> <li>Added more Cooperative Groups APIs.</li> <li>Rewritten device allocator to work around HSA bugs and performance issues.</li> <li>Significant performance improvements for most warp-level cooperative    operations.</li> <li>Various random APIs added.</li> </ul>"},{"location":"manual/CHANGELOG/#ptx","title":"PTX","text":"<ul> <li>Compiler diagnostics for unused PTX variables and attempts to return    the carry-bit.</li> <li>PTX variable references and <code>{}</code> now work correctly between <code>asm</code> blocks   within the same function.</li> <li>Added PTX <code>C</code> constraints (dynamic asm strings).</li> <li>Added the new mixed-precision <code>add/sub/fma</code> FP instructions.</li> <li>Added <code>membar</code> instruction.</li> <li>Partial support for <code>fence</code> instruction.</li> <li>half-float PTX instructions now work correctly even if <code>cuda_fp16.h</code> has not    been included.</li> <li>Fixed various parsing issues (undocumented syntax quirks etc.).</li> <li>Fixed an issue where template-dependent asm strings were mishandled.</li> <li>Fixed various miscompilations.</li> </ul>"},{"location":"manual/CHANGELOG/#thirdparty-project-demos_1","title":"Thirdparty Project demos","text":"<p>The <code>scale-validation</code> repo now has working demos for the following:</p> <ul> <li>whisper.cpp</li> <li>TCLB</li> </ul>"},{"location":"manual/CHANGELOG/#release-120-2024-11-27","title":"Release 1.2.0 (2024-11-27)","text":""},{"location":"manual/CHANGELOG/#library-enhancements_2","title":"Library Enhancements","text":"<ul> <li>Support for <code>gfx900</code> architecture.</li> <li>Support for <code>gfx1102</code> architecture.</li> </ul>"},{"location":"manual/CHANGELOG/#ptx_1","title":"PTX","text":"<ul> <li>Improved handling of wave64 in inline PTX.</li> <li>Various inline PTX compilation fixes.</li> </ul>"},{"location":"manual/CHANGELOG/#other","title":"Other","text":"<ul> <li>Support for Ubuntu 24.04.</li> <li>Upgraded to ROCm 6.2.2.</li> </ul>"},{"location":"manual/CHANGELOG/#release-110-2024-10-31","title":"Release 1.1.0 (2024-10-31)","text":""},{"location":"manual/CHANGELOG/#library-enhancements_3","title":"Library Enhancements","text":"<ul> <li>Added much of the CUDA graph API.</li> <li>Improvements to multi-GPU handling.</li> <li>Fixed rare shutdown-time segfaults.</li> <li>Added many random API functions. As usual, see The diff.</li> </ul>"},{"location":"manual/CHANGELOG/#ptx_2","title":"PTX","text":"<ul> <li><code>f16x2</code>, <code>u16x2</code> and <code>s16x2</code> types.</li> <li><code>fns</code> instruction</li> <li>Fixed miscompile of <code>sad</code> instruction.</li> </ul>"},{"location":"manual/CHANGELOG/#thirdparty-project-demos_2","title":"Thirdparty Project demos","text":"<p>The <code>scale-validation</code> repo now has working demos for the following  additional projects:</p> <ul> <li>FLAMEGPU2</li> <li>GPUJPEG</li> <li>gpu_jpeg2k</li> </ul>"},{"location":"manual/CHANGELOG/#release-1020-2024-09-05","title":"Release 1.0.2.0 (2024-09-05)","text":"<p>Documented a record of the CUDA APIs already available in SCALE, and those still to come: Implemented APIs.</p>"},{"location":"manual/CHANGELOG/#library-enhancements_4","title":"Library Enhancements","text":"<ul> <li>Kernel arguments larger than 4kb no longer crash the library.</li> <li>Programs that ignore CUDA error codes can no longer get stuck in a state   where the library always returns the error code you ignored.</li> <li>Fixed synchronisation bugs when using synchronous <code>cuMemset*</code> APIs.</li> <li>Fixed implicit synchronisation behaviour of <code>cuMemcpy2D/cuMemcpy2DAsync()</code>.</li> <li>Fixed precision issues in fp16 <code>exp2()</code>, <code>rsqrt()</code>, and <code>h2log()</code>.</li> <li><code>cudaEventRecord</code> for an empty event no longer returns a time in the past.</li> <li>Fixed occupancy API behaviour in edgecases that are not multiples of warp   size.</li> <li>Fixed rare crashes during static de-initialisation when library wrappers   were in use.</li> <li>All flags supported by SCALE's nvcc are now also accepted by our nvrtc   implementation.</li> <li>Various small header fixes.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler-enhancements_1","title":"Compiler Enhancements","text":"<ul> <li><code>decltype()</code> now works correctly for <code>__host__ __device__</code> functions.</li> <li><code>-Winvalid-constexpr</code> no longer defaults to <code>-Werror</code>, for consistency   with nvcc.</li> <li>PTX variable names including <code>%</code> are no longer rejected.</li> <li>Support for nvcc's nonstandard permissiveness surrounding missing   <code>typename</code> keywords in dependent types.</li> <li>Support for nvcc's wacky \"split declaration\" syntax for <code>__host__ __device</code>   functions (with a warning):   <pre><code>int foo();\n__device__ int foo();\n__host__ int foo() {\n    return 5;\n}\n// foo() is a __host__ __device__ function. :D\n</code></pre></li> <li>Newly-supported compiler flags (all of which are aliases for   standard flags, or combinations thereof):<ul> <li><code>-device-c</code></li> <li><code>-device-w</code></li> <li><code>-pre-include</code></li> <li><code>-library</code></li> <li><code>-output-file</code></li> <li><code>-define-macro</code></li> <li><code>-undefine-macro</code></li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#new-cuda-apis","title":"New CUDA APIs","text":""},{"location":"manual/CHANGELOG/#math-apis","title":"Math APIs","text":"<ul> <li><code>exp10(__half)</code></li> <li><code>exp2(__half)</code></li> <li><code>rcp(__half)</code></li> <li><code>rint(__half)</code></li> <li><code>h2exp10(__half2)</code></li> <li><code>h2exp2(__half2)</code></li> <li><code>h2rcp(__half2)</code></li> <li><code>h2rint(__half2)</code></li> </ul>"},{"location":"manual/CHANGELOG/#release-1010-2024-07-24","title":"Release 1.0.1.0 (2024-07-24)","text":"<p>This release primarily fixes issues that prevent people from successfully compiling their projects with SCALE. Many thanks to those users who submitted bug reports.</p>"},{"location":"manual/CHANGELOG/#cuda-apis","title":"CUDA APIs","text":"<ul> <li>The <code>extra</code> argument to <code>cuLaunchKernel</code> is now supported.</li> <li>Added support for some more undocumented NVIDIA headers.</li> <li>Fix various overload resolution issues with atomic APIs.</li> <li>Fix overload resolution issues with min/max.</li> <li>Added various undocumented macros to support projects that are explicitly   checking cuda include guard macros.</li> <li><code>lrint()</code> and <code>llrint()</code> no longer crash the compiler. :D</li> <li>Newly supported CUDA APIs:<ul> <li><code>nvrtcGetNumSupportedArchs</code></li> <li><code>nvrtcGetSupportedArchs</code></li> <li><code>cudaLaunchKernelEx</code>, <code>cuLaunchKernelEx</code>, <code>cudaLaunchKernelExC</code>: some  of the performance-hint launch options are no-ops.</li> <li><code>__vavgs2</code>, <code>__vavgs4</code></li> <li>All the <code>atomic*_block()</code> and <code>atomic*_system()</code> variants.</li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#compiler_1","title":"Compiler","text":"<ul> <li>Improved parsing of nvcc arguments:<ul> <li>Allow undocumented option variants (<code>-foo bar</code>, <code>--foo bar</code>,    <code>--foo=bar</code>, and <code>-foo=bar</code> are always allowed, it seems).</li> <li>Implement \"interesting\" quoting/escaping rules in nvcc arguments, such as    embedded quotes and <code>\\,</code>. We now correctly handle cursed arguments like:    <code>'-Xcompiler=-Wl\\,-O1' '-Xcompiler=-Wl\\,-rpath\\,/usr/lib,-Wl\\,-rpath-link\\,/usr/lib'</code></li> </ul> </li> <li> <p>Support for more nvcc arguments:</p> <ul> <li>NVCC-style diagnostic flags: <code>-Werror</code>, <code>-disable-warnings</code>, etc.</li> <li><code>--run</code>, <code>--run-args</code></li> <li><code>-Xlinker</code>, <code>-linker-options</code></li> <li><code>-no-exceptions</code>, <code>-noeh</code></li> <li><code>-minimal</code>: no-op. Exact semantics are undocumented, and build times   are reasonably fast anyway.</li> <li><code>-gen-opt-lto</code>, <code>-dlink-time-opt</code>, <code>-dlto</code>. No-ops: device LTO not yet   supported.</li> <li><code>-t</code>, <code>--threads</code>, <code>-split-compile</code>: No-ops: they're flags for making   compilation faster and are specific to how nvcc is implemented.</li> <li><code>-device-int128</code>: no-op: we always enable int128.</li> <li><code>-extra-device-vectorization</code>: no-op: vectorisation optimisations are   controlled by the usual <code>-O*</code> flags.</li> <li><code>-entries</code>, <code>-source-in-ptx</code>, <code>-src-in-ptx</code>: no-ops: there is no PTX.</li> <li><code>-use-local-env</code>, <code>-idp</code>, <code>-ddp</code>, <code>-dp</code>, etc.: ignored since they are   meaningless except on Windows.</li> </ul> </li> <li> <p>Allow variadic device functions in non-evaluated functions.</p> </li> <li>Don't warn about implicit conversion from <code>cudaLaneMask_t</code> to <code>bool</code>.</li> <li><code>__builtin_provable</code> no longer causes compiler crashes in <code>-O0</code>/<code>-O1</code> builds.</li> <li>Fixed a bug causing PTX <code>asm</code> blocks inside non-template, non-dependent   member functions of template classes to sometimes not be compiled,   causing PTX to end up in the AMD binary unmodified.</li> <li>CUDA launch tokens with spaces (ie.: <code>myKernel&lt;&lt; &lt;1, 1&gt;&gt; &gt;()</code>) are now   supported.</li> <li>Building non-cuda C translation units with SCALE-nvcc now works.</li> </ul>"},{"location":"manual/CHANGELOG/#other_1","title":"Other","text":"<ul> <li>The <code>meson</code> build system no longer regards SCALE-nvcc as a \"broken\" compiler.</li> <li><code>hsakmtsysinfo</code> no longer explodes if it doesn't like your GPU.</li> <li>New documentation pages.</li> <li>Published more details about thirdparty testing, including the build scripts.</li> </ul>"},{"location":"manual/CHANGELOG/#release-1000-2024-07-15","title":"Release 1.0.0.0 (2024-07-15)","text":"<p>Initial release</p>"},{"location":"manual/api-driver/","title":"Driver API","text":"<p>Read how this document is structured in the Introduction to implemented APIs.</p>"},{"location":"manual/api-driver/#61-data-types-used-by-cuda-driver","title":"6.1. Data types used by CUDA driver","text":"<pre><code>-struct CUDA_ARRAY3D_DESCRIPTOR_v2;\n-struct CUDA_ARRAY_DESCRIPTOR_v2;\n-struct CUDA_ARRAY_MEMORY_REQUIREMENTS_v1;\n-struct CUDA_ARRAY_SPARSE_PROPERTIES_v1;\n-struct CUDA_BATCH_MEM_OP_NODE_PARAMS_v2;\n-struct CUDA_CHILD_GRAPH_NODE_PARAMS;\n-struct CUDA_CONDITIONAL_NODE_PARAMS;\n-struct CUDA_EVENT_RECORD_NODE_PARAMS;\n-struct CUDA_EVENT_WAIT_NODE_PARAMS;\n-struct CUDA_EXTERNAL_MEMORY_BUFFER_DESC_v1;\n-struct CUDA_EXTERNAL_MEMORY_HANDLE_DESC_v1;\n-struct CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_v1;\n-struct CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_v1;\n-struct CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS_v1;\n-struct CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS_v1;\n-struct CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v1;\n-struct CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v2;\n-struct CUDA_EXT_SEM_WAIT_NODE_PARAMS_v1;\n-struct CUDA_EXT_SEM_WAIT_NODE_PARAMS_v2;\n-struct CUDA_GRAPH_INSTANTIATE_PARAMS;\n-struct CUDA_HOST_NODE_PARAMS_v1;\n-struct CUDA_HOST_NODE_PARAMS_v2;\n-struct CUDA_KERNEL_NODE_PARAMS_v1;\n-struct CUDA_KERNEL_NODE_PARAMS_v2;\n-struct CUDA_KERNEL_NODE_PARAMS_v3;\n-struct CUDA_LAUNCH_PARAMS_v1;\n-struct CUDA_MEMCPY2D_v2;\n-struct CUDA_MEMCPY3D_PEER_v1;\n-struct CUDA_MEMCPY3D_v2;\n-struct CUDA_MEMCPY_NODE_PARAMS;\n-struct CUDA_MEMSET_NODE_PARAMS_v1;\n-struct CUDA_MEMSET_NODE_PARAMS_v2;\n-struct CUDA_MEM_ALLOC_NODE_PARAMS_v1;\n-struct CUDA_MEM_ALLOC_NODE_PARAMS_v2;\n-struct CUDA_MEM_FREE_NODE_PARAMS;\n-struct CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_v1;\n-struct CUDA_RESOURCE_DESC_v1;\n-struct CUDA_RESOURCE_VIEW_DESC_v1;\n-struct CUDA_TEXTURE_DESC_v1;\n-struct CUaccessPolicyWindow_v1;\n-struct CUarrayMapInfo_v1;\n-struct CUasyncNotificationInfo;\n-struct CUctxCigParam;\n-struct CUctxCreateParams;\n-struct CUdevprop_v1;\n-struct CUeglFrame_v1;\n-struct CUexecAffinityParam_v1;\n-struct CUexecAffinitySmCount_v1;\n-struct CUgraphEdgeData;\n-struct CUgraphExecUpdateResultInfo_v1;\n-struct CUgraphNodeParams;\n-struct CUipcEventHandle_v1;\n-struct CUipcMemHandle_v1;\n-struct CUlaunchAttribute;\n-union CUlaunchAttributeValue;\n-struct CUlaunchConfig;\n-struct CUlaunchMemSyncDomainMap;\n-struct CUmemAccessDesc_v1;\n-struct CUmemAllocationProp_v1;\n-struct CUmemFabricHandle_v1;\n-struct CUmemLocation_v1;\n-struct CUmemPoolProps_v1;\n-struct CUmemPoolPtrExportData_v1;\n-struct CUmulticastObjectProp_v1;\n-union CUstreamBatchMemOpParams_v1;\n-struct CUtensorMap;\n-#define CUDA_ARRAY3D_2DARRAY\n-#define CUDA_ARRAY3D_COLOR_ATTACHMENT\n-#define CUDA_ARRAY3D_CUBEMAP\n-#define CUDA_ARRAY3D_DEFERRED_MAPPING\n-#define CUDA_ARRAY3D_DEPTH_TEXTURE\n-#define CUDA_ARRAY3D_LAYERED\n-#define CUDA_ARRAY3D_SPARSE\n-#define CUDA_ARRAY3D_SURFACE_LDST\n-#define CUDA_ARRAY3D_TEXTURE_GATHER\n-#define CUDA_ARRAY3D_VIDEO_ENCODE_DECODE\n-#define CUDA_COOPERATIVE_LAUNCH_MULTI_DEVICE_NO_POST_LAUNCH_SYNC\n-#define CUDA_COOPERATIVE_LAUNCH_MULTI_DEVICE_NO_PRE_LAUNCH_SYNC\n-#define CUDA_EGL_INFINITE_TIMEOUT\n-#define CUDA_EXTERNAL_MEMORY_DEDICATED\n-#define CUDA_EXTERNAL_SEMAPHORE_SIGNAL_SKIP_NVSCIBUF_MEMSYNC\n-#define CUDA_EXTERNAL_SEMAPHORE_WAIT_SKIP_NVSCIBUF_MEMSYNC\n-#define CUDA_NVSCISYNC_ATTR_SIGNAL\n-#define CUDA_NVSCISYNC_ATTR_WAIT\n#define CUDA_VERSION\n-#define CU_ARRAY_SPARSE_PROPERTIES_SINGLE_MIPTAIL\n-#define CU_DEVICE_CPU\n-#define CU_DEVICE_INVALID\n-#define CU_GRAPH_COND_ASSIGN_DEFAULT\n-#define CU_GRAPH_KERNEL_NODE_PORT_DEFAULT\n-#define CU_GRAPH_KERNEL_NODE_PORT_LAUNCH_ORDER\n-#define CU_GRAPH_KERNEL_NODE_PORT_PROGRAMMATIC\n-#define CU_IPC_HANDLE_SIZE\n-#define CU_LAUNCH_PARAM_BUFFER_POINTER\n-#define CU_LAUNCH_PARAM_BUFFER_POINTER_AS_INT\n-#define CU_LAUNCH_PARAM_BUFFER_SIZE\n-#define CU_LAUNCH_PARAM_BUFFER_SIZE_AS_INT\n-#define CU_LAUNCH_PARAM_END\n-#define CU_LAUNCH_PARAM_END_AS_INT\n-#define CU_MEMHOSTALLOC_DEVICEMAP\n-#define CU_MEMHOSTALLOC_PORTABLE\n-#define CU_MEMHOSTALLOC_WRITECOMBINED\n-#define CU_MEMHOSTREGISTER_DEVICEMAP\n-#define CU_MEMHOSTREGISTER_IOMEMORY\n-#define CU_MEMHOSTREGISTER_PORTABLE\n-#define CU_MEMHOSTREGISTER_READ_ONLY\n-#define CU_MEM_CREATE_USAGE_TILE_POOL\n-#define CU_PARAM_TR_DEFAULT\n#define CU_STREAM_LEGACY\n#define CU_STREAM_PER_THREAD\n-#define CU_TENSOR_MAP_NUM_QWORDS\n-#define CU_TRSA_OVERRIDE_FORMAT\n#define CU_TRSF_DISABLE_TRILINEAR_OPTIMIZATION\n#define CU_TRSF_NORMALIZED_COORDINATES\n#define CU_TRSF_READ_AS_INTEGER\n#define CU_TRSF_SEAMLESS_CUBEMAP\n#define CU_TRSF_SRGB\n-#define MAX_PLANES\n-typedef struct CUaccessPolicyWindow_v1 CUaccessPolicyWindow;\n-typedef CUarray_st * CUarray;\n-typedef void(* CUasyncCallback)(CUasyncNotificationInfo *, void *, CUasyncCallbackHandle);\n-typedef CUasyncCallbackEntry_st * CUasyncCallbackHandle;\n-typedef CUctx_st * CUcontext;\n-typedef CUdevice_v1 CUdevice;\n-typedef int CUdevice_v1;\n-typedef CUdeviceptr_v2 CUdeviceptr;\n-typedef unsigned int CUdeviceptr_v2;\n-typedef CUeglStreamConnection_st * CUeglStreamConnection;\n-typedef CUevent_st * CUevent;\n-typedef struct CUexecAffinityParam_v1 CUexecAffinityParam;\n-typedef CUextMemory_st * CUexternalMemory;\n-typedef CUextSemaphore_st * CUexternalSemaphore;\n-typedef CUfunc_st * CUfunction;\n-typedef CUgraph_st * CUgraph;\n-typedef cuuint64_t CUgraphConditionalHandle;\n-typedef CUgraphDeviceUpdatableNode_st * CUgraphDeviceNode;\n-typedef CUgraphExec_st * CUgraphExec;\n-typedef CUgraphNode_st * CUgraphNode;\n-typedef CUgraphicsResource_st * CUgraphicsResource;\n-typedef CUgreenCtx_st * CUgreenCtx;\n-typedef void(* CUhostFn)(void *);\n-typedef CUkern_st * CUkernel;\n-typedef CUlib_st * CUlibrary;\n-typedef CUmemPoolHandle_st * CUmemoryPool;\ntypedef CUmipmappedArray_st * CUmipmappedArray;\n-typedef CUmod_st * CUmodule;\ntypedef size_t(* CUoccupancyB2DSize)(int);\n-typedef CUstream_st * CUstream;\n-typedef void(* CUstreamCallback)(CUstream, CUresult, void *);\n-typedef CUsurfObject_v1 CUsurfObject;\n-typedef unsigned long long CUsurfObject_v1;\n-typedef CUsurfref_st * CUsurfref;\ntypedef CUtexObject_v1 CUtexObject;\ntypedef unsigned long long CUtexObject_v1;\ntypedef CUtexref_st * CUtexref;\n-typedef CUuserObject_st * CUuserObject;\n-enum CUDA_POINTER_ATTRIBUTE_ACCESS_FLAGS;\n-enum CUGPUDirectRDMAWritesOrdering;\n-enum CUaccessProperty;\n-enum CUaddress_mode;\n-enum CUarraySparseSubresourceType;\n-enum CUarray_cubemap_face;\n-enum CUarray_format;\n-enum CUasyncNotificationType;\n-enum CUclusterSchedulingPolicy;\n-enum CUcomputemode;\n-enum CUctx_flags;\n-enum CUdeviceNumaConfig;\n-enum CUdevice_P2PAttribute;\n-enum CUdevice_attribute;\n-enum CUdriverProcAddressQueryResult;\n-enum CUdriverProcAddress_flags;\n-enum CUeglColorFormat;\n-enum CUeglFrameType;\n-enum CUeglResourceLocationFlags;\nenum CUevent_flags;\n-enum CUevent_record_flags;\n-enum CUevent_sched_flags;\n-enum CUevent_wait_flags;\n-enum CUexecAffinityType;\n-enum CUexternalMemoryHandleType;\n-enum CUexternalSemaphoreHandleType;\n-enum CUfilter_mode;\n-enum CUflushGPUDirectRDMAWritesOptions;\n-enum CUflushGPUDirectRDMAWritesScope;\n-enum CUflushGPUDirectRDMAWritesTarget;\n-enum CUfunc_cache;\n-enum CUfunction_attribute;\n-enum CUgraphConditionalNodeType;\n-enum CUgraphDebugDot_flags;\n-enum CUgraphDependencyType;\n-enum CUgraphExecUpdateResult;\n-enum CUgraphInstantiateResult;\n-enum CUgraphInstantiate_flags;\n-enum CUgraphNodeType;\n-enum CUgraphicsMapResourceFlags;\n-enum CUgraphicsRegisterFlags;\n-enum CUipcMem_flags;\n-enum CUjitInputType;\n-enum CUjit_cacheMode;\n-enum CUjit_fallback;\n-enum CUjit_option;\n-enum CUjit_target;\n-enum CUlaunchAttributeID;\n-enum CUlaunchMemSyncDomain;\n-enum CUlibraryOption;\n-enum CUlimit;\n-enum CUmemAccess_flags;\n-enum CUmemAllocationCompType;\n-enum CUmemAllocationGranularity_flags;\n-enum CUmemAllocationHandleType;\n-enum CUmemAllocationType;\n-enum CUmemAttach_flags;\n-enum CUmemHandleType;\n-enum CUmemLocationType;\n-enum CUmemOperationType;\n-enum CUmemPool_attribute;\n-enum CUmemRangeHandleType;\n-enum CUmem_advise;\n-enum CUmemorytype;\n-enum CUmulticastGranularity_flags;\n-enum CUoccupancy_flags;\n-enum CUpointer_attribute;\n-enum CUresourceViewFormat;\n-enum CUresourcetype;\n-enum CUresult;\n-enum CUshared_carveout;\n-enum CUsharedconfig;\n-enum CUstreamBatchMemOpType;\n-enum CUstreamCaptureMode;\n-enum CUstreamCaptureStatus;\n-enum CUstreamMemoryBarrier_flags;\n-enum CUstreamUpdateCaptureDependencies_flags;\n-enum CUstreamWaitValue_flags;\n-enum CUstreamWriteValue_flags;\n-enum CUstream_flags;\n-enum CUtensorMapDataType;\n-enum CUtensorMapFloatOOBfill;\n-enum CUtensorMapInterleave;\n-enum CUtensorMapL2promotion;\n-enum CUtensorMapSwizzle;\n-enum CUuserObjectRetain_flags;\n-enum CUuserObject_flags;\n-enum cl_context_flags;\n-enum cl_event_flags;\n</code></pre>"},{"location":"manual/api-driver/#62-error-handling","title":"6.2. Error Handling","text":"<pre><code>CUresult cuGetErrorName(CUresult, const char * *);\nCUresult cuGetErrorString(CUresult, const char * *);\n</code></pre>"},{"location":"manual/api-driver/#63-initialization","title":"6.3. Initialization","text":"<pre><code>CUresult cuInit(unsigned);\n</code></pre>"},{"location":"manual/api-driver/#64-version-management","title":"6.4. Version Management","text":"<pre><code>CUresult cuDriverGetVersion(int *);\n</code></pre>"},{"location":"manual/api-driver/#65-device-management","title":"6.5. Device Management","text":"<pre><code>CUresult cuDeviceGet(CUdevice *, int);\nCUresult cuDeviceGetAttribute(int *, CUdevice_attribute, CUdevice);\nCUresult cuDeviceGetCount(int *);\n-CUresult cuDeviceGetDefaultMemPool(CUmemoryPool *, CUdevice);\n-CUresult cuDeviceGetExecAffinitySupport(int *, CUexecAffinityType, CUdevice);\nCUresult cuDeviceGetLuid(char *, unsigned *, CUdevice);\n-CUresult cuDeviceGetMemPool(CUmemoryPool *, CUdevice);\nCUresult cuDeviceGetName(char *, int, CUdevice);\n-CUresult cuDeviceGetNvSciSyncAttributes(void *, CUdevice, int);\n-CUresult cuDeviceGetTexture1DLinearMaxWidth(size_t *, CUarray_format, unsigned, CUdevice);\nCUresult cuDeviceGetUuid(CUuuid *, CUdevice);\nCUresult cuDeviceGetUuid_v2(CUuuid *, CUdevice);\n-CUresult cuDeviceSetMemPool(CUdevice, CUmemoryPool);\nCUresult cuDeviceTotalMem(size_t *, CUdevice);\n-CUresult cuFlushGPUDirectRDMAWrites(CUflushGPUDirectRDMAWritesTarget, CUflushGPUDirectRDMAWritesScope);\n</code></pre>"},{"location":"manual/api-driver/#66-device-management-deprecated","title":"6.6. Device Management [DEPRECATED]","text":"<pre><code>CUresult cuDeviceComputeCapability(int *, int *, CUdevice);\nCUresult cuDeviceGetProperties(CUdevprop *, CUdevice);\n</code></pre>"},{"location":"manual/api-driver/#67-primary-context-management","title":"6.7. Primary Context Management","text":"<pre><code>CUresult cuDevicePrimaryCtxGetState(CUdevice, unsigned *, int *);\nCUresult cuDevicePrimaryCtxRelease(CUdevice);\nCUresult cuDevicePrimaryCtxReset(CUdevice);\nCUresult cuDevicePrimaryCtxRetain(CUcontext *, CUdevice);\nCUresult cuDevicePrimaryCtxSetFlags(CUdevice, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#68-context-management","title":"6.8. Context Management","text":"<pre><code>CUresult cuCtxCreate(CUcontext *, unsigned, CUdevice);\n-CUresult cuCtxCreate_v3(CUcontext *, CUexecAffinityParam *, int, unsigned, CUdevice);\n-CUresult cuCtxCreate_v4(CUcontext *, CUctxCreateParams *, unsigned, CUdevice);\nCUresult cuCtxDestroy(CUcontext);\nCUresult cuCtxGetApiVersion(CUcontext, unsigned *);\nCUresult cuCtxGetCacheConfig(CUfunc_cache *);\nCUresult cuCtxGetCurrent(CUcontext *);\nCUresult cuCtxGetDevice(CUdevice *);\n-CUresult cuCtxGetExecAffinity(CUexecAffinityParam *, CUexecAffinityType);\nCUresult cuCtxGetFlags(unsigned *);\nCUresult cuCtxGetId(CUcontext, unsigned long long *);\nCUresult cuCtxGetLimit(size_t *, CUlimit);\nCUresult cuCtxGetStreamPriorityRange(int *, int *);\nCUresult cuCtxPopCurrent(CUcontext *);\nCUresult cuCtxPushCurrent(CUcontext);\n-CUresult cuCtxRecordEvent(CUcontext, CUevent);\nCUresult cuCtxResetPersistingL2Cache();\nCUresult cuCtxSetCacheConfig(CUfunc_cache);\nCUresult cuCtxSetCurrent(CUcontext);\nCUresult cuCtxSetFlags(unsigned);\nCUresult cuCtxSetLimit(CUlimit, size_t);\nCUresult cuCtxSynchronize();\n-CUresult cuCtxWaitEvent(CUcontext, CUevent);\n</code></pre>"},{"location":"manual/api-driver/#69-context-management-deprecated","title":"6.9. Context Management [DEPRECATED]","text":"<pre><code>CUresult cuCtxAttach(CUcontext *, unsigned);\nCUresult cuCtxDetach(CUcontext);\nCUresult cuCtxGetSharedMemConfig(CUsharedconfig *);\nCUresult cuCtxSetSharedMemConfig(CUsharedconfig);\n</code></pre>"},{"location":"manual/api-driver/#610-module-management","title":"6.10. Module Management","text":"<pre><code>-enum CUmoduleLoadingMode;\nCUresult cuLinkAddData(CUlinkState, CUjitInputType, void *, size_t, const char *, unsigned, CUjit_option *, void * *);\nCUresult cuLinkAddFile(CUlinkState, CUjitInputType, const char *, unsigned, CUjit_option *, void * *);\nCUresult cuLinkComplete(CUlinkState, void * *, size_t *);\nCUresult cuLinkCreate(unsigned, CUjit_option *, void * *, CUlinkState *);\nCUresult cuLinkDestroy(CUlinkState);\n-CUresult cuModuleEnumerateFunctions(CUfunction *, unsigned, CUmodule);\nCUresult cuModuleGetFunction(CUfunction *, CUmodule, const char *);\n-CUresult cuModuleGetFunctionCount(unsigned *, CUmodule);\nCUresult cuModuleGetGlobal(CUdeviceptr *, size_t *, CUmodule, const char *);\nCUresult cuModuleGetLoadingMode(CUmoduleLoadingMode *);\nCUresult cuModuleLoad(CUmodule *, const char *);\nCUresult cuModuleLoadData(CUmodule *, const void *);\nCUresult cuModuleLoadDataEx(CUmodule *, const void *, unsigned, CUjit_option *, void * *);\nCUresult cuModuleLoadFatBinary(CUmodule *, const void *);\nCUresult cuModuleUnload(CUmodule);\n</code></pre>"},{"location":"manual/api-driver/#611-module-management-deprecated","title":"6.11. Module Management [DEPRECATED]","text":"<pre><code>-CUresult cuModuleGetSurfRef(CUsurfref *, CUmodule, const char *);\n-CUresult cuModuleGetTexRef(CUtexref *, CUmodule, const char *);\n</code></pre>"},{"location":"manual/api-driver/#612-library-management","title":"6.12. Library Management","text":"<pre><code>-CUresult cuKernelGetAttribute(int *, CUfunction_attribute, CUkernel, CUdevice);\n-CUresult cuKernelGetFunction(CUfunction *, CUkernel);\n-CUresult cuKernelGetLibrary(CUlibrary *, CUkernel);\n-CUresult cuKernelGetName(const char * *, CUkernel);\n-CUresult cuKernelGetParamInfo(CUkernel, size_t, size_t *, size_t *);\n-CUresult cuKernelSetAttribute(CUfunction_attribute, int, CUkernel, CUdevice);\n-CUresult cuKernelSetCacheConfig(CUkernel, CUfunc_cache, CUdevice);\n-CUresult cuLibraryEnumerateKernels(CUkernel *, unsigned, CUlibrary);\n-CUresult cuLibraryGetGlobal(CUdeviceptr *, size_t *, CUlibrary, const char *);\n-CUresult cuLibraryGetKernel(CUkernel *, CUlibrary, const char *);\n-CUresult cuLibraryGetKernelCount(unsigned *, CUlibrary);\n-CUresult cuLibraryGetManaged(CUdeviceptr *, size_t *, CUlibrary, const char *);\n-CUresult cuLibraryGetModule(CUmodule *, CUlibrary);\n-CUresult cuLibraryGetUnifiedFunction(void * *, CUlibrary, const char *);\n-CUresult cuLibraryLoadData(CUlibrary *, const void *, CUjit_option *, void * *, unsigned, CUlibraryOption *, void * *, unsigned);\n-CUresult cuLibraryLoadFromFile(CUlibrary *, const char *, CUjit_option *, void * *, unsigned, CUlibraryOption *, void * *, unsigned);\n-CUresult cuLibraryUnload(CUlibrary);\n</code></pre>"},{"location":"manual/api-driver/#613-memory-management","title":"6.13. Memory Management","text":"<pre><code>-CUresult cuArray3DCreate(CUarray *, const CUDA_ARRAY3D_DESCRIPTOR *);\n-CUresult cuArray3DGetDescriptor(CUDA_ARRAY3D_DESCRIPTOR *, CUarray);\n-CUresult cuArrayCreate(CUarray *, const CUDA_ARRAY_DESCRIPTOR *);\nCUresult cuArrayDestroy(CUarray);\n-CUresult cuArrayGetDescriptor(CUDA_ARRAY_DESCRIPTOR *, CUarray);\n-CUresult cuArrayGetMemoryRequirements(CUDA_ARRAY_MEMORY_REQUIREMENTS *, CUarray, CUdevice);\n-CUresult cuArrayGetPlane(CUarray *, CUarray, unsigned);\n-CUresult cuArrayGetSparseProperties(CUDA_ARRAY_SPARSE_PROPERTIES *, CUarray);\nCUresult cuDeviceGetByPCIBusId(CUdevice *, const char *);\nCUresult cuDeviceGetPCIBusId(char *, int, CUdevice);\n-CUresult cuDeviceRegisterAsyncNotification(CUdevice, CUasyncCallback, void *, CUasyncCallbackHandle *);\n-CUresult cuDeviceUnregisterAsyncNotification(CUdevice, CUasyncCallbackHandle);\n-CUresult cuIpcCloseMemHandle(CUdeviceptr);\n-CUresult cuIpcGetEventHandle(CUipcEventHandle *, CUevent);\n-CUresult cuIpcGetMemHandle(CUipcMemHandle *, CUdeviceptr);\n-CUresult cuIpcOpenEventHandle(CUevent *, CUipcEventHandle);\n-CUresult cuIpcOpenMemHandle(CUdeviceptr *, CUipcMemHandle, unsigned);\nCUresult cuMemAlloc(CUdeviceptr *, size_t);\nCUresult cuMemAllocHost(void * *, size_t);\nCUresult cuMemAllocManaged(CUdeviceptr *, size_t, unsigned);\nCUresult cuMemAllocPitch(CUdeviceptr *, size_t *, size_t, size_t, unsigned);\nCUresult cuMemFree(CUdeviceptr);\nCUresult cuMemFreeHost(void *);\nCUresult cuMemGetAddressRange(CUdeviceptr *, size_t *, CUdeviceptr);\n-CUresult cuMemGetHandleForAddressRange(void *, CUdeviceptr, size_t, CUmemRangeHandleType, unsigned long long);\nCUresult cuMemGetInfo(size_t *, size_t *);\nCUresult cuMemHostAlloc(void * *, size_t, unsigned);\nCUresult cuMemHostGetDevicePointer(CUdeviceptr *, void *, unsigned);\n-CUresult cuMemHostGetFlags(unsigned *, void *);\nCUresult cuMemHostRegister(void *, size_t, unsigned);\nCUresult cuMemHostUnregister(void *);\nCUresult cuMemcpy(CUdeviceptr, CUdeviceptr, size_t);\nCUresult cuMemcpy2D(const CUDA_MEMCPY2D *);\nCUresult cuMemcpy2DAsync(const CUDA_MEMCPY2D *, CUstream);\nCUresult cuMemcpy2DUnaligned(const CUDA_MEMCPY2D *);\nCUresult cuMemcpy3D(const CUDA_MEMCPY3D *);\nCUresult cuMemcpy3DAsync(const CUDA_MEMCPY3D *, CUstream);\n-CUresult cuMemcpy3DPeer(const CUDA_MEMCPY3D_PEER *);\n-CUresult cuMemcpy3DPeerAsync(const CUDA_MEMCPY3D_PEER *, CUstream);\nCUresult cuMemcpyAsync(CUdeviceptr, CUdeviceptr, size_t, CUstream);\n-CUresult cuMemcpyAtoA(CUarray, size_t, CUarray, size_t, size_t);\n-CUresult cuMemcpyAtoD(CUdeviceptr, CUarray, size_t, size_t);\n-CUresult cuMemcpyAtoH(void *, CUarray, size_t, size_t);\n-CUresult cuMemcpyAtoHAsync(void *, CUarray, size_t, size_t, CUstream);\n-CUresult cuMemcpyDtoA(CUarray, size_t, CUdeviceptr, size_t);\nCUresult cuMemcpyDtoD(CUdeviceptr, CUdeviceptr, size_t);\nCUresult cuMemcpyDtoDAsync(CUdeviceptr, CUdeviceptr, size_t, CUstream);\nCUresult cuMemcpyDtoH(void *, CUdeviceptr, size_t);\nCUresult cuMemcpyDtoHAsync(void *, CUdeviceptr, size_t, CUstream);\n-CUresult cuMemcpyHtoA(CUarray, size_t, const void *, size_t);\n-CUresult cuMemcpyHtoAAsync(CUarray, size_t, const void *, size_t, CUstream);\nCUresult cuMemcpyHtoD(CUdeviceptr, const void *, size_t);\nCUresult cuMemcpyHtoDAsync(CUdeviceptr, const void *, size_t, CUstream);\n-CUresult cuMemcpyPeer(CUdeviceptr, CUcontext, CUdeviceptr, CUcontext, size_t);\n-CUresult cuMemcpyPeerAsync(CUdeviceptr, CUcontext, CUdeviceptr, CUcontext, size_t, CUstream);\nCUresult cuMemsetD16(CUdeviceptr, unsigned short, size_t);\nCUresult cuMemsetD16Async(CUdeviceptr, unsigned short, size_t, CUstream);\nCUresult cuMemsetD2D16(CUdeviceptr, size_t, unsigned short, size_t, size_t);\nCUresult cuMemsetD2D16Async(CUdeviceptr, size_t, unsigned short, size_t, size_t, CUstream);\nCUresult cuMemsetD2D32(CUdeviceptr, size_t, unsigned, size_t, size_t);\nCUresult cuMemsetD2D32Async(CUdeviceptr, size_t, unsigned, size_t, size_t, CUstream);\nCUresult cuMemsetD2D8(CUdeviceptr, size_t, unsigned char, size_t, size_t);\nCUresult cuMemsetD2D8Async(CUdeviceptr, size_t, unsigned char, size_t, size_t, CUstream);\nCUresult cuMemsetD32(CUdeviceptr, unsigned, size_t);\nCUresult cuMemsetD32Async(CUdeviceptr, unsigned, size_t, CUstream);\nCUresult cuMemsetD8(CUdeviceptr, unsigned char, size_t);\nCUresult cuMemsetD8Async(CUdeviceptr, unsigned char, size_t, CUstream);\n-CUresult cuMipmappedArrayCreate(CUmipmappedArray *, const CUDA_ARRAY3D_DESCRIPTOR *, unsigned);\n-CUresult cuMipmappedArrayDestroy(CUmipmappedArray);\n-CUresult cuMipmappedArrayGetLevel(CUarray *, CUmipmappedArray, unsigned);\n-CUresult cuMipmappedArrayGetMemoryRequirements(CUDA_ARRAY_MEMORY_REQUIREMENTS *, CUmipmappedArray, CUdevice);\n-CUresult cuMipmappedArrayGetSparseProperties(CUDA_ARRAY_SPARSE_PROPERTIES *, CUmipmappedArray);\n</code></pre>"},{"location":"manual/api-driver/#614-virtual-memory-management","title":"6.14. Virtual Memory Management","text":"<pre><code>CUresult cuMemAddressFree(CUdeviceptr, size_t);\nCUresult cuMemAddressReserve(CUdeviceptr *, size_t, size_t, CUdeviceptr, unsigned long long);\nCUresult cuMemCreate(CUmemGenericAllocationHandle *, size_t, const CUmemAllocationProp *, unsigned long long);\n-CUresult cuMemExportToShareableHandle(void *, CUmemGenericAllocationHandle, CUmemAllocationHandleType, unsigned long long);\nCUresult cuMemGetAccess(unsigned long long *, const CUmemLocation *, CUdeviceptr);\nCUresult cuMemGetAllocationGranularity(size_t *, const CUmemAllocationProp *, CUmemAllocationGranularity_flags);\nCUresult cuMemGetAllocationPropertiesFromHandle(CUmemAllocationProp *, CUmemGenericAllocationHandle);\n-CUresult cuMemImportFromShareableHandle(CUmemGenericAllocationHandle *, void *, CUmemAllocationHandleType);\nCUresult cuMemMap(CUdeviceptr, size_t, size_t, CUmemGenericAllocationHandle, unsigned long long);\n-CUresult cuMemMapArrayAsync(CUarrayMapInfo *, unsigned, CUstream);\nCUresult cuMemRelease(CUmemGenericAllocationHandle);\nCUresult cuMemRetainAllocationHandle(CUmemGenericAllocationHandle *, void *);\nCUresult cuMemSetAccess(CUdeviceptr, size_t, const CUmemAccessDesc *, size_t);\nCUresult cuMemUnmap(CUdeviceptr, size_t);\n</code></pre>"},{"location":"manual/api-driver/#615-stream-ordered-memory-allocator","title":"6.15. Stream Ordered Memory Allocator","text":"<pre><code>-CUresult cuMemAllocAsync(CUdeviceptr *, size_t, CUstream);\n-CUresult cuMemAllocFromPoolAsync(CUdeviceptr *, size_t, CUmemoryPool, CUstream);\n-CUresult cuMemFreeAsync(CUdeviceptr, CUstream);\n-CUresult cuMemPoolCreate(CUmemoryPool *, const CUmemPoolProps *);\n-CUresult cuMemPoolDestroy(CUmemoryPool);\n-CUresult cuMemPoolExportPointer(CUmemPoolPtrExportData *, CUdeviceptr);\n-CUresult cuMemPoolExportToShareableHandle(void *, CUmemoryPool, CUmemAllocationHandleType, unsigned long long);\n-CUresult cuMemPoolGetAccess(CUmemAccess_flags *, CUmemoryPool, CUmemLocation *);\n-CUresult cuMemPoolGetAttribute(CUmemoryPool, CUmemPool_attribute, void *);\n-CUresult cuMemPoolImportFromShareableHandle(CUmemoryPool *, void *, CUmemAllocationHandleType, unsigned long long);\n-CUresult cuMemPoolImportPointer(CUdeviceptr *, CUmemoryPool, CUmemPoolPtrExportData *);\n-CUresult cuMemPoolSetAccess(CUmemoryPool, const CUmemAccessDesc *, size_t);\n-CUresult cuMemPoolSetAttribute(CUmemoryPool, CUmemPool_attribute, void *);\n-CUresult cuMemPoolTrimTo(CUmemoryPool, size_t);\n</code></pre>"},{"location":"manual/api-driver/#616-multicast-object-management","title":"6.16. Multicast Object Management","text":"<pre><code>-CUresult cuMulticastAddDevice(CUmemGenericAllocationHandle, CUdevice);\n-CUresult cuMulticastBindAddr(CUmemGenericAllocationHandle, size_t, CUdeviceptr, size_t, unsigned long long);\n-CUresult cuMulticastBindMem(CUmemGenericAllocationHandle, size_t, CUmemGenericAllocationHandle, size_t, size_t, unsigned long long);\n-CUresult cuMulticastCreate(CUmemGenericAllocationHandle *, const CUmulticastObjectProp *);\n-CUresult cuMulticastGetGranularity(size_t *, const CUmulticastObjectProp *, CUmulticastGranularity_flags);\n-CUresult cuMulticastUnbind(CUmemGenericAllocationHandle, CUdevice, size_t, size_t);\n</code></pre>"},{"location":"manual/api-driver/#617-unified-addressing","title":"6.17. Unified Addressing","text":"<pre><code>CUresult cuMemAdvise(CUdeviceptr, size_t, CUmem_advise, CUdevice);\nCUresult cuMemAdvise_v2(CUdeviceptr, size_t, CUmem_advise, CUmemLocation);\nCUresult cuMemPrefetchAsync(CUdeviceptr, size_t, CUdevice, CUstream);\n-CUresult cuMemPrefetchAsync_v2(CUdeviceptr, size_t, CUmemLocation, unsigned, CUstream);\n-CUresult cuMemRangeGetAttribute(void *, size_t, CUmem_range_attribute, CUdeviceptr, size_t);\n-CUresult cuMemRangeGetAttributes(void * *, size_t *, CUmem_range_attribute *, size_t, CUdeviceptr, size_t);\nCUresult cuPointerGetAttribute(void *, CUpointer_attribute, CUdeviceptr);\nCUresult cuPointerGetAttributes(unsigned, CUpointer_attribute *, void * *, CUdeviceptr);\n-CUresult cuPointerSetAttribute(const void *, CUpointer_attribute, CUdeviceptr);\n</code></pre>"},{"location":"manual/api-driver/#618-stream-management","title":"6.18. Stream Management","text":"<pre><code>CUresult cuStreamAddCallback(CUstream, CUstreamCallback, void *, unsigned);\n-CUresult cuStreamAttachMemAsync(CUstream, CUdeviceptr, size_t, unsigned);\n-CUresult cuStreamBeginCapture(CUstream, CUstreamCaptureMode);\n-CUresult cuStreamBeginCaptureToGraph(CUstream, CUgraph, const CUgraphNode *, const CUgraphEdgeData *, size_t, CUstreamCaptureMode);\n-CUresult cuStreamCopyAttributes(CUstream, CUstream);\nCUresult cuStreamCreate(CUstream *, unsigned);\nCUresult cuStreamCreateWithPriority(CUstream *, unsigned, int);\nCUresult cuStreamDestroy(CUstream);\n-CUresult cuStreamEndCapture(CUstream, CUgraph *);\n-CUresult cuStreamGetAttribute(CUstream, CUstreamAttrID, CUstreamAttrValue *);\n-CUresult cuStreamGetCaptureInfo(CUstream, CUstreamCaptureStatus *, cuuint64_t *, CUgraph *, const CUgraphNode * *, size_t *);\n-CUresult cuStreamGetCaptureInfo_v3(CUstream, CUstreamCaptureStatus *, cuuint64_t *, CUgraph *, const CUgraphNode * *, const CUgraphEdgeData * *, size_t *);\nCUresult cuStreamGetCtx(CUstream, CUcontext *);\n-CUresult cuStreamGetCtx_v2(CUstream, CUcontext *, CUgreenCtx *);\nCUresult cuStreamGetFlags(CUstream, unsigned *);\nCUresult cuStreamGetId(CUstream, unsigned long long *);\nCUresult cuStreamGetPriority(CUstream, int *);\n-CUresult cuStreamIsCapturing(CUstream, CUstreamCaptureStatus *);\nCUresult cuStreamQuery(CUstream);\n-CUresult cuStreamSetAttribute(CUstream, CUstreamAttrID, const CUstreamAttrValue *);\nCUresult cuStreamSynchronize(CUstream);\n-CUresult cuStreamUpdateCaptureDependencies(CUstream, CUgraphNode *, size_t, unsigned);\n-CUresult cuStreamUpdateCaptureDependencies_v2(CUstream, CUgraphNode *, const CUgraphEdgeData *, size_t, unsigned);\nCUresult cuStreamWaitEvent(CUstream, CUevent, unsigned);\n-CUresult cuThreadExchangeStreamCaptureMode(CUstreamCaptureMode *);\n</code></pre>"},{"location":"manual/api-driver/#619-event-management","title":"6.19. Event Management","text":"<pre><code>CUresult cuEventCreate(CUevent *, unsigned);\nCUresult cuEventDestroy(CUevent);\nCUresult cuEventElapsedTime(float *, CUevent, CUevent);\nCUresult cuEventQuery(CUevent);\nCUresult cuEventRecord(CUevent, CUstream);\n-CUresult cuEventRecordWithFlags(CUevent, CUstream, unsigned);\nCUresult cuEventSynchronize(CUevent);\n</code></pre>"},{"location":"manual/api-driver/#620-external-resource-interoperability","title":"6.20. External Resource Interoperability","text":"<pre><code>-CUresult cuDestroyExternalMemory(CUexternalMemory);\n-CUresult cuDestroyExternalSemaphore(CUexternalSemaphore);\n-CUresult cuExternalMemoryGetMappedBuffer(CUdeviceptr *, CUexternalMemory, const CUDA_EXTERNAL_MEMORY_BUFFER_DESC *);\n-CUresult cuExternalMemoryGetMappedMipmappedArray(CUmipmappedArray *, CUexternalMemory, const CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC *);\n-CUresult cuImportExternalMemory(CUexternalMemory *, const CUDA_EXTERNAL_MEMORY_HANDLE_DESC *);\n-CUresult cuImportExternalSemaphore(CUexternalSemaphore *, const CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC *);\n-CUresult cuSignalExternalSemaphoresAsync(const CUexternalSemaphore *, const CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS *, unsigned, CUstream);\n-CUresult cuWaitExternalSemaphoresAsync(const CUexternalSemaphore *, const CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS *, unsigned, CUstream);\n</code></pre>"},{"location":"manual/api-driver/#621-stream-memory-operations","title":"6.21. Stream Memory Operations","text":"<pre><code>-CUresult cuStreamBatchMemOp(CUstream, unsigned, CUstreamBatchMemOpParams *, unsigned);\n-CUresult cuStreamWaitValue32(CUstream, CUdeviceptr, cuuint32_t, unsigned);\n-CUresult cuStreamWaitValue64(CUstream, CUdeviceptr, cuuint64_t, unsigned);\n-CUresult cuStreamWriteValue32(CUstream, CUdeviceptr, cuuint32_t, unsigned);\n-CUresult cuStreamWriteValue64(CUstream, CUdeviceptr, cuuint64_t, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#622-execution-control","title":"6.22. Execution Control","text":"<pre><code>CUresult cuFuncGetAttribute(int *, CUfunction_attribute, CUfunction);\n-CUresult cuFuncGetModule(CUmodule *, CUfunction);\n-CUresult cuFuncGetName(const char * *, CUfunction);\n-CUresult cuFuncGetParamInfo(CUfunction, size_t, size_t *, size_t *);\n-CUresult cuFuncIsLoaded(CUfunctionLoadingState *, CUfunction);\n-CUresult cuFuncLoad(CUfunction);\nCUresult cuFuncSetAttribute(CUfunction, CUfunction_attribute, int);\nCUresult cuFuncSetCacheConfig(CUfunction, CUfunc_cache);\n-CUresult cuLaunchCooperativeKernel(CUfunction, unsigned, unsigned, unsigned, unsigned, unsigned, unsigned, unsigned, CUstream, void * *);\n-CUresult cuLaunchCooperativeKernelMultiDevice(CUDA_LAUNCH_PARAMS *, unsigned, unsigned);\nCUresult cuLaunchHostFunc(CUstream, CUhostFn, void *);\n-CUresult cuLaunchKernel(CUfunction, unsigned, unsigned, unsigned, unsigned, unsigned, unsigned, unsigned, CUstream, void * *, void * *);\nCUresult cuLaunchKernelEx(const CUlaunchConfig *, CUfunction, void * *, void * *);\n</code></pre>"},{"location":"manual/api-driver/#623-execution-control-deprecated","title":"6.23. Execution Control [DEPRECATED]","text":"<pre><code>-CUresult cuFuncSetBlockShape(CUfunction, int, int, int);\nCUresult cuFuncSetSharedMemConfig(CUfunction, CUsharedconfig);\n-CUresult cuFuncSetSharedSize(CUfunction, unsigned);\n-CUresult cuLaunch(CUfunction);\n-CUresult cuLaunchGrid(CUfunction, int, int);\n-CUresult cuLaunchGridAsync(CUfunction, int, int, CUstream);\n-CUresult cuParamSetSize(CUfunction, unsigned);\n-CUresult cuParamSetTexRef(CUfunction, int, CUtexref);\n-CUresult cuParamSetf(CUfunction, int, float);\n-CUresult cuParamSeti(CUfunction, int, unsigned);\n-CUresult cuParamSetv(CUfunction, int, void *, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#624-graph-management","title":"6.24. Graph Management","text":"<pre><code>-CUresult cuDeviceGetGraphMemAttribute(CUdevice, CUgraphMem_attribute, void *);\n-CUresult cuDeviceGraphMemTrim(CUdevice);\n-CUresult cuDeviceSetGraphMemAttribute(CUdevice, CUgraphMem_attribute, void *);\n-CUresult cuGraphAddBatchMemOpNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, const CUDA_BATCH_MEM_OP_NODE_PARAMS *);\n-CUresult cuGraphAddChildGraphNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, CUgraph);\n-CUresult cuGraphAddDependencies(CUgraph, const CUgraphNode *, const CUgraphNode *, size_t);\n-CUresult cuGraphAddDependencies_v2(CUgraph, const CUgraphNode *, const CUgraphNode *, const CUgraphEdgeData *, size_t);\n-CUresult cuGraphAddEmptyNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t);\n-CUresult cuGraphAddEventRecordNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, CUevent);\n-CUresult cuGraphAddEventWaitNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, CUevent);\n-CUresult cuGraphAddExternalSemaphoresSignalNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS *);\n-CUresult cuGraphAddExternalSemaphoresWaitNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, const CUDA_EXT_SEM_WAIT_NODE_PARAMS *);\n-CUresult cuGraphAddHostNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, const CUDA_HOST_NODE_PARAMS *);\n-CUresult cuGraphAddKernelNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, const CUDA_KERNEL_NODE_PARAMS *);\n-CUresult cuGraphAddMemAllocNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, CUDA_MEM_ALLOC_NODE_PARAMS *);\n-CUresult cuGraphAddMemFreeNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, CUdeviceptr);\n-CUresult cuGraphAddMemcpyNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, const CUDA_MEMCPY3D *, CUcontext);\n-CUresult cuGraphAddMemsetNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, const CUDA_MEMSET_NODE_PARAMS *, CUcontext);\n-CUresult cuGraphAddNode(CUgraphNode *, CUgraph, const CUgraphNode *, size_t, CUgraphNodeParams *);\n-CUresult cuGraphAddNode_v2(CUgraphNode *, CUgraph, const CUgraphNode *, const CUgraphEdgeData *, size_t, CUgraphNodeParams *);\n-CUresult cuGraphBatchMemOpNodeGetParams(CUgraphNode, CUDA_BATCH_MEM_OP_NODE_PARAMS *);\n-CUresult cuGraphBatchMemOpNodeSetParams(CUgraphNode, const CUDA_BATCH_MEM_OP_NODE_PARAMS *);\n-CUresult cuGraphChildGraphNodeGetGraph(CUgraphNode, CUgraph *);\n-CUresult cuGraphClone(CUgraph *, CUgraph);\n-CUresult cuGraphConditionalHandleCreate(CUgraphConditionalHandle *, CUgraph, CUcontext, unsigned, unsigned);\n-CUresult cuGraphCreate(CUgraph *, unsigned);\n-CUresult cuGraphDebugDotPrint(CUgraph, const char *, unsigned);\n-CUresult cuGraphDestroy(CUgraph);\n-CUresult cuGraphDestroyNode(CUgraphNode);\n-CUresult cuGraphEventRecordNodeGetEvent(CUgraphNode, CUevent *);\n-CUresult cuGraphEventRecordNodeSetEvent(CUgraphNode, CUevent);\n-CUresult cuGraphEventWaitNodeGetEvent(CUgraphNode, CUevent *);\n-CUresult cuGraphEventWaitNodeSetEvent(CUgraphNode, CUevent);\n-CUresult cuGraphExecBatchMemOpNodeSetParams(CUgraphExec, CUgraphNode, const CUDA_BATCH_MEM_OP_NODE_PARAMS *);\n-CUresult cuGraphExecChildGraphNodeSetParams(CUgraphExec, CUgraphNode, CUgraph);\n-CUresult cuGraphExecDestroy(CUgraphExec);\n-CUresult cuGraphExecEventRecordNodeSetEvent(CUgraphExec, CUgraphNode, CUevent);\n-CUresult cuGraphExecEventWaitNodeSetEvent(CUgraphExec, CUgraphNode, CUevent);\n-CUresult cuGraphExecExternalSemaphoresSignalNodeSetParams(CUgraphExec, CUgraphNode, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS *);\n-CUresult cuGraphExecExternalSemaphoresWaitNodeSetParams(CUgraphExec, CUgraphNode, const CUDA_EXT_SEM_WAIT_NODE_PARAMS *);\n-CUresult cuGraphExecGetFlags(CUgraphExec, cuuint64_t *);\n-CUresult cuGraphExecHostNodeSetParams(CUgraphExec, CUgraphNode, const CUDA_HOST_NODE_PARAMS *);\n-CUresult cuGraphExecKernelNodeSetParams(CUgraphExec, CUgraphNode, const CUDA_KERNEL_NODE_PARAMS *);\n-CUresult cuGraphExecMemcpyNodeSetParams(CUgraphExec, CUgraphNode, const CUDA_MEMCPY3D *, CUcontext);\n-CUresult cuGraphExecMemsetNodeSetParams(CUgraphExec, CUgraphNode, const CUDA_MEMSET_NODE_PARAMS *, CUcontext);\n-CUresult cuGraphExecNodeSetParams(CUgraphExec, CUgraphNode, CUgraphNodeParams *);\n-CUresult cuGraphExecUpdate(CUgraphExec, CUgraph, CUgraphExecUpdateResultInfo *);\n-CUresult cuGraphExternalSemaphoresSignalNodeGetParams(CUgraphNode, CUDA_EXT_SEM_SIGNAL_NODE_PARAMS *);\n-CUresult cuGraphExternalSemaphoresSignalNodeSetParams(CUgraphNode, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS *);\n-CUresult cuGraphExternalSemaphoresWaitNodeGetParams(CUgraphNode, CUDA_EXT_SEM_WAIT_NODE_PARAMS *);\n-CUresult cuGraphExternalSemaphoresWaitNodeSetParams(CUgraphNode, const CUDA_EXT_SEM_WAIT_NODE_PARAMS *);\n-CUresult cuGraphGetEdges(CUgraph, CUgraphNode *, CUgraphNode *, size_t *);\n-CUresult cuGraphGetEdges_v2(CUgraph, CUgraphNode *, CUgraphNode *, CUgraphEdgeData *, size_t *);\n-CUresult cuGraphGetNodes(CUgraph, CUgraphNode *, size_t *);\n-CUresult cuGraphGetRootNodes(CUgraph, CUgraphNode *, size_t *);\n-CUresult cuGraphHostNodeGetParams(CUgraphNode, CUDA_HOST_NODE_PARAMS *);\n-CUresult cuGraphHostNodeSetParams(CUgraphNode, const CUDA_HOST_NODE_PARAMS *);\n-CUresult cuGraphInstantiate(CUgraphExec *, CUgraph, unsigned long long);\n-CUresult cuGraphInstantiateWithParams(CUgraphExec *, CUgraph, CUDA_GRAPH_INSTANTIATE_PARAMS *);\n-CUresult cuGraphKernelNodeCopyAttributes(CUgraphNode, CUgraphNode);\n-CUresult cuGraphKernelNodeGetAttribute(CUgraphNode, CUkernelNodeAttrID, CUkernelNodeAttrValue *);\n-CUresult cuGraphKernelNodeGetParams(CUgraphNode, CUDA_KERNEL_NODE_PARAMS *);\n-CUresult cuGraphKernelNodeSetAttribute(CUgraphNode, CUkernelNodeAttrID, const CUkernelNodeAttrValue *);\n-CUresult cuGraphKernelNodeSetParams(CUgraphNode, const CUDA_KERNEL_NODE_PARAMS *);\n-CUresult cuGraphLaunch(CUgraphExec, CUstream);\n-CUresult cuGraphMemAllocNodeGetParams(CUgraphNode, CUDA_MEM_ALLOC_NODE_PARAMS *);\n-CUresult cuGraphMemFreeNodeGetParams(CUgraphNode, CUdeviceptr *);\n-CUresult cuGraphMemcpyNodeGetParams(CUgraphNode, CUDA_MEMCPY3D *);\n-CUresult cuGraphMemcpyNodeSetParams(CUgraphNode, const CUDA_MEMCPY3D *);\n-CUresult cuGraphMemsetNodeGetParams(CUgraphNode, CUDA_MEMSET_NODE_PARAMS *);\n-CUresult cuGraphMemsetNodeSetParams(CUgraphNode, const CUDA_MEMSET_NODE_PARAMS *);\n-CUresult cuGraphNodeFindInClone(CUgraphNode *, CUgraphNode, CUgraph);\n-CUresult cuGraphNodeGetDependencies(CUgraphNode, CUgraphNode *, size_t *);\n-CUresult cuGraphNodeGetDependencies_v2(CUgraphNode, CUgraphNode *, CUgraphEdgeData *, size_t *);\n-CUresult cuGraphNodeGetDependentNodes(CUgraphNode, CUgraphNode *, size_t *);\n-CUresult cuGraphNodeGetDependentNodes_v2(CUgraphNode, CUgraphNode *, CUgraphEdgeData *, size_t *);\n-CUresult cuGraphNodeGetEnabled(CUgraphExec, CUgraphNode, unsigned *);\n-CUresult cuGraphNodeGetType(CUgraphNode, CUgraphNodeType *);\n-CUresult cuGraphNodeSetEnabled(CUgraphExec, CUgraphNode, unsigned);\n-CUresult cuGraphNodeSetParams(CUgraphNode, CUgraphNodeParams *);\n-CUresult cuGraphReleaseUserObject(CUgraph, CUuserObject, unsigned);\n-CUresult cuGraphRemoveDependencies(CUgraph, const CUgraphNode *, const CUgraphNode *, size_t);\n-CUresult cuGraphRemoveDependencies_v2(CUgraph, const CUgraphNode *, const CUgraphNode *, const CUgraphEdgeData *, size_t);\n-CUresult cuGraphRetainUserObject(CUgraph, CUuserObject, unsigned, unsigned);\n-CUresult cuGraphUpload(CUgraphExec, CUstream);\n-CUresult cuUserObjectCreate(CUuserObject *, void *, CUhostFn, unsigned, unsigned);\n-CUresult cuUserObjectRelease(CUuserObject, unsigned);\n-CUresult cuUserObjectRetain(CUuserObject, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#625-occupancy","title":"6.25. Occupancy","text":"<pre><code>CUresult cuOccupancyAvailableDynamicSMemPerBlock(size_t *, CUfunction, int, int);\nCUresult cuOccupancyMaxActiveBlocksPerMultiprocessor(int *, CUfunction, int, size_t);\nCUresult cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int *, CUfunction, int, size_t, unsigned);\n-CUresult cuOccupancyMaxActiveClusters(int *, CUfunction, const CUlaunchConfig *);\nCUresult cuOccupancyMaxPotentialBlockSize(int *, int *, CUfunction, CUoccupancyB2DSize, size_t, int);\nCUresult cuOccupancyMaxPotentialBlockSizeWithFlags(int *, int *, CUfunction, CUoccupancyB2DSize, size_t, int, unsigned);\n-CUresult cuOccupancyMaxPotentialClusterSize(int *, CUfunction, const CUlaunchConfig *);\n</code></pre>"},{"location":"manual/api-driver/#626-texture-reference-management-deprecated","title":"6.26. Texture Reference Management [DEPRECATED]","text":"<pre><code>CUresult cuTexRefCreate(CUtexref *);\nCUresult cuTexRefDestroy(CUtexref);\nCUresult cuTexRefGetAddress(CUdeviceptr *, CUtexref);\nCUresult cuTexRefGetAddressMode(CUaddress_mode *, CUtexref, int);\nCUresult cuTexRefGetArray(CUarray *, CUtexref);\n-CUresult cuTexRefGetBorderColor(float *, CUtexref);\nCUresult cuTexRefGetFilterMode(CUfilter_mode *, CUtexref);\n-CUresult cuTexRefGetFlags(unsigned *, CUtexref);\n-CUresult cuTexRefGetFormat(CUarray_format *, int *, CUtexref);\n-CUresult cuTexRefGetMaxAnisotropy(int *, CUtexref);\n-CUresult cuTexRefGetMipmapFilterMode(CUfilter_mode *, CUtexref);\n-CUresult cuTexRefGetMipmapLevelBias(float *, CUtexref);\n-CUresult cuTexRefGetMipmapLevelClamp(float *, float *, CUtexref);\n-CUresult cuTexRefGetMipmappedArray(CUmipmappedArray *, CUtexref);\n-CUresult cuTexRefSetAddress(size_t *, CUtexref, CUdeviceptr, size_t);\n-CUresult cuTexRefSetAddress2D(CUtexref, const CUDA_ARRAY_DESCRIPTOR *, CUdeviceptr, size_t);\n-CUresult cuTexRefSetAddressMode(CUtexref, int, CUaddress_mode);\n-CUresult cuTexRefSetArray(CUtexref, CUarray, unsigned);\n-CUresult cuTexRefSetBorderColor(CUtexref, float *);\n-CUresult cuTexRefSetFilterMode(CUtexref, CUfilter_mode);\n-CUresult cuTexRefSetFlags(CUtexref, unsigned);\n-CUresult cuTexRefSetFormat(CUtexref, CUarray_format, int);\n-CUresult cuTexRefSetMaxAnisotropy(CUtexref, unsigned);\n-CUresult cuTexRefSetMipmapFilterMode(CUtexref, CUfilter_mode);\n-CUresult cuTexRefSetMipmapLevelBias(CUtexref, float);\n-CUresult cuTexRefSetMipmapLevelClamp(CUtexref, float, float);\n-CUresult cuTexRefSetMipmappedArray(CUtexref, CUmipmappedArray, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#627-surface-reference-management-deprecated","title":"6.27. Surface Reference Management [DEPRECATED]","text":"<pre><code>-CUresult cuSurfRefGetArray(CUarray *, CUsurfref);\n-CUresult cuSurfRefSetArray(CUsurfref, CUarray, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#628-texture-object-management","title":"6.28. Texture Object Management","text":"<pre><code>CUresult cuTexObjectCreate(CUtexObject *, const CUDA_RESOURCE_DESC *, const CUDA_TEXTURE_DESC *, const CUDA_RESOURCE_VIEW_DESC *);\nCUresult cuTexObjectDestroy(CUtexObject);\n-CUresult cuTexObjectGetResourceDesc(CUDA_RESOURCE_DESC *, CUtexObject);\n-CUresult cuTexObjectGetResourceViewDesc(CUDA_RESOURCE_VIEW_DESC *, CUtexObject);\n-CUresult cuTexObjectGetTextureDesc(CUDA_TEXTURE_DESC *, CUtexObject);\n</code></pre>"},{"location":"manual/api-driver/#629-surface-object-management","title":"6.29. Surface Object Management","text":"<pre><code>-CUresult cuSurfObjectCreate(CUsurfObject *, const CUDA_RESOURCE_DESC *);\n-CUresult cuSurfObjectDestroy(CUsurfObject);\n-CUresult cuSurfObjectGetResourceDesc(CUDA_RESOURCE_DESC *, CUsurfObject);\n</code></pre>"},{"location":"manual/api-driver/#630-tensor-map-object-managment","title":"6.30. Tensor Map Object Managment","text":"<pre><code>-CUresult cuTensorMapEncodeIm2col(CUtensorMap *, CUtensorMapDataType, cuuint32_t, void *, const cuuint64_t *, const cuuint64_t *, const int *, const int *, cuuint32_t, cuuint32_t, const cuuint32_t *, CUtensorMapInterleave, CUtensorMapSwizzle, CUtensorMapL2promotion, CUtensorMapFloatOOBfill);\n-CUresult cuTensorMapEncodeTiled(CUtensorMap *, CUtensorMapDataType, cuuint32_t, void *, const cuuint64_t *, const cuuint64_t *, const cuuint32_t *, const cuuint32_t *, CUtensorMapInterleave, CUtensorMapSwizzle, CUtensorMapL2promotion, CUtensorMapFloatOOBfill);\n-CUresult cuTensorMapReplaceAddress(CUtensorMap *, void *);\n</code></pre>"},{"location":"manual/api-driver/#631-peer-context-memory-access","title":"6.31. Peer Context Memory Access","text":"<pre><code>-CUresult cuCtxDisablePeerAccess(CUcontext);\n-CUresult cuCtxEnablePeerAccess(CUcontext, unsigned);\n-CUresult cuDeviceCanAccessPeer(int *, CUdevice, CUdevice);\n-CUresult cuDeviceGetP2PAttribute(int *, CUdevice_P2PAttribute, CUdevice, CUdevice);\n</code></pre>"},{"location":"manual/api-driver/#632-graphics-interoperability","title":"6.32. Graphics Interoperability","text":"<pre><code>-CUresult cuGraphicsMapResources(unsigned, CUgraphicsResource *, CUstream);\n-CUresult cuGraphicsResourceGetMappedMipmappedArray(CUmipmappedArray *, CUgraphicsResource);\n-CUresult cuGraphicsResourceGetMappedPointer(CUdeviceptr *, size_t *, CUgraphicsResource);\n-CUresult cuGraphicsResourceSetMapFlags(CUgraphicsResource, unsigned);\n-CUresult cuGraphicsSubResourceGetMappedArray(CUarray *, CUgraphicsResource, unsigned, unsigned);\n-CUresult cuGraphicsUnmapResources(unsigned, CUgraphicsResource *, CUstream);\n-CUresult cuGraphicsUnregisterResource(CUgraphicsResource);\n</code></pre>"},{"location":"manual/api-driver/#633-driver-entry-point-access","title":"6.33. Driver Entry Point Access","text":"<pre><code>-CUresult cuGetProcAddress(const char *, void * *, int, cuuint64_t, CUdriverProcAddressQueryResult *);\n</code></pre>"},{"location":"manual/api-driver/#634-coredump-attributes-control-api","title":"6.34. Coredump Attributes Control API","text":"<pre><code>-enum CUCoredumpGenerationFlags;\n-enum CUcoredumpSettings;\n-CUresult cuCoredumpGetAttribute(CUcoredumpSettings, void *, size_t *);\n-CUresult cuCoredumpGetAttributeGlobal(CUcoredumpSettings, void *, size_t *);\n-CUresult cuCoredumpSetAttribute(CUcoredumpSettings, void *, size_t *);\n-CUresult cuCoredumpSetAttributeGlobal(CUcoredumpSettings, void *, size_t *);\n</code></pre>"},{"location":"manual/api-driver/#635-green-contexts","title":"6.35. Green Contexts","text":"<pre><code>-struct CUdevResource;\n-struct CUdevSmResource;\n-typedef CUdevResourceDesc_st * CUdevResourceDesc;\n-enum CUdevResourceType;\n-CUresult cuCtxFromGreenCtx(CUcontext *, CUgreenCtx);\n-CUresult cuCtxGetDevResource(CUcontext, CUdevResource *, CUdevResourceType);\n-CUresult cuDevResourceGenerateDesc(CUdevResourceDesc *, CUdevResource *, unsigned);\n-CUresult cuDevSmResourceSplitByCount(CUdevResource *, unsigned *, const CUdevResource *, CUdevResource *, unsigned, unsigned);\n-CUresult cuDeviceGetDevResource(CUdevice, CUdevResource *, CUdevResourceType);\n-CUresult cuGreenCtxCreate(CUgreenCtx *, CUdevResourceDesc, CUdevice, unsigned);\n-CUresult cuGreenCtxDestroy(CUgreenCtx);\n-CUresult cuGreenCtxGetDevResource(CUgreenCtx, CUdevResource *, CUdevResourceType);\n-CUresult cuGreenCtxRecordEvent(CUgreenCtx, CUevent);\n-CUresult cuGreenCtxStreamCreate(CUstream *, CUgreenCtx, unsigned, int);\n-CUresult cuGreenCtxWaitEvent(CUgreenCtx, CUevent);\n-CUresult cuStreamGetGreenCtx(CUstream, CUgreenCtx *);\n</code></pre>"},{"location":"manual/api-driver/#636-profiler-control-deprecated","title":"6.36. Profiler Control [DEPRECATED]","text":"<pre><code>CUresult cuProfilerInitialize(const char *, const char *, CUoutput_mode);\n</code></pre>"},{"location":"manual/api-driver/#637-profiler-control","title":"6.37. Profiler Control","text":"<pre><code>CUresult cuProfilerStart();\nCUresult cuProfilerStop();\n</code></pre>"},{"location":"manual/api-driver/#638-opengl-interoperability","title":"6.38. OpenGL Interoperability","text":"<pre><code>-enum CUGLDeviceList;\n-CUresult cuGLGetDevices(unsigned *, CUdevice *, unsigned, CUGLDeviceList);\n-CUresult cuGraphicsGLRegisterBuffer(CUgraphicsResource *, GLuint, unsigned);\n-CUresult cuGraphicsGLRegisterImage(CUgraphicsResource *, GLuint, GLenum, unsigned);\n-CUresult cuWGLGetDevice(CUdevice *, HGPUNV);\n</code></pre>"},{"location":"manual/api-driver/#6381-opengl-interoperability-deprecated","title":"6.38.1. OpenGL Interoperability [DEPRECATED]","text":"<pre><code>-enum CUGLmap_flags;\n-CUresult cuGLCtxCreate(CUcontext *, unsigned, CUdevice);\n-CUresult cuGLInit();\n-CUresult cuGLMapBufferObject(CUdeviceptr *, size_t *, GLuint);\n-CUresult cuGLMapBufferObjectAsync(CUdeviceptr *, size_t *, GLuint, CUstream);\n-CUresult cuGLRegisterBufferObject(GLuint);\n-CUresult cuGLSetBufferObjectMapFlags(GLuint, unsigned);\n-CUresult cuGLUnmapBufferObject(GLuint);\n-CUresult cuGLUnmapBufferObjectAsync(GLuint, CUstream);\n-CUresult cuGLUnregisterBufferObject(GLuint);\n</code></pre>"},{"location":"manual/api-driver/#639-direct3d-9-interoperability","title":"6.39. Direct3D 9 Interoperability","text":"<pre><code>-enum CUd3d9DeviceList;\n-CUresult cuD3D9CtxCreate(CUcontext *, CUdevice *, unsigned, IDirect3DDevice9 *);\n-CUresult cuD3D9CtxCreateOnDevice(CUcontext *, unsigned, IDirect3DDevice9 *, CUdevice);\n-CUresult cuD3D9GetDevice(CUdevice *, const char *);\n-CUresult cuD3D9GetDevices(unsigned *, CUdevice *, unsigned, IDirect3DDevice9 *, CUd3d9DeviceList);\n-CUresult cuD3D9GetDirect3DDevice(IDirect3DDevice9 * *);\n-CUresult cuGraphicsD3D9RegisterResource(CUgraphicsResource *, IDirect3DResource9 *, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#6391-direct3d-9-interoperability-deprecated","title":"6.39.1. Direct3D 9 Interoperability [DEPRECATED]","text":"<pre><code>-enum CUd3d9map_flags;\n-enum CUd3d9register_flags;\n-CUresult cuD3D9MapResources(unsigned, IDirect3DResource9 * *);\n-CUresult cuD3D9RegisterResource(IDirect3DResource9 *, unsigned);\n-CUresult cuD3D9ResourceGetMappedArray(CUarray *, IDirect3DResource9 *, unsigned, unsigned);\n-CUresult cuD3D9ResourceGetMappedPitch(size_t *, size_t *, IDirect3DResource9 *, unsigned, unsigned);\n-CUresult cuD3D9ResourceGetMappedPointer(CUdeviceptr *, IDirect3DResource9 *, unsigned, unsigned);\n-CUresult cuD3D9ResourceGetMappedSize(size_t *, IDirect3DResource9 *, unsigned, unsigned);\n-CUresult cuD3D9ResourceGetSurfaceDimensions(size_t *, size_t *, size_t *, IDirect3DResource9 *, unsigned, unsigned);\n-CUresult cuD3D9ResourceSetMapFlags(IDirect3DResource9 *, unsigned);\n-CUresult cuD3D9UnmapResources(unsigned, IDirect3DResource9 * *);\n-CUresult cuD3D9UnregisterResource(IDirect3DResource9 *);\n</code></pre>"},{"location":"manual/api-driver/#640-direct3d-10-interoperability","title":"6.40. Direct3D 10 Interoperability","text":"<pre><code>-enum CUd3d10DeviceList;\n-CUresult cuD3D10GetDevice(CUdevice *, IDXGIAdapter *);\n-CUresult cuD3D10GetDevices(unsigned *, CUdevice *, unsigned, ID3D10Device *, CUd3d10DeviceList);\n-CUresult cuGraphicsD3D10RegisterResource(CUgraphicsResource *, ID3D10Resource *, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#6401-direct3d-10-interoperability-deprecated","title":"6.40.1. Direct3D 10 Interoperability [DEPRECATED]","text":"<pre><code>-enum CUD3D10map_flags;\n-enum CUD3D10register_flags;\n-CUresult cuD3D10CtxCreate(CUcontext *, CUdevice *, unsigned, ID3D10Device *);\n-CUresult cuD3D10CtxCreateOnDevice(CUcontext *, unsigned, ID3D10Device *, CUdevice);\n-CUresult cuD3D10GetDirect3DDevice(ID3D10Device * *);\n-CUresult cuD3D10MapResources(unsigned, ID3D10Resource * *);\n-CUresult cuD3D10RegisterResource(ID3D10Resource *, unsigned);\n-CUresult cuD3D10ResourceGetMappedArray(CUarray *, ID3D10Resource *, unsigned);\n-CUresult cuD3D10ResourceGetMappedPitch(size_t *, size_t *, ID3D10Resource *, unsigned);\n-CUresult cuD3D10ResourceGetMappedPointer(CUdeviceptr *, ID3D10Resource *, unsigned);\n-CUresult cuD3D10ResourceGetMappedSize(size_t *, ID3D10Resource *, unsigned);\n-CUresult cuD3D10ResourceGetSurfaceDimensions(size_t *, size_t *, size_t *, ID3D10Resource *, unsigned);\n-CUresult cuD3D10ResourceSetMapFlags(ID3D10Resource *, unsigned);\n-CUresult cuD3D10UnmapResources(unsigned, ID3D10Resource * *);\n-CUresult cuD3D10UnregisterResource(ID3D10Resource *);\n</code></pre>"},{"location":"manual/api-driver/#641-direct3d-11-interoperability","title":"6.41. Direct3D 11 Interoperability","text":"<pre><code>-enum CUd3d11DeviceList;\n-CUresult cuD3D11GetDevice(CUdevice *, IDXGIAdapter *);\n-CUresult cuD3D11GetDevices(unsigned *, CUdevice *, unsigned, ID3D11Device *, CUd3d11DeviceList);\n-CUresult cuGraphicsD3D11RegisterResource(CUgraphicsResource *, ID3D11Resource *, unsigned);\n</code></pre>"},{"location":"manual/api-driver/#6411-direct3d-11-interoperability-deprecated","title":"6.41.1. Direct3D 11 Interoperability [DEPRECATED]","text":"<pre><code>-CUresult cuD3D11CtxCreate(CUcontext *, CUdevice *, unsigned, ID3D11Device *);\n-CUresult cuD3D11CtxCreateOnDevice(CUcontext *, unsigned, ID3D11Device *, CUdevice);\n-CUresult cuD3D11GetDirect3DDevice(ID3D11Device * *);\n</code></pre>"},{"location":"manual/api-driver/#642-vdpau-interoperability","title":"6.42. VDPAU Interoperability","text":"<pre><code>-CUresult cuGraphicsVDPAURegisterOutputSurface(CUgraphicsResource *, VdpOutputSurface, unsigned);\n-CUresult cuGraphicsVDPAURegisterVideoSurface(CUgraphicsResource *, VdpVideoSurface, unsigned);\n-CUresult cuVDPAUCtxCreate(CUcontext *, unsigned, CUdevice, VdpDevice, VdpGetProcAddress *);\n-CUresult cuVDPAUGetDevice(CUdevice *, VdpDevice, VdpGetProcAddress *);\n</code></pre>"},{"location":"manual/api-driver/#643-egl-interoperability","title":"6.43. EGL Interoperability","text":"<pre><code>-CUresult cuEGLStreamConsumerAcquireFrame(CUeglStreamConnection *, CUgraphicsResource *, CUstream *, unsigned);\n-CUresult cuEGLStreamConsumerConnect(CUeglStreamConnection *, EGLStreamKHR);\n-CUresult cuEGLStreamConsumerConnectWithFlags(CUeglStreamConnection *, EGLStreamKHR, unsigned);\n-CUresult cuEGLStreamConsumerDisconnect(CUeglStreamConnection *);\n-CUresult cuEGLStreamConsumerReleaseFrame(CUeglStreamConnection *, CUgraphicsResource, CUstream *);\n-CUresult cuEGLStreamProducerConnect(CUeglStreamConnection *, EGLStreamKHR, EGLint, EGLint);\n-CUresult cuEGLStreamProducerDisconnect(CUeglStreamConnection *);\n-CUresult cuEGLStreamProducerPresentFrame(CUeglStreamConnection *, CUeglFrame, CUstream *);\n-CUresult cuEGLStreamProducerReturnFrame(CUeglStreamConnection *, CUeglFrame *, CUstream *);\n-CUresult cuEventCreateFromEGLSync(CUevent *, EGLSyncKHR, unsigned);\n-CUresult cuGraphicsEGLRegisterImage(CUgraphicsResource *, EGLImageKHR, unsigned);\n-CUresult cuGraphicsResourceGetMappedEglFrame(CUeglFrame *, CUgraphicsResource, unsigned, unsigned);\n</code></pre>"},{"location":"manual/api-math/","title":"Math API","text":"<p>Read how this document is structured in the Introduction to implemented APIs.</p>"},{"location":"manual/api-math/#11-fp8-intrinsics","title":"1.1. FP8 Intrinsics","text":"<p>No matches in this section.</p>"},{"location":"manual/api-math/#111-fp8-conversion-and-data-movement","title":"1.1.1. FP8 Conversion and Data Movement","text":"<pre><code>-typedef unsigned char __nv_fp8_storage_t;\n-typedef unsigned short int __nv_fp8x2_storage_t;\n-typedef unsigned int __nv_fp8x4_storage_t;\n-enum __nv_fp8_interpretation_t;\n-enum __nv_saturation_t;\n-__nv_fp8x2_storage_t __nv_cvt_bfloat16raw2_to_fp8x2(__nv_bfloat162_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8x2_storage_t __nv_cvt_bfloat16raw2_to_fp8x2(__nv_bfloat162_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__nv_fp8_storage_t __nv_cvt_bfloat16raw_to_fp8(__nv_bfloat16_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8_storage_t __nv_cvt_bfloat16raw_to_fp8(__nv_bfloat16_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__nv_fp8x2_storage_t __nv_cvt_double2_to_fp8x2(double2, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8x2_storage_t __nv_cvt_double2_to_fp8x2(double2, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__nv_fp8_storage_t __nv_cvt_double_to_fp8(double, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8_storage_t __nv_cvt_double_to_fp8(double, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__nv_fp8x2_storage_t __nv_cvt_float2_to_fp8x2(float2, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8x2_storage_t __nv_cvt_float2_to_fp8x2(float2, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__nv_fp8_storage_t __nv_cvt_float_to_fp8(float, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8_storage_t __nv_cvt_float_to_fp8(float, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__half_raw __nv_cvt_fp8_to_halfraw(__nv_fp8_storage_t, __nv_fp8_interpretation_t);\n-__device__ __half_raw __nv_cvt_fp8_to_halfraw(__nv_fp8_storage_t, __nv_fp8_interpretation_t);\n-__half2_raw __nv_cvt_fp8x2_to_halfraw2(__nv_fp8x2_storage_t, __nv_fp8_interpretation_t);\n-__device__ __half2_raw __nv_cvt_fp8x2_to_halfraw2(__nv_fp8x2_storage_t, __nv_fp8_interpretation_t);\n-__nv_fp8x2_storage_t __nv_cvt_halfraw2_to_fp8x2(__half2_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8x2_storage_t __nv_cvt_halfraw2_to_fp8x2(__half2_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__nv_fp8_storage_t __nv_cvt_halfraw_to_fp8(__half_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n-__device__ __nv_fp8_storage_t __nv_cvt_halfraw_to_fp8(__half_raw, __nv_saturation_t, __nv_fp8_interpretation_t);\n</code></pre>"},{"location":"manual/api-math/#112-c-struct-for-handling-fp8-data-type-of-e5m2-kind","title":"1.1.2. C++ struct for handling fp8 data type of e5m2 kind.","text":"<pre><code>-struct __nv_fp8_e5m2;\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const long long int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const long long int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const long int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const long int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const short int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const short int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned long long int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned long long int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned long int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned long int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned short int val);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const unsigned short int val);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const double f);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const double f);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const float f);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const float f);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const __nv_bfloat16 f);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const __nv_bfloat16 f);\n-__nv_fp8_e5m2::__nv_fp8_e5m2(const __half f);\n-__device__ __nv_fp8_e5m2::__nv_fp8_e5m2(const __half f);\n-__nv_fp8_e5m2::__nv_fp8_e5m2();\n-__nv_fp8_e5m2::operator __half()const;\n-__device__ __nv_fp8_e5m2::operator __half()const;\n-__nv_fp8_e5m2::operator __nv_bfloat16()const;\n-__device__ __nv_fp8_e5m2::operator __nv_bfloat16()const;\n-__nv_fp8_e5m2::operator bool()const;\n-__device__ __nv_fp8_e5m2::operator bool()const;\n-__nv_fp8_e5m2::operator char()const;\n-__device__ __nv_fp8_e5m2::operator char()const;\n-__nv_fp8_e5m2::operator double()const;\n-__device__ __nv_fp8_e5m2::operator double()const;\n-__nv_fp8_e5m2::operator float()const;\n-__device__ __nv_fp8_e5m2::operator float()const;\n-__nv_fp8_e5m2::operator int()const;\n-__device__ __nv_fp8_e5m2::operator int()const;\n-__nv_fp8_e5m2::operator long int()const;\n-__device__ __nv_fp8_e5m2::operator long int()const;\n-__nv_fp8_e5m2::operator long long int()const;\n-__device__ __nv_fp8_e5m2::operator long long int()const;\n-__nv_fp8_e5m2::operator short int()const;\n-__device__ __nv_fp8_e5m2::operator short int()const;\n-__nv_fp8_e5m2::operator signed char()const;\n-__device__ __nv_fp8_e5m2::operator signed char()const;\n-__nv_fp8_e5m2::operator unsigned char()const;\n-__device__ __nv_fp8_e5m2::operator unsigned char()const;\n-__nv_fp8_e5m2::operator unsigned int()const;\n-__device__ __nv_fp8_e5m2::operator unsigned int()const;\n-__nv_fp8_e5m2::operator unsigned long int()const;\n-__device__ __nv_fp8_e5m2::operator unsigned long int()const;\n-__nv_fp8_e5m2::operator unsigned long long int()const;\n-__device__ __nv_fp8_e5m2::operator unsigned long long int()const;\n-__nv_fp8_e5m2::operator unsigned short int()const;\n-__device__ __nv_fp8_e5m2::operator unsigned short int()const;\n-__nv_fp8_storage_t __nv_fp8_e5m2::__x;\n</code></pre>"},{"location":"manual/api-math/#113-c-struct-for-handling-vector-type-of-two-fp8-values-of-e5m2-kind","title":"1.1.3. C++ struct for handling vector type of two fp8 values of e5m2 kind.","text":"<pre><code>-struct __nv_fp8x2_e5m2;\n-__nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const double2 f);\n-__device__ __nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const double2 f);\n-__nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const float2 f);\n-__device__ __nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const float2 f);\n-__nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const __nv_bfloat162 f);\n-__device__ __nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const __nv_bfloat162 f);\n-__nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const __half2 f);\n-__device__ __nv_fp8x2_e5m2::__nv_fp8x2_e5m2(const __half2 f);\n-__nv_fp8x2_e5m2::__nv_fp8x2_e5m2();\n-__nv_fp8x2_e5m2::operator __half2()const;\n-__device__ __nv_fp8x2_e5m2::operator __half2()const;\n-__nv_fp8x2_e5m2::operator float2()const;\n-__device__ __nv_fp8x2_e5m2::operator float2()const;\n-__nv_fp8x2_storage_t __nv_fp8x2_e5m2::__x;\n</code></pre>"},{"location":"manual/api-math/#114-c-struct-for-handling-vector-type-of-four-fp8-values-of-e5m2-kind","title":"1.1.4. C++ struct for handling vector type of four fp8 values of e5m2 kind.","text":"<pre><code>-struct __nv_fp8x4_e5m2;\n-__nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const double4 f);\n-__device__ __nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const double4 f);\n-__nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const float4 f);\n-__device__ __nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const float4 f);\n-__nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const __nv_bfloat162 flo, const __nv_bfloat162 fhi);\n-__device__ __nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const __nv_bfloat162 flo, const __nv_bfloat162 fhi);\n-__nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const __half2 flo, const __half2 fhi);\n-__device__ __nv_fp8x4_e5m2::__nv_fp8x4_e5m2(const __half2 flo, const __half2 fhi);\n-__nv_fp8x4_e5m2::__nv_fp8x4_e5m2();\n-__nv_fp8x4_e5m2::operator float4()const;\n-__device__ __nv_fp8x4_e5m2::operator float4()const;\n-__nv_fp8x4_storage_t __nv_fp8x4_e5m2::__x;\n</code></pre>"},{"location":"manual/api-math/#115-c-struct-for-handling-fp8-data-type-of-e4m3-kind","title":"1.1.5. C++ struct for handling fp8 data type of e4m3 kind.","text":"<pre><code>-struct __nv_fp8_e4m3;\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const long long int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const long long int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const long int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const long int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const short int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const short int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned long long int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned long long int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned long int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned long int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned short int val);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const unsigned short int val);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const double f);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const double f);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const float f);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const float f);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const __nv_bfloat16 f);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const __nv_bfloat16 f);\n-__nv_fp8_e4m3::__nv_fp8_e4m3(const __half f);\n-__device__ __nv_fp8_e4m3::__nv_fp8_e4m3(const __half f);\n-__nv_fp8_e4m3::__nv_fp8_e4m3();\n-__nv_fp8_e4m3::operator __half()const;\n-__device__ __nv_fp8_e4m3::operator __half()const;\n-__nv_fp8_e4m3::operator __nv_bfloat16()const;\n-__device__ __nv_fp8_e4m3::operator __nv_bfloat16()const;\n-__nv_fp8_e4m3::operator bool()const;\n-__device__ __nv_fp8_e4m3::operator bool()const;\n-__nv_fp8_e4m3::operator char()const;\n-__device__ __nv_fp8_e4m3::operator char()const;\n-__nv_fp8_e4m3::operator double()const;\n-__device__ __nv_fp8_e4m3::operator double()const;\n-__nv_fp8_e4m3::operator float()const;\n-__device__ __nv_fp8_e4m3::operator float()const;\n-__nv_fp8_e4m3::operator int()const;\n-__device__ __nv_fp8_e4m3::operator int()const;\n-__nv_fp8_e4m3::operator long int()const;\n-__device__ __nv_fp8_e4m3::operator long int()const;\n-__nv_fp8_e4m3::operator long long int()const;\n-__device__ __nv_fp8_e4m3::operator long long int()const;\n-__nv_fp8_e4m3::operator short int()const;\n-__device__ __nv_fp8_e4m3::operator short int()const;\n-__nv_fp8_e4m3::operator signed char()const;\n-__device__ __nv_fp8_e4m3::operator signed char()const;\n-__nv_fp8_e4m3::operator unsigned char()const;\n-__device__ __nv_fp8_e4m3::operator unsigned char()const;\n-__nv_fp8_e4m3::operator unsigned int()const;\n-__device__ __nv_fp8_e4m3::operator unsigned int()const;\n-__nv_fp8_e4m3::operator unsigned long int()const;\n-__device__ __nv_fp8_e4m3::operator unsigned long int()const;\n-__nv_fp8_e4m3::operator unsigned long long int()const;\n-__device__ __nv_fp8_e4m3::operator unsigned long long int()const;\n-__nv_fp8_e4m3::operator unsigned short int()const;\n-__device__ __nv_fp8_e4m3::operator unsigned short int()const;\n-__nv_fp8_storage_t __nv_fp8_e4m3::__x;\n</code></pre>"},{"location":"manual/api-math/#116-c-struct-for-handling-vector-type-of-two-fp8-values-of-e4m3-kind","title":"1.1.6. C++ struct for handling vector type of two fp8 values of e4m3 kind.","text":"<pre><code>-struct __nv_fp8x2_e4m3;\n-__nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const double2 f);\n-__device__ __nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const double2 f);\n-__nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const float2 f);\n-__device__ __nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const float2 f);\n-__nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const __nv_bfloat162 f);\n-__device__ __nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const __nv_bfloat162 f);\n-__nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const __half2 f);\n-__device__ __nv_fp8x2_e4m3::__nv_fp8x2_e4m3(const __half2 f);\n-__nv_fp8x2_e4m3::__nv_fp8x2_e4m3();\n-__nv_fp8x2_e4m3::operator __half2()const;\n-__device__ __nv_fp8x2_e4m3::operator __half2()const;\n-__nv_fp8x2_e4m3::operator float2()const;\n-__device__ __nv_fp8x2_e4m3::operator float2()const;\n-__nv_fp8x2_storage_t __nv_fp8x2_e4m3::__x;\n</code></pre>"},{"location":"manual/api-math/#117-c-struct-for-handling-vector-type-of-four-fp8-values-of-e4m3-kind","title":"1.1.7. C++ struct for handling vector type of four fp8 values of e4m3 kind.","text":"<pre><code>-struct __nv_fp8x4_e4m3;\n-__nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const double4 f);\n-__device__ __nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const double4 f);\n-__nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const float4 f);\n-__device__ __nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const float4 f);\n-__nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const __nv_bfloat162 flo, const __nv_bfloat162 fhi);\n-__device__ __nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const __nv_bfloat162 flo, const __nv_bfloat162 fhi);\n-__nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const __half2 flo, const __half2 fhi);\n-__device__ __nv_fp8x4_e4m3::__nv_fp8x4_e4m3(const __half2 flo, const __half2 fhi);\n-__nv_fp8x4_e4m3::__nv_fp8x4_e4m3();\n-__nv_fp8x4_e4m3::operator float4()const;\n-__device__ __nv_fp8x4_e4m3::operator float4()const;\n-__nv_fp8x4_storage_t __nv_fp8x4_e4m3::__x;\n</code></pre>"},{"location":"manual/api-math/#12-half-precision-intrinsics","title":"1.2. Half Precision Intrinsics","text":"<pre><code>struct __half;\nstruct __half2;\n-struct __half2_raw;\n-struct __half_raw;\n-typedef struct __half __nv_half;\n-typedef struct __half2 __nv_half2;\n-typedef struct __half2_raw __nv_half2_raw;\n-typedef struct __half_raw __nv_half_raw;\n-typedef struct __half half;\n-typedef struct __half2 half2;\n-typedef struct __half nv_half;\n-typedef struct __half2 nv_half2;\n</code></pre>"},{"location":"manual/api-math/#121-half-arithmetic-constants","title":"1.2.1. Half Arithmetic Constants","text":"<pre><code>#define CUDART_INF_FP16\n#define CUDART_MAX_NORMAL_FP16\n#define CUDART_MIN_DENORM_FP16\n#define CUDART_NAN_FP16\n#define CUDART_NEG_ZERO_FP16\n#define CUDART_ONE_FP16\n#define CUDART_ZERO_FP16\n</code></pre>"},{"location":"manual/api-math/#122-half-arithmetic-functions","title":"1.2.2. Half Arithmetic Functions","text":"<pre><code>-__half __habs(__half);\n-__device__ __half __habs(__half);\n-__half __hadd(__half, __half);\n-__device__ __half __hadd(__half, __half);\n-__half __hadd_rn(__half, __half);\n-__device__ __half __hadd_rn(__half, __half);\n-__half __hadd_sat(__half, __half);\n-__device__ __half __hadd_sat(__half, __half);\n-__half __hdiv(__half, __half);\n-__device__ __half __hdiv(__half, __half);\n__device__ __half __hfma(__half, __half, __half);\n__device__ __half __hfma_relu(__half, __half, __half);\n__device__ __half __hfma_sat(__half, __half, __half);\n-__half __hmul(__half, __half);\n-__device__ __half __hmul(__half, __half);\n-__half __hmul_rn(__half, __half);\n-__device__ __half __hmul_rn(__half, __half);\n-__half __hmul_sat(__half, __half);\n-__device__ __half __hmul_sat(__half, __half);\n-__half __hneg(__half);\n-__device__ __half __hneg(__half);\n-__half __hsub(__half, __half);\n-__device__ __half __hsub(__half, __half);\n-__half __hsub_rn(__half, __half);\n-__device__ __half __hsub_rn(__half, __half);\n-__half __hsub_sat(__half, __half);\n-__device__ __half __hsub_sat(__half, __half);\n-__device__ __half atomicAdd(const __half *, __half);\n__half operator*(const __half &amp;, const __half &amp;);\n__device__ __half operator*(const __half &amp;, const __half &amp;);\n__half &amp; operator*=(__half &amp;, const __half &amp;);\n__device__ __half &amp; operator*=(__half &amp;, const __half &amp;);\n__half operator+(const __half &amp;);\n__device__ __half operator+(const __half &amp;);\n__half operator+(const __half &amp;, const __half &amp;);\n__device__ __half operator+(const __half &amp;, const __half &amp;);\n__half operator++(__half &amp;, int);\n__device__ __half operator++(__half &amp;, int);\n__half &amp; operator++(__half &amp;);\n__device__ __half &amp; operator++(__half &amp;);\n__half &amp; operator+=(__half &amp;, const __half &amp;);\n__device__ __half &amp; operator+=(__half &amp;, const __half &amp;);\n__half operator-(const __half &amp;);\n__device__ __half operator-(const __half &amp;);\n__half operator-(const __half &amp;, const __half &amp;);\n__device__ __half operator-(const __half &amp;, const __half &amp;);\n__half operator--(__half &amp;, int);\n__device__ __half operator--(__half &amp;, int);\n__half &amp; operator--(__half &amp;);\n__device__ __half &amp; operator--(__half &amp;);\n__half &amp; operator-=(__half &amp;, const __half &amp;);\n__device__ __half &amp; operator-=(__half &amp;, const __half &amp;);\n__half operator/(const __half &amp;, const __half &amp;);\n__device__ __half operator/(const __half &amp;, const __half &amp;);\n__half &amp; operator/=(__half &amp;, const __half &amp;);\n__device__ __half &amp; operator/=(__half &amp;, const __half &amp;);\n</code></pre>"},{"location":"manual/api-math/#123-half2-arithmetic-functions","title":"1.2.3. Half2 Arithmetic Functions","text":"<pre><code>-__half2 __h2div(__half2, __half2);\n-__device__ __half2 __h2div(__half2, __half2);\n-__half2 __habs2(__half2);\n-__device__ __half2 __habs2(__half2);\n-__half2 __hadd2(__half2, __half2);\n-__device__ __half2 __hadd2(__half2, __half2);\n-__half2 __hadd2_rn(__half2, __half2);\n-__device__ __half2 __hadd2_rn(__half2, __half2);\n-__half2 __hadd2_sat(__half2, __half2);\n-__device__ __half2 __hadd2_sat(__half2, __half2);\n__device__ __half2 __hcmadd(__half2, __half2, __half2);\n__device__ __half2 __hfma2(__half2, __half2, __half2);\n__device__ __half2 __hfma2_relu(__half2, __half2, __half2);\n__device__ __half2 __hfma2_sat(__half2, __half2, __half2);\n-__half2 __hmul2(__half2, __half2);\n-__device__ __half2 __hmul2(__half2, __half2);\n-__half2 __hmul2_rn(__half2, __half2);\n-__device__ __half2 __hmul2_rn(__half2, __half2);\n-__half2 __hmul2_sat(__half2, __half2);\n-__device__ __half2 __hmul2_sat(__half2, __half2);\n-__half2 __hneg2(__half2);\n-__device__ __half2 __hneg2(__half2);\n-__half2 __hsub2(__half2, __half2);\n-__device__ __half2 __hsub2(__half2, __half2);\n-__half2 __hsub2_rn(__half2, __half2);\n-__device__ __half2 __hsub2_rn(__half2, __half2);\n-__half2 __hsub2_sat(__half2, __half2);\n-__device__ __half2 __hsub2_sat(__half2, __half2);\n-__device__ __half2 atomicAdd(const __half2 *, __half2);\n__half2 operator*(const __half2 &amp;, const __half2 &amp;);\n__device__ __half2 operator*(const __half2 &amp;, const __half2 &amp;);\n__half2 &amp; operator*=(__half2 &amp;, const __half2 &amp;);\n__device__ __half2 &amp; operator*=(__half2 &amp;, const __half2 &amp;);\n__half2 operator+(const __half2 &amp;);\n__device__ __half2 operator+(const __half2 &amp;);\n__half2 operator+(const __half2 &amp;, const __half2 &amp;);\n__device__ __half2 operator+(const __half2 &amp;, const __half2 &amp;);\n__half2 operator++(__half2 &amp;, int);\n__device__ __half2 operator++(__half2 &amp;, int);\n__half2 &amp; operator++(__half2 &amp;);\n__device__ __half2 &amp; operator++(__half2 &amp;);\n__half2 &amp; operator+=(__half2 &amp;, const __half2 &amp;);\n__device__ __half2 &amp; operator+=(__half2 &amp;, const __half2 &amp;);\n__half2 operator-(const __half2 &amp;);\n__device__ __half2 operator-(const __half2 &amp;);\n__half2 operator-(const __half2 &amp;, const __half2 &amp;);\n__device__ __half2 operator-(const __half2 &amp;, const __half2 &amp;);\n__half2 operator--(__half2 &amp;, int);\n__device__ __half2 operator--(__half2 &amp;, int);\n__half2 &amp; operator--(__half2 &amp;);\n__device__ __half2 &amp; operator--(__half2 &amp;);\n__half2 &amp; operator-=(__half2 &amp;, const __half2 &amp;);\n__device__ __half2 &amp; operator-=(__half2 &amp;, const __half2 &amp;);\n__half2 operator/(const __half2 &amp;, const __half2 &amp;);\n__device__ __half2 operator/(const __half2 &amp;, const __half2 &amp;);\n__half2 &amp; operator/=(__half2 &amp;, const __half2 &amp;);\n__device__ __half2 &amp; operator/=(__half2 &amp;, const __half2 &amp;);\n</code></pre>"},{"location":"manual/api-math/#124-half-comparison-functions","title":"1.2.4. Half Comparison Functions","text":"<pre><code>-bool __heq(__half, __half);\n-__device__ bool __heq(__half, __half);\n-bool __hequ(__half, __half);\n-__device__ bool __hequ(__half, __half);\n-bool __hge(__half, __half);\n-__device__ bool __hge(__half, __half);\n-bool __hgeu(__half, __half);\n-__device__ bool __hgeu(__half, __half);\n-bool __hgt(__half, __half);\n-__device__ bool __hgt(__half, __half);\n-bool __hgtu(__half, __half);\n-__device__ bool __hgtu(__half, __half);\n-int __hisinf(__half);\n-__device__ int __hisinf(__half);\n-bool __hisnan(__half);\n-__device__ bool __hisnan(__half);\n-bool __hle(__half, __half);\n-__device__ bool __hle(__half, __half);\n-bool __hleu(__half, __half);\n-__device__ bool __hleu(__half, __half);\n-bool __hlt(__half, __half);\n-__device__ bool __hlt(__half, __half);\n-bool __hltu(__half, __half);\n-__device__ bool __hltu(__half, __half);\n-__half __hmax(__half, __half);\n-__device__ __half __hmax(__half, __half);\n-__half __hmax_nan(__half, __half);\n-__device__ __half __hmax_nan(__half, __half);\n-__half __hmin(__half, __half);\n-__device__ __half __hmin(__half, __half);\n-__half __hmin_nan(__half, __half);\n-__device__ __half __hmin_nan(__half, __half);\n-bool __hne(__half, __half);\n-__device__ bool __hne(__half, __half);\n-bool __hneu(__half, __half);\n-__device__ bool __hneu(__half, __half);\nbool operator!=(const __half &amp;, const __half &amp;);\n__device__ bool operator!=(const __half &amp;, const __half &amp;);\nbool operator&lt;(const __half &amp;, const __half &amp;);\n__device__ bool operator&lt;(const __half &amp;, const __half &amp;);\nbool operator&lt;=(const __half &amp;, const __half &amp;);\n__device__ bool operator&lt;=(const __half &amp;, const __half &amp;);\nbool operator==(const __half &amp;, const __half &amp;);\n__device__ bool operator==(const __half &amp;, const __half &amp;);\nbool operator&gt;(const __half &amp;, const __half &amp;);\n__device__ bool operator&gt;(const __half &amp;, const __half &amp;);\nbool operator&gt;=(const __half &amp;, const __half &amp;);\n__device__ bool operator&gt;=(const __half &amp;, const __half &amp;);\n</code></pre>"},{"location":"manual/api-math/#125-half2-comparison-functions","title":"1.2.5. Half2 Comparison Functions","text":"<pre><code>-bool __hbeq2(__half2, __half2);\n-__device__ bool __hbeq2(__half2, __half2);\n-bool __hbequ2(__half2, __half2);\n-__device__ bool __hbequ2(__half2, __half2);\n-bool __hbge2(__half2, __half2);\n-__device__ bool __hbge2(__half2, __half2);\n-bool __hbgeu2(__half2, __half2);\n-__device__ bool __hbgeu2(__half2, __half2);\n-bool __hbgt2(__half2, __half2);\n-__device__ bool __hbgt2(__half2, __half2);\n-bool __hbgtu2(__half2, __half2);\n-__device__ bool __hbgtu2(__half2, __half2);\n-bool __hble2(__half2, __half2);\n-__device__ bool __hble2(__half2, __half2);\n-bool __hbleu2(__half2, __half2);\n-__device__ bool __hbleu2(__half2, __half2);\n-bool __hblt2(__half2, __half2);\n-__device__ bool __hblt2(__half2, __half2);\n-bool __hbltu2(__half2, __half2);\n-__device__ bool __hbltu2(__half2, __half2);\n-bool __hbne2(__half2, __half2);\n-__device__ bool __hbne2(__half2, __half2);\n-bool __hbneu2(__half2, __half2);\n-__device__ bool __hbneu2(__half2, __half2);\n-__half2 __heq2(__half2, __half2);\n-__device__ __half2 __heq2(__half2, __half2);\n-unsigned __heq2_mask(__half2, __half2);\n-__device__ unsigned __heq2_mask(__half2, __half2);\n-__half2 __hequ2(__half2, __half2);\n-__device__ __half2 __hequ2(__half2, __half2);\n-unsigned __hequ2_mask(__half2, __half2);\n-__device__ unsigned __hequ2_mask(__half2, __half2);\n-__half2 __hge2(__half2, __half2);\n-__device__ __half2 __hge2(__half2, __half2);\n-unsigned __hge2_mask(__half2, __half2);\n-__device__ unsigned __hge2_mask(__half2, __half2);\n-__half2 __hgeu2(__half2, __half2);\n-__device__ __half2 __hgeu2(__half2, __half2);\n-unsigned __hgeu2_mask(__half2, __half2);\n-__device__ unsigned __hgeu2_mask(__half2, __half2);\n-__half2 __hgt2(__half2, __half2);\n-__device__ __half2 __hgt2(__half2, __half2);\n-unsigned __hgt2_mask(__half2, __half2);\n-__device__ unsigned __hgt2_mask(__half2, __half2);\n-__half2 __hgtu2(__half2, __half2);\n-__device__ __half2 __hgtu2(__half2, __half2);\n-unsigned __hgtu2_mask(__half2, __half2);\n-__device__ unsigned __hgtu2_mask(__half2, __half2);\n-__half2 __hisnan2(__half2);\n-__device__ __half2 __hisnan2(__half2);\n-__half2 __hle2(__half2, __half2);\n-__device__ __half2 __hle2(__half2, __half2);\n-unsigned __hle2_mask(__half2, __half2);\n-__device__ unsigned __hle2_mask(__half2, __half2);\n-__half2 __hleu2(__half2, __half2);\n-__device__ __half2 __hleu2(__half2, __half2);\n-unsigned __hleu2_mask(__half2, __half2);\n-__device__ unsigned __hleu2_mask(__half2, __half2);\n-__half2 __hlt2(__half2, __half2);\n-__device__ __half2 __hlt2(__half2, __half2);\n-unsigned __hlt2_mask(__half2, __half2);\n-__device__ unsigned __hlt2_mask(__half2, __half2);\n-__half2 __hltu2(__half2, __half2);\n-__device__ __half2 __hltu2(__half2, __half2);\n-unsigned __hltu2_mask(__half2, __half2);\n-__device__ unsigned __hltu2_mask(__half2, __half2);\n-__half2 __hmax2(__half2, __half2);\n-__device__ __half2 __hmax2(__half2, __half2);\n-__half2 __hmax2_nan(__half2, __half2);\n-__device__ __half2 __hmax2_nan(__half2, __half2);\n-__half2 __hmin2(__half2, __half2);\n-__device__ __half2 __hmin2(__half2, __half2);\n-__half2 __hmin2_nan(__half2, __half2);\n-__device__ __half2 __hmin2_nan(__half2, __half2);\n-__half2 __hne2(__half2, __half2);\n-__device__ __half2 __hne2(__half2, __half2);\n-unsigned __hne2_mask(__half2, __half2);\n-__device__ unsigned __hne2_mask(__half2, __half2);\n-__half2 __hneu2(__half2, __half2);\n-__device__ __half2 __hneu2(__half2, __half2);\n-unsigned __hneu2_mask(__half2, __half2);\n-__device__ unsigned __hneu2_mask(__half2, __half2);\n-bool operator!=(const __half2 &amp;, const __half2 &amp;);\n-__device__ bool operator!=(const __half2 &amp;, const __half2 &amp;);\n-bool operator&lt;(const __half2 &amp;, const __half2 &amp;);\n-__device__ bool operator&lt;(const __half2 &amp;, const __half2 &amp;);\n-bool operator&lt;=(const __half2 &amp;, const __half2 &amp;);\n-__device__ bool operator&lt;=(const __half2 &amp;, const __half2 &amp;);\n-bool operator==(const __half2 &amp;, const __half2 &amp;);\n-__device__ bool operator==(const __half2 &amp;, const __half2 &amp;);\n-bool operator&gt;(const __half2 &amp;, const __half2 &amp;);\n-__device__ bool operator&gt;(const __half2 &amp;, const __half2 &amp;);\n-bool operator&gt;=(const __half2 &amp;, const __half2 &amp;);\n-__device__ bool operator&gt;=(const __half2 &amp;, const __half2 &amp;);\n</code></pre>"},{"location":"manual/api-math/#126-half-precision-conversion-and-data-movement","title":"1.2.6. Half Precision Conversion and Data Movement","text":"<pre><code>-__half __double2half(double);\n-__device__ __half __double2half(double);\n__half2 __float22half2_rn(float2);\n__device__ __half2 __float22half2_rn(float2);\n__half __float2half(float);\n__device__ __half __float2half(float);\n__half2 __float2half2_rn(float);\n__device__ __half2 __float2half2_rn(float);\n-__half __float2half_rd(float);\n-__device__ __half __float2half_rd(float);\n-__half __float2half_rn(float);\n__device__ __half __float2half_rn(float);\n-__half __float2half_ru(float);\n-__device__ __half __float2half_ru(float);\n-__half __float2half_rz(float);\n-__device__ __half __float2half_rz(float);\n-__half2 __floats2half2_rn(float, float);\n__device__ __half2 __floats2half2_rn(float, float);\nfloat2 __half22float2(__half2);\n__device__ float2 __half22float2(__half2);\n-signed char __half2char_rz(__half);\n-__device__ signed char __half2char_rz(__half);\nfloat __half2float(__half);\n__device__ float __half2float(__half);\n__half2 __half2half2(__half);\n__device__ __half2 __half2half2(__half);\n__device__ int __half2int_rd(__half);\n__device__ int __half2int_rn(__half);\n__device__ int __half2int_ru(__half);\n-int __half2int_rz(__half);\n__device__ int __half2int_rz(__half);\n__device__ long long __half2ll_rd(__half);\n__device__ long long __half2ll_rn(__half);\n__device__ long long __half2ll_ru(__half);\n-long long __half2ll_rz(__half);\n__device__ long long __half2ll_rz(__half);\n__device__ short __half2short_rd(__half);\n__device__ short __half2short_rn(__half);\n__device__ short __half2short_ru(__half);\n-short __half2short_rz(__half);\n__device__ short __half2short_rz(__half);\n-unsigned char __half2uchar_rz(__half);\n-__device__ unsigned char __half2uchar_rz(__half);\n__device__ unsigned __half2uint_rd(__half);\n__device__ unsigned __half2uint_rn(__half);\n__device__ unsigned __half2uint_ru(__half);\n-unsigned __half2uint_rz(__half);\n__device__ unsigned __half2uint_rz(__half);\n__device__ unsigned long long __half2ull_rd(__half);\n__device__ unsigned long long __half2ull_rn(__half);\n__device__ unsigned long long __half2ull_ru(__half);\n-unsigned long long __half2ull_rz(__half);\n__device__ unsigned long long __half2ull_rz(__half);\n__device__ unsigned short __half2ushort_rd(__half);\n__device__ unsigned short __half2ushort_rn(__half);\n__device__ unsigned short __half2ushort_ru(__half);\n-unsigned short __half2ushort_rz(__half);\n__device__ unsigned short __half2ushort_rz(__half);\n-short __half_as_short(__half);\n__device__ short __half_as_short(__half);\n-unsigned short __half_as_ushort(__half);\n__device__ unsigned short __half_as_ushort(__half);\n__half2 __halves2half2(__half, __half);\n__device__ __half2 __halves2half2(__half, __half);\nfloat __high2float(__half2);\n__device__ float __high2float(__half2);\n__half __high2half(__half2);\n__device__ __half __high2half(__half2);\n__half2 __high2half2(__half2);\n__device__ __half2 __high2half2(__half2);\n__half2 __highs2half2(__half2, __half2);\n__device__ __half2 __highs2half2(__half2, __half2);\n-__half __int2half_rd(int);\n-__device__ __half __int2half_rd(int);\n-__half __int2half_rn(int);\n__device__ __half __int2half_rn(int);\n-__half __int2half_ru(int);\n-__device__ __half __int2half_ru(int);\n-__half __int2half_rz(int);\n-__device__ __half __int2half_rz(int);\n__device__ __half __ldca(const __half *);\n__device__ __half2 __ldca(const __half2 *);\n__device__ __half __ldcg(const __half *);\n__device__ __half2 __ldcg(const __half2 *);\n__device__ __half __ldcs(const __half *);\n__device__ __half2 __ldcs(const __half2 *);\n__device__ __half __ldcv(const __half *);\n__device__ __half2 __ldcv(const __half2 *);\n__device__ __half __ldg(const __half *);\n__device__ __half2 __ldg(const __half2 *);\n__device__ __half __ldlu(const __half *);\n__device__ __half2 __ldlu(const __half2 *);\n-__half __ll2half_rd(long long);\n-__device__ __half __ll2half_rd(long long);\n-__half __ll2half_rn(long long);\n__device__ __half __ll2half_rn(long long);\n-__half __ll2half_ru(long long);\n-__device__ __half __ll2half_ru(long long);\n-__half __ll2half_rz(long long);\n-__device__ __half __ll2half_rz(long long);\nfloat __low2float(__half2);\n__device__ float __low2float(__half2);\n__half __low2half(__half2);\n__device__ __half __low2half(__half2);\n__half2 __low2half2(__half2);\n__device__ __half2 __low2half2(__half2);\n__half2 __lowhigh2highlow(__half2);\n__device__ __half2 __lowhigh2highlow(__half2);\n__half2 __lows2half2(__half2, __half2);\n__device__ __half2 __lows2half2(__half2, __half2);\n-__device__ __half __shfl_down_sync(unsigned, __half, unsigned, int = warpSize);\n-__device__ __half2 __shfl_down_sync(unsigned, __half2, unsigned, int = warpSize);\n-__device__ __half __shfl_sync(unsigned, __half, int, int = warpSize);\n-__device__ __half2 __shfl_sync(unsigned, __half2, int, int = warpSize);\n-__device__ __half __shfl_up_sync(unsigned, __half, unsigned, int = warpSize);\n-__device__ __half2 __shfl_up_sync(unsigned, __half2, unsigned, int = warpSize);\n-__device__ __half __shfl_xor_sync(unsigned, __half, int, int = warpSize);\n-__device__ __half2 __shfl_xor_sync(unsigned, __half2, int, int = warpSize);\n-__half __short2half_rd(short);\n-__device__ __half __short2half_rd(short);\n-__half __short2half_rn(short);\n__device__ __half __short2half_rn(short);\n-__half __short2half_ru(short);\n-__device__ __half __short2half_ru(short);\n-__half __short2half_rz(short);\n-__device__ __half __short2half_rz(short);\n-__half __short_as_half(short);\n__device__ __half __short_as_half(short);\n-__device__ void __stcg(const __half *, __half);\n-__device__ void __stcg(const __half2 *, __half2);\n-__device__ void __stcs(const __half *, __half);\n-__device__ void __stcs(const __half2 *, __half2);\n-__device__ void __stwb(const __half *, __half);\n-__device__ void __stwb(const __half2 *, __half2);\n-__device__ void __stwt(const __half *, __half);\n-__device__ void __stwt(const __half2 *, __half2);\n-__half __uint2half_rd(unsigned);\n-__device__ __half __uint2half_rd(unsigned);\n-__half __uint2half_rn(unsigned);\n__device__ __half __uint2half_rn(unsigned);\n-__half __uint2half_ru(unsigned);\n-__device__ __half __uint2half_ru(unsigned);\n-__half __uint2half_rz(unsigned);\n-__device__ __half __uint2half_rz(unsigned);\n-__half __ull2half_rd(unsigned long long);\n-__device__ __half __ull2half_rd(unsigned long long);\n-__half __ull2half_rn(unsigned long long);\n__device__ __half __ull2half_rn(unsigned long long);\n-__half __ull2half_ru(unsigned long long);\n-__device__ __half __ull2half_ru(unsigned long long);\n-__half __ull2half_rz(unsigned long long);\n-__device__ __half __ull2half_rz(unsigned long long);\n-__half __ushort2half_rd(unsigned short);\n-__device__ __half __ushort2half_rd(unsigned short);\n-__half __ushort2half_rn(unsigned short);\n__device__ __half __ushort2half_rn(unsigned short);\n-__half __ushort2half_ru(unsigned short);\n-__device__ __half __ushort2half_ru(unsigned short);\n-__half __ushort2half_rz(unsigned short);\n-__device__ __half __ushort2half_rz(unsigned short);\n-__half __ushort_as_half(unsigned short);\n__device__ __half __ushort_as_half(unsigned short);\n__half2 make_half2(__half, __half);\n__device__ __half2 make_half2(__half, __half);\n</code></pre>"},{"location":"manual/api-math/#127-half-math-functions","title":"1.2.7. Half Math Functions","text":"<pre><code>__device__ __half hceil(__half);\n__device__ __half hcos(__half);\n__device__ __half hexp(__half);\n__device__ __half hexp10(__half);\n__device__ __half hexp2(__half);\n__device__ __half hfloor(__half);\n__device__ __half hlog(__half);\n__device__ __half hlog10(__half);\n__device__ __half hlog2(__half);\n__device__ __half hrcp(__half);\n__device__ __half hrint(__half);\n__device__ __half hrsqrt(__half);\n__device__ __half hsin(__half);\n__device__ __half hsqrt(__half);\n__device__ __half htrunc(__half);\n</code></pre>"},{"location":"manual/api-math/#128-half2-math-functions","title":"1.2.8. Half2 Math Functions","text":"<pre><code>__device__ __half2 h2ceil(__half2);\n__device__ __half2 h2cos(__half2);\n__device__ __half2 h2exp(__half2);\n__device__ __half2 h2exp10(__half2);\n__device__ __half2 h2exp2(__half2);\n__device__ __half2 h2floor(__half2);\n__device__ __half2 h2log(__half2);\n__device__ __half2 h2log10(__half2);\n__device__ __half2 h2log2(__half2);\n__device__ __half2 h2rcp(__half2);\n__device__ __half2 h2rint(__half2);\n__device__ __half2 h2rsqrt(__half2);\n__device__ __half2 h2sin(__half2);\n__device__ __half2 h2sqrt(__half2);\n__device__ __half2 h2trunc(__half2);\n</code></pre>"},{"location":"manual/api-math/#13-bfloat16-precision-intrinsics","title":"1.3. Bfloat16 Precision Intrinsics","text":"<pre><code>struct __nv_bfloat16;\nstruct __nv_bfloat162;\n-struct __nv_bfloat162_raw;\n-struct __nv_bfloat16_raw;\n-typedef struct __nv_bfloat16 nv_bfloat16;\n-typedef struct __nv_bfloat162 nv_bfloat162;\n</code></pre>"},{"location":"manual/api-math/#131-bfloat16-arithmetic-constants","title":"1.3.1. Bfloat16 Arithmetic Constants","text":"<pre><code>-#define CUDART_INF_BF16\n-#define CUDART_MAX_NORMAL_BF16\n-#define CUDART_MIN_DENORM_BF16\n-#define CUDART_NAN_BF16\n-#define CUDART_NEG_ZERO_BF16\n-#define CUDART_ONE_BF16\n-#define CUDART_ZERO_BF16\n</code></pre>"},{"location":"manual/api-math/#132-bfloat16-arithmetic-functions","title":"1.3.2. Bfloat16 Arithmetic Functions","text":"<pre><code>-__nv_bfloat162 __h2div(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __h2div(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat16 __habs(__nv_bfloat16);\n-__device__ __nv_bfloat16 __habs(__nv_bfloat16);\n-__nv_bfloat16 __hadd(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hadd(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hadd_rn(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hadd_rn(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hadd_sat(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hadd_sat(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hdiv(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hdiv(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hfma(__nv_bfloat16, __nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hfma_relu(__nv_bfloat16, __nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hfma_sat(__nv_bfloat16, __nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hmul(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hmul(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hmul_rn(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hmul_rn(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hmul_sat(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hmul_sat(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hneg(__nv_bfloat16);\n-__device__ __nv_bfloat16 __hneg(__nv_bfloat16);\n-__nv_bfloat16 __hsub(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hsub(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hsub_rn(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hsub_rn(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hsub_sat(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hsub_sat(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 atomicAdd(const __nv_bfloat16 *, __nv_bfloat16);\n-__nv_bfloat16 operator*(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 operator*(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__nv_bfloat16 &amp; operator*=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 &amp; operator*=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__nv_bfloat16 operator+(const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 operator+(const __nv_bfloat16 &amp;);\n-__nv_bfloat16 operator+(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 operator+(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__nv_bfloat16 operator++(__nv_bfloat16 &amp;, int);\n-__device__ __nv_bfloat16 operator++(__nv_bfloat16 &amp;, int);\n-__nv_bfloat16 &amp; operator++(__nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 &amp; operator++(__nv_bfloat16 &amp;);\n-__nv_bfloat16 &amp; operator+=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 &amp; operator+=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__nv_bfloat16 operator-(const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 operator-(const __nv_bfloat16 &amp;);\n-__nv_bfloat16 operator-(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 operator-(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__nv_bfloat16 operator--(__nv_bfloat16 &amp;, int);\n-__device__ __nv_bfloat16 operator--(__nv_bfloat16 &amp;, int);\n-__nv_bfloat16 &amp; operator--(__nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 &amp; operator--(__nv_bfloat16 &amp;);\n-__nv_bfloat16 &amp; operator-=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 &amp; operator-=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__nv_bfloat16 operator/(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 operator/(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__nv_bfloat16 &amp; operator/=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __nv_bfloat16 &amp; operator/=(__nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n</code></pre>"},{"location":"manual/api-math/#133-bfloat162-arithmetic-functions","title":"1.3.3. Bfloat162 Arithmetic Functions","text":"<pre><code>-__nv_bfloat162 __habs2(__nv_bfloat162);\n-__device__ __nv_bfloat162 __habs2(__nv_bfloat162);\n-__nv_bfloat162 __hadd2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hadd2(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hadd2_rn(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hadd2_rn(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hadd2_sat(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hadd2_sat(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hcmadd(__nv_bfloat162, __nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hfma2(__nv_bfloat162, __nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hfma2_relu(__nv_bfloat162, __nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hfma2_sat(__nv_bfloat162, __nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hmul2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hmul2(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hmul2_rn(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hmul2_rn(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hmul2_sat(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hmul2_sat(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hneg2(__nv_bfloat162);\n-__device__ __nv_bfloat162 __hneg2(__nv_bfloat162);\n-__nv_bfloat162 __hsub2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hsub2(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hsub2_rn(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hsub2_rn(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hsub2_sat(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hsub2_sat(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 atomicAdd(const __nv_bfloat162 *, __nv_bfloat162);\n-__nv_bfloat162 operator*(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 operator*(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__nv_bfloat162 &amp; operator*=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 &amp; operator*=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__nv_bfloat162 operator+(const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 operator+(const __nv_bfloat162 &amp;);\n-__nv_bfloat162 operator+(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 operator+(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__nv_bfloat162 operator++(__nv_bfloat162 &amp;, int);\n-__device__ __nv_bfloat162 operator++(__nv_bfloat162 &amp;, int);\n-__nv_bfloat162 &amp; operator++(__nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 &amp; operator++(__nv_bfloat162 &amp;);\n-__nv_bfloat162 &amp; operator+=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 &amp; operator+=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__nv_bfloat162 operator-(const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 operator-(const __nv_bfloat162 &amp;);\n-__nv_bfloat162 operator-(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 operator-(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__nv_bfloat162 operator--(__nv_bfloat162 &amp;, int);\n-__device__ __nv_bfloat162 operator--(__nv_bfloat162 &amp;, int);\n-__nv_bfloat162 &amp; operator--(__nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 &amp; operator--(__nv_bfloat162 &amp;);\n-__nv_bfloat162 &amp; operator-=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 &amp; operator-=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__nv_bfloat162 operator/(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 operator/(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__nv_bfloat162 &amp; operator/=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __nv_bfloat162 &amp; operator/=(__nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n</code></pre>"},{"location":"manual/api-math/#134-bfloat16-comparison-functions","title":"1.3.4. Bfloat16 Comparison Functions","text":"<pre><code>-bool __heq(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __heq(__nv_bfloat16, __nv_bfloat16);\n-bool __hequ(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hequ(__nv_bfloat16, __nv_bfloat16);\n-bool __hge(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hge(__nv_bfloat16, __nv_bfloat16);\n-bool __hgeu(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hgeu(__nv_bfloat16, __nv_bfloat16);\n-bool __hgt(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hgt(__nv_bfloat16, __nv_bfloat16);\n-bool __hgtu(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hgtu(__nv_bfloat16, __nv_bfloat16);\n-int __hisinf(__nv_bfloat16);\n-__device__ int __hisinf(__nv_bfloat16);\n-bool __hisnan(__nv_bfloat16);\n-__device__ bool __hisnan(__nv_bfloat16);\n-bool __hle(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hle(__nv_bfloat16, __nv_bfloat16);\n-bool __hleu(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hleu(__nv_bfloat16, __nv_bfloat16);\n-bool __hlt(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hlt(__nv_bfloat16, __nv_bfloat16);\n-bool __hltu(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hltu(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hmax(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hmax(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hmax_nan(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hmax_nan(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hmin(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hmin(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __hmin_nan(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat16 __hmin_nan(__nv_bfloat16, __nv_bfloat16);\n-bool __hne(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hne(__nv_bfloat16, __nv_bfloat16);\n-bool __hneu(__nv_bfloat16, __nv_bfloat16);\n-__device__ bool __hneu(__nv_bfloat16, __nv_bfloat16);\n-__CUDA_BF16_FORCEINLINE__ bool operator!=(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator!=(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&lt;(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&lt;(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&lt;=(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&lt;=(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator==(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator==(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&gt;(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&gt;(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&gt;=(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&gt;=(const __nv_bfloat16 &amp;, const __nv_bfloat16 &amp;);\n</code></pre>"},{"location":"manual/api-math/#135-bfloat162-comparison-functions","title":"1.3.5. Bfloat162 Comparison Functions","text":"<pre><code>-bool __hbeq2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbeq2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbequ2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbequ2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbge2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbge2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbgeu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbgeu2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbgt2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbgt2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbgtu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbgtu2(__nv_bfloat162, __nv_bfloat162);\n-bool __hble2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hble2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbleu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbleu2(__nv_bfloat162, __nv_bfloat162);\n-bool __hblt2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hblt2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbltu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbltu2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbne2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbne2(__nv_bfloat162, __nv_bfloat162);\n-bool __hbneu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ bool __hbneu2(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __heq2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __heq2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __heq2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __heq2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hequ2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hequ2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hequ2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hequ2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hge2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hge2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hge2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hge2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hgeu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hgeu2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hgeu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hgeu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hgt2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hgt2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hgt2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hgt2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hgtu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hgtu2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hgtu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hgtu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hisnan2(__nv_bfloat162);\n-__device__ __nv_bfloat162 __hisnan2(__nv_bfloat162);\n-__nv_bfloat162 __hle2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hle2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hle2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hle2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hleu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hleu2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hleu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hleu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hlt2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hlt2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hlt2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hlt2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hltu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hltu2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hltu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hltu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hmax2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hmax2(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hmax2_nan(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hmax2_nan(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hmin2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hmin2(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hmin2_nan(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hmin2_nan(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hne2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hne2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hne2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hne2_mask(__nv_bfloat162, __nv_bfloat162);\n-__nv_bfloat162 __hneu2(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __hneu2(__nv_bfloat162, __nv_bfloat162);\n-unsigned __hneu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__device__ unsigned __hneu2_mask(__nv_bfloat162, __nv_bfloat162);\n-__CUDA_BF16_FORCEINLINE__ bool operator!=(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator!=(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&lt;(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&lt;(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&lt;=(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&lt;=(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator==(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator==(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&gt;(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&gt;(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__CUDA_BF16_FORCEINLINE__ bool operator&gt;=(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n-__device__ __CUDA_BF16_FORCEINLINE__ bool operator&gt;=(const __nv_bfloat162 &amp;, const __nv_bfloat162 &amp;);\n</code></pre>"},{"location":"manual/api-math/#136-bfloat16-precision-conversion-and-data-movement","title":"1.3.6. Bfloat16 Precision Conversion and Data Movement","text":"<pre><code>-float2 __bfloat1622float2(__nv_bfloat162);\n-__device__ float2 __bfloat1622float2(__nv_bfloat162);\n-__nv_bfloat162 __bfloat162bfloat162(__nv_bfloat16);\n-__device__ __nv_bfloat162 __bfloat162bfloat162(__nv_bfloat16);\n-signed char __bfloat162char_rz(__nv_bfloat16);\n-__device__ signed char __bfloat162char_rz(__nv_bfloat16);\n-float __bfloat162float(__nv_bfloat16);\n-__device__ float __bfloat162float(__nv_bfloat16);\n-__device__ int __bfloat162int_rd(__nv_bfloat16);\n-__device__ int __bfloat162int_rn(__nv_bfloat16);\n-__device__ int __bfloat162int_ru(__nv_bfloat16);\n-int __bfloat162int_rz(__nv_bfloat16);\n-__device__ int __bfloat162int_rz(__nv_bfloat16);\n-__device__ long long __bfloat162ll_rd(__nv_bfloat16);\n-__device__ long long __bfloat162ll_rn(__nv_bfloat16);\n-__device__ long long __bfloat162ll_ru(__nv_bfloat16);\n-long long __bfloat162ll_rz(__nv_bfloat16);\n-__device__ long long __bfloat162ll_rz(__nv_bfloat16);\n-__device__ short __bfloat162short_rd(__nv_bfloat16);\n-__device__ short __bfloat162short_rn(__nv_bfloat16);\n-__device__ short __bfloat162short_ru(__nv_bfloat16);\n-short __bfloat162short_rz(__nv_bfloat16);\n-__device__ short __bfloat162short_rz(__nv_bfloat16);\n-unsigned char __bfloat162uchar_rz(__nv_bfloat16);\n-__device__ unsigned char __bfloat162uchar_rz(__nv_bfloat16);\n-__device__ unsigned __bfloat162uint_rd(__nv_bfloat16);\n-__device__ unsigned __bfloat162uint_rn(__nv_bfloat16);\n-__device__ unsigned __bfloat162uint_ru(__nv_bfloat16);\n-unsigned __bfloat162uint_rz(__nv_bfloat16);\n-__device__ unsigned __bfloat162uint_rz(__nv_bfloat16);\n-__device__ unsigned long long __bfloat162ull_rd(__nv_bfloat16);\n-__device__ unsigned long long __bfloat162ull_rn(__nv_bfloat16);\n-__device__ unsigned long long __bfloat162ull_ru(__nv_bfloat16);\n-unsigned long long __bfloat162ull_rz(__nv_bfloat16);\n-__device__ unsigned long long __bfloat162ull_rz(__nv_bfloat16);\n-__device__ unsigned short __bfloat162ushort_rd(__nv_bfloat16);\n-__device__ unsigned short __bfloat162ushort_rn(__nv_bfloat16);\n-__device__ unsigned short __bfloat162ushort_ru(__nv_bfloat16);\n-unsigned short __bfloat162ushort_rz(__nv_bfloat16);\n-__device__ unsigned short __bfloat162ushort_rz(__nv_bfloat16);\n-short __bfloat16_as_short(__nv_bfloat16);\n-__device__ short __bfloat16_as_short(__nv_bfloat16);\n-unsigned short __bfloat16_as_ushort(__nv_bfloat16);\n-__device__ unsigned short __bfloat16_as_ushort(__nv_bfloat16);\n-__nv_bfloat16 __double2bfloat16(double);\n-__device__ __nv_bfloat16 __double2bfloat16(double);\n-__nv_bfloat162 __float22bfloat162_rn(float2);\n-__device__ __nv_bfloat162 __float22bfloat162_rn(float2);\n-__nv_bfloat16 __float2bfloat16(float);\n-__device__ __nv_bfloat16 __float2bfloat16(float);\n-__nv_bfloat162 __float2bfloat162_rn(float);\n-__device__ __nv_bfloat162 __float2bfloat162_rn(float);\n-__nv_bfloat16 __float2bfloat16_rd(float);\n-__device__ __nv_bfloat16 __float2bfloat16_rd(float);\n-__nv_bfloat16 __float2bfloat16_rn(float);\n-__device__ __nv_bfloat16 __float2bfloat16_rn(float);\n-__nv_bfloat16 __float2bfloat16_ru(float);\n-__device__ __nv_bfloat16 __float2bfloat16_ru(float);\n-__nv_bfloat16 __float2bfloat16_rz(float);\n-__device__ __nv_bfloat16 __float2bfloat16_rz(float);\n-__nv_bfloat162 __floats2bfloat162_rn(float, float);\n-__device__ __nv_bfloat162 __floats2bfloat162_rn(float, float);\n-__nv_bfloat162 __halves2bfloat162(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat162 __halves2bfloat162(__nv_bfloat16, __nv_bfloat16);\n-__nv_bfloat16 __high2bfloat16(__nv_bfloat162);\n-__device__ __nv_bfloat16 __high2bfloat16(__nv_bfloat162);\n-__nv_bfloat162 __high2bfloat162(__nv_bfloat162);\n-__device__ __nv_bfloat162 __high2bfloat162(__nv_bfloat162);\n-float __high2float(__nv_bfloat162);\n-__device__ float __high2float(__nv_bfloat162);\n-__nv_bfloat162 __highs2bfloat162(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __highs2bfloat162(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat16 __int2bfloat16_rd(int);\n-__nv_bfloat16 __int2bfloat16_rn(int);\n-__device__ __nv_bfloat16 __int2bfloat16_rn(int);\n-__device__ __nv_bfloat16 __int2bfloat16_ru(int);\n-__device__ __nv_bfloat16 __int2bfloat16_rz(int);\n__device__ __nv_bfloat16 __ldca(const __nv_bfloat16 *);\n__device__ __nv_bfloat162 __ldca(const __nv_bfloat162 *);\n__device__ __nv_bfloat16 __ldcg(const __nv_bfloat16 *);\n__device__ __nv_bfloat162 __ldcg(const __nv_bfloat162 *);\n__device__ __nv_bfloat16 __ldcs(const __nv_bfloat16 *);\n__device__ __nv_bfloat162 __ldcs(const __nv_bfloat162 *);\n__device__ __nv_bfloat16 __ldcv(const __nv_bfloat16 *);\n__device__ __nv_bfloat162 __ldcv(const __nv_bfloat162 *);\n__device__ __nv_bfloat16 __ldg(const __nv_bfloat16 *);\n__device__ __nv_bfloat162 __ldg(const __nv_bfloat162 *);\n__device__ __nv_bfloat16 __ldlu(const __nv_bfloat16 *);\n__device__ __nv_bfloat162 __ldlu(const __nv_bfloat162 *);\n-__device__ __nv_bfloat16 __ll2bfloat16_rd(long long);\n-__nv_bfloat16 __ll2bfloat16_rn(long long);\n-__device__ __nv_bfloat16 __ll2bfloat16_rn(long long);\n-__device__ __nv_bfloat16 __ll2bfloat16_ru(long long);\n-__device__ __nv_bfloat16 __ll2bfloat16_rz(long long);\n-__nv_bfloat16 __low2bfloat16(__nv_bfloat162);\n-__device__ __nv_bfloat16 __low2bfloat16(__nv_bfloat162);\n-__nv_bfloat162 __low2bfloat162(__nv_bfloat162);\n-__device__ __nv_bfloat162 __low2bfloat162(__nv_bfloat162);\n-float __low2float(__nv_bfloat162);\n-__device__ float __low2float(__nv_bfloat162);\n-__nv_bfloat162 __lowhigh2highlow(__nv_bfloat162);\n-__device__ __nv_bfloat162 __lowhigh2highlow(__nv_bfloat162);\n-__nv_bfloat162 __lows2bfloat162(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat162 __lows2bfloat162(__nv_bfloat162, __nv_bfloat162);\n-__device__ __nv_bfloat16 __shfl_down_sync(unsigned, __nv_bfloat16, unsigned, int = warpSize);\n-__device__ __nv_bfloat162 __shfl_down_sync(unsigned, __nv_bfloat162, unsigned, int = warpSize);\n-__device__ __nv_bfloat16 __shfl_sync(unsigned, __nv_bfloat16, int, int = warpSize);\n-__device__ __nv_bfloat162 __shfl_sync(unsigned, __nv_bfloat162, int, int = warpSize);\n-__device__ __nv_bfloat16 __shfl_up_sync(unsigned, __nv_bfloat16, unsigned, int = warpSize);\n-__device__ __nv_bfloat162 __shfl_up_sync(unsigned, __nv_bfloat162, unsigned, int = warpSize);\n-__device__ __nv_bfloat16 __shfl_xor_sync(unsigned, __nv_bfloat16, int, int = warpSize);\n-__device__ __nv_bfloat162 __shfl_xor_sync(unsigned, __nv_bfloat162, int, int = warpSize);\n-__device__ __nv_bfloat16 __short2bfloat16_rd(short);\n-__nv_bfloat16 __short2bfloat16_rn(short);\n-__device__ __nv_bfloat16 __short2bfloat16_rn(short);\n-__device__ __nv_bfloat16 __short2bfloat16_ru(short);\n-__device__ __nv_bfloat16 __short2bfloat16_rz(short);\n-__nv_bfloat16 __short_as_bfloat16(short);\n-__device__ __nv_bfloat16 __short_as_bfloat16(short);\n-__device__ void __stcg(const __nv_bfloat16 *, __nv_bfloat16);\n-__device__ void __stcg(const __nv_bfloat162 *, __nv_bfloat162);\n-__device__ void __stcs(const __nv_bfloat16 *, __nv_bfloat16);\n-__device__ void __stcs(const __nv_bfloat162 *, __nv_bfloat162);\n-__device__ void __stwb(const __nv_bfloat16 *, __nv_bfloat16);\n-__device__ void __stwb(const __nv_bfloat162 *, __nv_bfloat162);\n-__device__ void __stwt(const __nv_bfloat16 *, __nv_bfloat16);\n-__device__ void __stwt(const __nv_bfloat162 *, __nv_bfloat162);\n-__device__ __nv_bfloat16 __uint2bfloat16_rd(unsigned);\n-__nv_bfloat16 __uint2bfloat16_rn(unsigned);\n-__device__ __nv_bfloat16 __uint2bfloat16_rn(unsigned);\n-__device__ __nv_bfloat16 __uint2bfloat16_ru(unsigned);\n-__device__ __nv_bfloat16 __uint2bfloat16_rz(unsigned);\n-__device__ __nv_bfloat16 __ull2bfloat16_rd(unsigned long long);\n-__nv_bfloat16 __ull2bfloat16_rn(unsigned long long);\n-__device__ __nv_bfloat16 __ull2bfloat16_rn(unsigned long long);\n-__device__ __nv_bfloat16 __ull2bfloat16_ru(unsigned long long);\n-__device__ __nv_bfloat16 __ull2bfloat16_rz(unsigned long long);\n-__device__ __nv_bfloat16 __ushort2bfloat16_rd(unsigned short);\n-__nv_bfloat16 __ushort2bfloat16_rn(unsigned short);\n-__device__ __nv_bfloat16 __ushort2bfloat16_rn(unsigned short);\n-__device__ __nv_bfloat16 __ushort2bfloat16_ru(unsigned short);\n-__device__ __nv_bfloat16 __ushort2bfloat16_rz(unsigned short);\n-__nv_bfloat16 __ushort_as_bfloat16(unsigned short);\n-__device__ __nv_bfloat16 __ushort_as_bfloat16(unsigned short);\n-__nv_bfloat162 make_bfloat162(__nv_bfloat16, __nv_bfloat16);\n-__device__ __nv_bfloat162 make_bfloat162(__nv_bfloat16, __nv_bfloat16);\n</code></pre>"},{"location":"manual/api-math/#137-bfloat16-math-functions","title":"1.3.7. Bfloat16 Math Functions","text":"<pre><code>-__device__ __nv_bfloat16 hceil(__nv_bfloat16);\n-__device__ __nv_bfloat16 hcos(__nv_bfloat16);\n-__device__ __nv_bfloat16 hexp(__nv_bfloat16);\n-__device__ __nv_bfloat16 hexp10(__nv_bfloat16);\n-__device__ __nv_bfloat16 hexp2(__nv_bfloat16);\n-__device__ __nv_bfloat16 hfloor(__nv_bfloat16);\n-__device__ __nv_bfloat16 hlog(__nv_bfloat16);\n-__device__ __nv_bfloat16 hlog10(__nv_bfloat16);\n-__device__ __nv_bfloat16 hlog2(__nv_bfloat16);\n-__device__ __nv_bfloat16 hrcp(__nv_bfloat16);\n-__device__ __nv_bfloat16 hrint(__nv_bfloat16);\n-__device__ __nv_bfloat16 hrsqrt(__nv_bfloat16);\n-__device__ __nv_bfloat16 hsin(__nv_bfloat16);\n-__device__ __nv_bfloat16 hsqrt(__nv_bfloat16);\n-__device__ __nv_bfloat16 htrunc(__nv_bfloat16);\n</code></pre>"},{"location":"manual/api-math/#138-bfloat162-math-functions","title":"1.3.8. Bfloat162 Math Functions","text":"<pre><code>-__device__ __nv_bfloat162 h2ceil(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2cos(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2exp(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2exp10(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2exp2(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2floor(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2log(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2log10(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2log2(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2rcp(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2rint(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2rsqrt(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2sin(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2sqrt(__nv_bfloat162);\n-__device__ __nv_bfloat162 h2trunc(__nv_bfloat162);\n</code></pre>"},{"location":"manual/api-math/#14-mathematical-functions","title":"1.4. Mathematical Functions","text":"<p>No matches in this section.</p>"},{"location":"manual/api-math/#15-single-precision-mathematical-functions","title":"1.5. Single Precision Mathematical Functions","text":"<pre><code>__device__ float acosf(float);\n__device__ float acoshf(float);\n__device__ float asinf(float);\n__device__ float asinhf(float);\n__device__ float atan2f(float, float);\n__device__ float atanf(float);\n__device__ float atanhf(float);\n__device__ float cbrtf(float);\n__device__ float ceilf(float);\n__device__ float copysignf(float, float);\n__device__ float cosf(float);\n__device__ float coshf(float);\n__device__ float cospif(float);\n-__device__ float cyl_bessel_i0f(float);\n-__device__ float cyl_bessel_i1f(float);\n__device__ float erfcf(float);\n__device__ float erfcinvf(float);\n-__device__ float erfcxf(float);\n__device__ float erff(float);\n__device__ float erfinvf(float);\n__device__ float exp10f(float);\n__device__ float exp2f(float);\n__device__ float expf(float);\n__device__ float expm1f(float);\n__device__ float fabsf(float);\n__device__ float fdimf(float, float);\n__device__ float fdividef(float, float);\n__device__ float floorf(float);\n__device__ float fmaf(float, float, float);\n__device__ float fmaxf(float, float);\n__device__ float fminf(float, float);\n__device__ float fmodf(float, float);\n__device__ float frexpf(float, int *);\n__device__ float hypotf(float, float);\n__device__ int ilogbf(float);\n-__device__ int isfinite(float);\n-__device__ int isinf(float);\n-__device__ int isnan(float);\n-__device__ float j0f(float);\n-__device__ float j1f(float);\n-__device__ float jnf(int, float);\n__device__ float ldexpf(float, int);\n__device__ float lgammaf(float);\n-__device__ long long llrintf(float);\n__device__ long long llroundf(float);\n__device__ float log10f(float);\n__device__ float log1pf(float);\n__device__ float log2f(float);\n__device__ float logbf(float);\n__device__ float logf(float);\n-__device__ long lrintf(float);\n__device__ long lroundf(float);\n__device__ float max(float, float);\n__device__ float min(float, float);\n__device__ float modff(float, float *);\n__device__ float nanf(const char *);\n__device__ float nearbyintf(float);\n-__device__ float nextafterf(float, float);\n__device__ float norm3df(float, float, float);\n__device__ float norm4df(float, float, float, float);\n-__device__ float normcdff(float);\n-__device__ float normcdfinvf(float);\n__device__ float normf(int, const float *);\n__device__ float powf(float, float);\n-__device__ float rcbrtf(float);\n__device__ float remainderf(float, float);\n__device__ float remquof(float, float, int *);\n__device__ float rhypotf(float, float);\n__device__ float rintf(float);\n__device__ float rnorm3df(float, float, float);\n__device__ float rnorm4df(float, float, float, float);\n__device__ float rnormf(int, const float *);\n__device__ float roundf(float);\n__device__ float rsqrtf(float);\n__device__ float scalblnf(float, long);\n__device__ float scalbnf(float, int);\n__device__ int signbit(float);\n__device__ void sincosf(float, float *, float *);\n__device__ void sincospif(float, float *, float *);\n__device__ float sinf(float);\n__device__ float sinhf(float);\n__device__ float sinpif(float);\n__device__ float sqrtf(float);\n__device__ float tanf(float);\n__device__ float tanhf(float);\n-__device__ float tgammaf(float);\n__device__ float truncf(float);\n-__device__ float y0f(float);\n-__device__ float y1f(float);\n-__device__ float ynf(int, float);\n</code></pre>"},{"location":"manual/api-math/#16-double-precision-mathematical-functions","title":"1.6. Double Precision Mathematical Functions","text":"<pre><code>__device__ double acos(double);\n__device__ double acosh(double);\n__device__ double asin(double);\n__device__ double asinh(double);\n__device__ double atan(double);\n__device__ double atan2(double, double);\n__device__ double atanh(double);\n__device__ double cbrt(double);\n__device__ double ceil(double);\n__device__ double copysign(double, double);\n__device__ double cos(double);\n__device__ double cosh(double);\n__device__ double cospi(double);\n-__device__ double cyl_bessel_i0(double);\n-__device__ double cyl_bessel_i1(double);\n__device__ double erf(double);\n__device__ double erfc(double);\n__device__ double erfcinv(double);\n-__device__ double erfcx(double);\n__device__ double erfinv(double);\n__device__ double exp(double);\n__device__ double exp10(double);\n__device__ double exp2(double);\n__device__ double expm1(double);\n__device__ double fabs(double);\n__device__ double fdim(double, double);\n__device__ double floor(double);\n__device__ double fma(double, double, double);\n__device__ double fmax(double, double);\n__device__ double fmin(double, double);\n__device__ double fmod(double, double);\n__device__ double frexp(double, int *);\n__device__ double hypot(double, double);\n__device__ int ilogb(double);\n-__device__ int isfinite(double);\n-__device__ int isinf(double);\n-__device__ int isnan(double);\n-__device__ double j0(double);\n-__device__ double j1(double);\n-__device__ double jn(int, double);\n__device__ double ldexp(double, int);\n__device__ double lgamma(double);\n__device__ long long llrint(double);\n__device__ long long llround(double);\n__device__ double log(double);\n__device__ double log10(double);\n__device__ double log1p(double);\n__device__ double log2(double);\n__device__ double logb(double);\n__device__ long lrint(double);\n__device__ long lround(double);\n__device__ double max(double, float);\n__device__ double max(float, double);\n__device__ double max(double, double);\n__device__ double min(double, float);\n__device__ double min(float, double);\n__device__ double min(double, double);\n__device__ double modf(double, double *);\n__device__ double nan(const char *);\n__device__ double nearbyint(double);\n-__device__ double nextafter(double, double);\n__device__ double norm(int, const double *);\n__device__ double norm3d(double, double, double);\n__device__ double norm4d(double, double, double, double);\n-__device__ double normcdf(double);\n-__device__ double normcdfinv(double);\n__device__ double pow(double, double);\n-__device__ double rcbrt(double);\n__device__ double remainder(double, double);\n__device__ double remquo(double, double, int *);\n__device__ double rhypot(double, double);\n__device__ double rint(double);\n__device__ double rnorm(int, const double *);\n__device__ double rnorm3d(double, double, double);\n__device__ double rnorm4d(double, double, double, double);\n__device__ double round(double);\n__device__ double rsqrt(double);\n__device__ double scalbln(double, long);\n__device__ double scalbn(double, int);\n__device__ int signbit(double);\n__device__ double sin(double);\n__device__ void sincos(double, double *, double *);\n__device__ void sincospi(double, double *, double *);\n__device__ double sinh(double);\n__device__ double sinpi(double);\n__device__ double sqrt(double);\n__device__ double tan(double);\n__device__ double tanh(double);\n-__device__ double tgamma(double);\n__device__ double trunc(double);\n-__device__ double y0(double);\n-__device__ double y1(double);\n-__device__ double yn(int, double);\n</code></pre>"},{"location":"manual/api-math/#17-integer-mathematical-functions","title":"1.7. Integer Mathematical Functions","text":"<pre><code>-__device__ int abs(int);\n__device__ long labs(long);\n__device__ long long llabs(long long);\n__device__ long long llmax(long long, long long);\n__device__ long long llmin(long long, long long);\n__device__ unsigned long long max(unsigned long long, long long);\n__device__ unsigned long long max(long long, unsigned long long);\n__device__ unsigned long long max(unsigned long long, unsigned long long);\n__device__ long long max(long long, long long);\n__device__ unsigned long max(unsigned long, long);\n__device__ unsigned long max(long, unsigned long);\n__device__ unsigned long max(unsigned long, unsigned long);\n__device__ long max(long, long);\n__device__ unsigned max(unsigned, int);\n__device__ unsigned max(int, unsigned);\n__device__ unsigned max(unsigned, unsigned);\n__device__ int max(int, int);\n__device__ unsigned long long min(unsigned long long, long long);\n__device__ unsigned long long min(long long, unsigned long long);\n__device__ unsigned long long min(unsigned long long, unsigned long long);\n__device__ long long min(long long, long long);\n__device__ unsigned long min(unsigned long, long);\n__device__ unsigned long min(long, unsigned long);\n__device__ unsigned long min(unsigned long, unsigned long);\n__device__ long min(long, long);\n__device__ unsigned min(unsigned, int);\n__device__ unsigned min(int, unsigned);\n__device__ unsigned min(unsigned, unsigned);\n__device__ int min(int, int);\n__device__ unsigned long long ullmax(unsigned long long, unsigned long long);\n__device__ unsigned long long ullmin(unsigned long long, unsigned long long);\n__device__ unsigned umax(unsigned, unsigned);\n__device__ unsigned umin(unsigned, unsigned);\n</code></pre>"},{"location":"manual/api-math/#18-single-precision-intrinsics","title":"1.8. Single Precision Intrinsics","text":"<pre><code>__device__ float __cosf(float);\n__device__ float __exp10f(float);\n__device__ float __expf(float);\n__device__ float __fadd_rd(float, float);\n__device__ float __fadd_rn(float, float);\n__device__ float __fadd_ru(float, float);\n__device__ float __fadd_rz(float, float);\n__device__ float __fdiv_rd(float, float);\n__device__ float __fdiv_rn(float, float);\n__device__ float __fdiv_ru(float, float);\n__device__ float __fdiv_rz(float, float);\n__device__ float __fdividef(float, float);\n__device__ float __fmaf_ieee_rd(float, float, float);\n__device__ float __fmaf_ieee_rn(float, float, float);\n__device__ float __fmaf_ieee_ru(float, float, float);\n__device__ float __fmaf_ieee_rz(float, float, float);\n__device__ float __fmaf_rd(float, float, float);\n__device__ float __fmaf_rn(float, float, float);\n__device__ float __fmaf_ru(float, float, float);\n__device__ float __fmaf_rz(float, float, float);\n__device__ float __fmul_rd(float, float);\n__device__ float __fmul_rn(float, float);\n__device__ float __fmul_ru(float, float);\n__device__ float __fmul_rz(float, float);\n__device__ float __frcp_rd(float);\n__device__ float __frcp_rn(float);\n__device__ float __frcp_ru(float);\n__device__ float __frcp_rz(float);\n__device__ float __frsqrt_rn(float);\n__device__ float __fsqrt_rd(float);\n__device__ float __fsqrt_rn(float);\n__device__ float __fsqrt_ru(float);\n__device__ float __fsqrt_rz(float);\n__device__ float __fsub_rd(float, float);\n__device__ float __fsub_rn(float, float);\n__device__ float __fsub_ru(float, float);\n__device__ float __fsub_rz(float, float);\n__device__ float __log10f(float);\n__device__ float __log2f(float);\n__device__ float __logf(float);\n__device__ float __powf(float, float);\n__device__ float __saturatef(float);\n__device__ void __sincosf(float, float *, float *);\n__device__ float __sinf(float);\n__device__ float __tanf(float);\n</code></pre>"},{"location":"manual/api-math/#19-double-precision-intrinsics","title":"1.9. Double Precision Intrinsics","text":"<pre><code>__device__ double __dadd_rd(double, double);\n__device__ double __dadd_rn(double, double);\n__device__ double __dadd_ru(double, double);\n__device__ double __dadd_rz(double, double);\n__device__ double __ddiv_rd(double, double);\n__device__ double __ddiv_rn(double, double);\n__device__ double __ddiv_ru(double, double);\n__device__ double __ddiv_rz(double, double);\n__device__ double __dmul_rd(double, double);\n__device__ double __dmul_rn(double, double);\n__device__ double __dmul_ru(double, double);\n__device__ double __dmul_rz(double, double);\n__device__ double __drcp_rd(double);\n__device__ double __drcp_rn(double);\n__device__ double __drcp_ru(double);\n__device__ double __drcp_rz(double);\n__device__ double __dsqrt_rd(double);\n__device__ double __dsqrt_rn(double);\n__device__ double __dsqrt_ru(double);\n__device__ double __dsqrt_rz(double);\n__device__ double __dsub_rd(double, double);\n__device__ double __dsub_rn(double, double);\n__device__ double __dsub_ru(double, double);\n__device__ double __dsub_rz(double, double);\n__device__ double __fma_rd(double, double, double);\n__device__ double __fma_rn(double, double, double);\n__device__ double __fma_ru(double, double, double);\n__device__ double __fma_rz(double, double, double);\n</code></pre>"},{"location":"manual/api-math/#110-integer-intrinsics","title":"1.10. Integer Intrinsics","text":"<pre><code>__device__ unsigned __brev(unsigned);\n__device__ unsigned long long __brevll(unsigned long long);\n__device__ unsigned __byte_perm(unsigned, unsigned, unsigned);\n__device__ int __clz(int);\n__device__ int __clzll(long long);\n__device__ unsigned __dp2a_hi(ushort2, uchar4, unsigned);\n__device__ int __dp2a_hi(short2, char4, int);\n__device__ unsigned __dp2a_hi(unsigned, unsigned, unsigned);\n__device__ int __dp2a_hi(int, int, int);\n__device__ unsigned __dp2a_lo(ushort2, uchar4, unsigned);\n__device__ int __dp2a_lo(short2, char4, int);\n__device__ unsigned __dp2a_lo(unsigned, unsigned, unsigned);\n__device__ int __dp2a_lo(int, int, int);\n__device__ unsigned __dp4a(uchar4, uchar4, unsigned);\n__device__ int __dp4a(char4, char4, int);\n__device__ unsigned __dp4a(unsigned, unsigned, unsigned);\n__device__ int __dp4a(int, int, int);\n__device__ int __ffs(int);\n__device__ int __ffsll(long long);\n-__device__ unsigned __fns(unsigned, unsigned, int);\n__device__ unsigned __funnelshift_l(unsigned, unsigned, unsigned);\n__device__ unsigned __funnelshift_lc(unsigned, unsigned, unsigned);\n__device__ unsigned __funnelshift_r(unsigned, unsigned, unsigned);\n__device__ unsigned __funnelshift_rc(unsigned, unsigned, unsigned);\n__device__ int __hadd(int, int);\n__device__ int __mul24(int, int);\n__device__ long long __mul64hi(long long, long long);\n__device__ int __mulhi(int, int);\n__device__ int __popc(unsigned);\n__device__ int __popcll(unsigned long long);\n__device__ int __rhadd(int, int);\n-__device__ unsigned __sad(int, int, unsigned);\n__device__ unsigned __uhadd(unsigned, unsigned);\n__device__ unsigned __umul24(unsigned, unsigned);\n__device__ unsigned long long __umul64hi(unsigned long long, unsigned long long);\n__device__ unsigned __umulhi(unsigned, unsigned);\n__device__ unsigned __urhadd(unsigned, unsigned);\n__device__ unsigned __usad(unsigned, unsigned, unsigned);\n</code></pre>"},{"location":"manual/api-math/#111-type-casting-intrinsics","title":"1.11. Type Casting Intrinsics","text":"<pre><code>-__device__ float __double2float_rd(double);\n-__device__ float __double2float_rn(double);\n-__device__ float __double2float_ru(double);\n-__device__ float __double2float_rz(double);\n__device__ int __double2hiint(double);\n__device__ int __double2int_rd(double);\n__device__ int __double2int_rn(double);\n__device__ int __double2int_ru(double);\n__device__ int __double2int_rz(double);\n__device__ long long __double2ll_rd(double);\n__device__ long long __double2ll_rn(double);\n__device__ long long __double2ll_ru(double);\n__device__ long long __double2ll_rz(double);\n__device__ int __double2loint(double);\n__device__ unsigned __double2uint_rd(double);\n__device__ unsigned __double2uint_rn(double);\n__device__ unsigned __double2uint_ru(double);\n__device__ unsigned __double2uint_rz(double);\n__device__ unsigned long long __double2ull_rd(double);\n__device__ unsigned long long __double2ull_rn(double);\n__device__ unsigned long long __double2ull_ru(double);\n__device__ unsigned long long __double2ull_rz(double);\n__device__ long long __double_as_longlong(double);\n__device__ int __float2int_rd(float);\n__device__ int __float2int_rn(float);\n__device__ int __float2int_ru(float);\n__device__ int __float2int_rz(float);\n__device__ long long __float2ll_rd(float);\n__device__ long long __float2ll_rn(float);\n__device__ long long __float2ll_ru(float);\n__device__ long long __float2ll_rz(float);\n__device__ unsigned __float2uint_rd(float);\n__device__ unsigned __float2uint_rn(float);\n__device__ unsigned __float2uint_ru(float);\n__device__ unsigned __float2uint_rz(float);\n__device__ unsigned long long __float2ull_rd(float);\n__device__ unsigned long long __float2ull_rn(float);\n__device__ unsigned long long __float2ull_ru(float);\n__device__ unsigned long long __float2ull_rz(float);\n__device__ int __float_as_int(float);\n__device__ unsigned __float_as_uint(float);\n__device__ double __hiloint2double(int, int);\n__device__ double __int2double_rn(int);\n-__device__ float __int2float_rd(int);\n-__device__ float __int2float_rn(int);\n-__device__ float __int2float_ru(int);\n-__device__ float __int2float_rz(int);\n__device__ float __int_as_float(int);\n-__device__ double __ll2double_rd(long long);\n-__device__ double __ll2double_rn(long long);\n-__device__ double __ll2double_ru(long long);\n-__device__ double __ll2double_rz(long long);\n-__device__ float __ll2float_rd(long long);\n-__device__ float __ll2float_rn(long long);\n-__device__ float __ll2float_ru(long long);\n-__device__ float __ll2float_rz(long long);\n__device__ double __longlong_as_double(long long);\n__device__ double __uint2double_rn(unsigned);\n-__device__ float __uint2float_rd(unsigned);\n-__device__ float __uint2float_rn(unsigned);\n-__device__ float __uint2float_ru(unsigned);\n-__device__ float __uint2float_rz(unsigned);\n__device__ float __uint_as_float(unsigned);\n-__device__ double __ull2double_rd(unsigned long long);\n-__device__ double __ull2double_rn(unsigned long long);\n-__device__ double __ull2double_ru(unsigned long long);\n-__device__ double __ull2double_rz(unsigned long long);\n-__device__ float __ull2float_rd(unsigned long long);\n-__device__ float __ull2float_rn(unsigned long long);\n-__device__ float __ull2float_ru(unsigned long long);\n-__device__ float __ull2float_rz(unsigned long long);\n</code></pre>"},{"location":"manual/api-math/#112-simd-intrinsics","title":"1.12. SIMD Intrinsics","text":"<pre><code>__device__ unsigned __vabs2(unsigned);\n__device__ unsigned __vabs4(unsigned);\n__device__ unsigned __vabsdiffs2(unsigned, unsigned);\n__device__ unsigned __vabsdiffs4(unsigned, unsigned);\n__device__ unsigned __vabsdiffu2(unsigned, unsigned);\n__device__ unsigned __vabsdiffu4(unsigned, unsigned);\n__device__ unsigned __vabsss2(unsigned);\n__device__ unsigned __vabsss4(unsigned);\n__device__ unsigned __vadd2(unsigned, unsigned);\n__device__ unsigned __vadd4(unsigned, unsigned);\n__device__ unsigned __vaddss2(unsigned, unsigned);\n__device__ unsigned __vaddss4(unsigned, unsigned);\n__device__ unsigned __vaddus2(unsigned, unsigned);\n__device__ unsigned __vaddus4(unsigned, unsigned);\n__device__ unsigned __vavgs2(unsigned, unsigned);\n__device__ unsigned __vavgs4(unsigned, unsigned);\n__device__ unsigned __vavgu2(unsigned, unsigned);\n__device__ unsigned __vavgu4(unsigned, unsigned);\n__device__ unsigned __vcmpeq2(unsigned, unsigned);\n__device__ unsigned __vcmpeq4(unsigned, unsigned);\n__device__ unsigned __vcmpges2(unsigned, unsigned);\n__device__ unsigned __vcmpges4(unsigned, unsigned);\n__device__ unsigned __vcmpgeu2(unsigned, unsigned);\n__device__ unsigned __vcmpgeu4(unsigned, unsigned);\n__device__ unsigned __vcmpgts2(unsigned, unsigned);\n__device__ unsigned __vcmpgts4(unsigned, unsigned);\n__device__ unsigned __vcmpgtu2(unsigned, unsigned);\n__device__ unsigned __vcmpgtu4(unsigned, unsigned);\n__device__ unsigned __vcmples2(unsigned, unsigned);\n__device__ unsigned __vcmples4(unsigned, unsigned);\n__device__ unsigned __vcmpleu2(unsigned, unsigned);\n__device__ unsigned __vcmpleu4(unsigned, unsigned);\n__device__ unsigned __vcmplts2(unsigned, unsigned);\n__device__ unsigned __vcmplts4(unsigned, unsigned);\n__device__ unsigned __vcmpltu2(unsigned, unsigned);\n__device__ unsigned __vcmpltu4(unsigned, unsigned);\n__device__ unsigned __vcmpne2(unsigned, unsigned);\n__device__ unsigned __vcmpne4(unsigned, unsigned);\n__device__ unsigned __vhaddu2(unsigned, unsigned);\n__device__ unsigned __vhaddu4(unsigned, unsigned);\n-unsigned __viaddmax_s16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmax_s16x2(unsigned, unsigned, unsigned);\n-unsigned __viaddmax_s16x2_relu(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmax_s16x2_relu(unsigned, unsigned, unsigned);\n-int __viaddmax_s32(int, int, int);\n__device__ int __viaddmax_s32(int, int, int);\n-int __viaddmax_s32_relu(int, int, int);\n__device__ int __viaddmax_s32_relu(int, int, int);\n-unsigned __viaddmax_u16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmax_u16x2(unsigned, unsigned, unsigned);\n-unsigned __viaddmax_u32(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmax_u32(unsigned, unsigned, unsigned);\n-unsigned __viaddmin_s16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmin_s16x2(unsigned, unsigned, unsigned);\n-unsigned __viaddmin_s16x2_relu(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmin_s16x2_relu(unsigned, unsigned, unsigned);\n-int __viaddmin_s32(int, int, int);\n__device__ int __viaddmin_s32(int, int, int);\n-int __viaddmin_s32_relu(int, int, int);\n__device__ int __viaddmin_s32_relu(int, int, int);\n-unsigned __viaddmin_u16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmin_u16x2(unsigned, unsigned, unsigned);\n-unsigned __viaddmin_u32(unsigned, unsigned, unsigned);\n__device__ unsigned __viaddmin_u32(unsigned, unsigned, unsigned);\n-unsigned __vibmax_s16x2(unsigned, unsigned, const bool *, const bool *);\n-__device__ unsigned __vibmax_s16x2(unsigned, unsigned, const bool *, const bool *);\n-int __vibmax_s32(int, int, const bool *);\n-__device__ int __vibmax_s32(int, int, const bool *);\n-unsigned __vibmax_u16x2(unsigned, unsigned, const bool *, const bool *);\n-__device__ unsigned __vibmax_u16x2(unsigned, unsigned, const bool *, const bool *);\n-unsigned __vibmax_u32(unsigned, unsigned, const bool *);\n-__device__ unsigned __vibmax_u32(unsigned, unsigned, const bool *);\n-unsigned __vibmin_s16x2(unsigned, unsigned, const bool *, const bool *);\n-__device__ unsigned __vibmin_s16x2(unsigned, unsigned, const bool *, const bool *);\n-int __vibmin_s32(int, int, const bool *);\n-__device__ int __vibmin_s32(int, int, const bool *);\n-unsigned __vibmin_u16x2(unsigned, unsigned, const bool *, const bool *);\n-__device__ unsigned __vibmin_u16x2(unsigned, unsigned, const bool *, const bool *);\n-unsigned __vibmin_u32(unsigned, unsigned, const bool *);\n-__device__ unsigned __vibmin_u32(unsigned, unsigned, const bool *);\n-unsigned __vimax3_s16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __vimax3_s16x2(unsigned, unsigned, unsigned);\n-unsigned __vimax3_s16x2_relu(unsigned, unsigned, unsigned);\n__device__ unsigned __vimax3_s16x2_relu(unsigned, unsigned, unsigned);\n-int __vimax3_s32(int, int, int);\n__device__ int __vimax3_s32(int, int, int);\n-int __vimax3_s32_relu(int, int, int);\n__device__ int __vimax3_s32_relu(int, int, int);\n-unsigned __vimax3_u16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __vimax3_u16x2(unsigned, unsigned, unsigned);\n-unsigned __vimax3_u32(unsigned, unsigned, unsigned);\n__device__ unsigned __vimax3_u32(unsigned, unsigned, unsigned);\n-unsigned __vimax_s16x2_relu(unsigned, unsigned);\n__device__ unsigned __vimax_s16x2_relu(unsigned, unsigned);\n-int __vimax_s32_relu(int, int);\n__device__ int __vimax_s32_relu(int, int);\n-unsigned __vimin3_s16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __vimin3_s16x2(unsigned, unsigned, unsigned);\n-unsigned __vimin3_s16x2_relu(unsigned, unsigned, unsigned);\n__device__ unsigned __vimin3_s16x2_relu(unsigned, unsigned, unsigned);\n-int __vimin3_s32(int, int, int);\n__device__ int __vimin3_s32(int, int, int);\n-int __vimin3_s32_relu(int, int, int);\n__device__ int __vimin3_s32_relu(int, int, int);\n-unsigned __vimin3_u16x2(unsigned, unsigned, unsigned);\n__device__ unsigned __vimin3_u16x2(unsigned, unsigned, unsigned);\n-unsigned __vimin3_u32(unsigned, unsigned, unsigned);\n__device__ unsigned __vimin3_u32(unsigned, unsigned, unsigned);\n-unsigned __vimin_s16x2_relu(unsigned, unsigned);\n__device__ unsigned __vimin_s16x2_relu(unsigned, unsigned);\n-int __vimin_s32_relu(int, int);\n__device__ int __vimin_s32_relu(int, int);\n__device__ unsigned __vmaxs2(unsigned, unsigned);\n__device__ unsigned __vmaxs4(unsigned, unsigned);\n__device__ unsigned __vmaxu2(unsigned, unsigned);\n__device__ unsigned __vmaxu4(unsigned, unsigned);\n__device__ unsigned __vmins2(unsigned, unsigned);\n__device__ unsigned __vmins4(unsigned, unsigned);\n__device__ unsigned __vminu2(unsigned, unsigned);\n__device__ unsigned __vminu4(unsigned, unsigned);\n__device__ unsigned __vneg2(unsigned);\n__device__ unsigned __vneg4(unsigned);\n-__device__ unsigned __vnegss2(unsigned);\n-__device__ unsigned __vnegss4(unsigned);\n-__device__ unsigned __vsads2(unsigned, unsigned);\n-__device__ unsigned __vsads4(unsigned, unsigned);\n__device__ unsigned __vsadu2(unsigned, unsigned);\n__device__ unsigned __vsadu4(unsigned, unsigned);\n__device__ unsigned __vseteq2(unsigned, unsigned);\n__device__ unsigned __vseteq4(unsigned, unsigned);\n__device__ unsigned __vsetges2(unsigned, unsigned);\n__device__ unsigned __vsetges4(unsigned, unsigned);\n__device__ unsigned __vsetgeu2(unsigned, unsigned);\n__device__ unsigned __vsetgeu4(unsigned, unsigned);\n__device__ unsigned __vsetgts2(unsigned, unsigned);\n__device__ unsigned __vsetgts4(unsigned, unsigned);\n__device__ unsigned __vsetgtu2(unsigned, unsigned);\n__device__ unsigned __vsetgtu4(unsigned, unsigned);\n__device__ unsigned __vsetles2(unsigned, unsigned);\n__device__ unsigned __vsetles4(unsigned, unsigned);\n__device__ unsigned __vsetleu2(unsigned, unsigned);\n__device__ unsigned __vsetleu4(unsigned, unsigned);\n__device__ unsigned __vsetlts2(unsigned, unsigned);\n__device__ unsigned __vsetlts4(unsigned, unsigned);\n__device__ unsigned __vsetltu2(unsigned, unsigned);\n__device__ unsigned __vsetltu4(unsigned, unsigned);\n__device__ unsigned __vsetne2(unsigned, unsigned);\n__device__ unsigned __vsetne4(unsigned, unsigned);\n__device__ unsigned __vsub2(unsigned, unsigned);\n__device__ unsigned __vsub4(unsigned, unsigned);\n__device__ unsigned __vsubss2(unsigned, unsigned);\n__device__ unsigned __vsubss4(unsigned, unsigned);\n-__device__ unsigned __vsubus2(unsigned, unsigned);\n-__device__ unsigned __vsubus4(unsigned, unsigned);\n</code></pre>"},{"location":"manual/api-runtime/","title":"Runtime API","text":"<p>Read how this document is structured in the Introduction to implemented APIs.</p>"},{"location":"manual/api-runtime/#61-device-management","title":"6.1. Device Management","text":"<pre><code>cudaError_t cudaChooseDevice(int *, const cudaDeviceProp *);\n-cudaError_t cudaDeviceFlushGPUDirectRDMAWrites(cudaFlushGPUDirectRDMAWritesTarget, cudaFlushGPUDirectRDMAWritesScope);\ncudaError_t cudaDeviceGetAttribute(int *, cudaDeviceAttr, int);\n-__device__ cudaError_t cudaDeviceGetAttribute(int *, cudaDeviceAttr, int);\ncudaError_t cudaDeviceGetByPCIBusId(int *, const char *);\ncudaError_t cudaDeviceGetCacheConfig(cudaFuncCache *);\n__device__ cudaError_t cudaDeviceGetCacheConfig(cudaFuncCache *);\n-cudaError_t cudaDeviceGetDefaultMemPool(cudaMemPool_t *, int);\ncudaError_t cudaDeviceGetLimit(size_t *, cudaLimit);\n-__device__ cudaError_t cudaDeviceGetLimit(size_t *, cudaLimit);\n-cudaError_t cudaDeviceGetMemPool(cudaMemPool_t *, int);\n-cudaError_t cudaDeviceGetNvSciSyncAttributes(void *, int, int);\n-cudaError_t cudaDeviceGetP2PAttribute(int *, cudaDeviceP2PAttr, int, int);\ncudaError_t cudaDeviceGetPCIBusId(char *, int, int);\ncudaError_t cudaDeviceGetStreamPriorityRange(int *, int *);\n-cudaError_t cudaDeviceGetTexture1DLinearMaxWidth(size_t *, const cudaChannelFormatDesc *, int);\n-cudaError_t cudaDeviceRegisterAsyncNotification(int, cudaAsyncCallback, void *, cudaAsyncCallbackHandle_t *);\ncudaError_t cudaDeviceReset();\ncudaError_t cudaDeviceSetCacheConfig(cudaFuncCache);\ncudaError_t cudaDeviceSetLimit(cudaLimit, size_t);\n-cudaError_t cudaDeviceSetMemPool(int, cudaMemPool_t);\ncudaError_t cudaDeviceSynchronize();\n-__device__ cudaError_t cudaDeviceSynchronize();\n-cudaError_t cudaDeviceUnregisterAsyncNotification(int, cudaAsyncCallbackHandle_t);\ncudaError_t cudaGetDevice(int *);\n-__device__ cudaError_t cudaGetDevice(int *);\ncudaError_t cudaGetDeviceCount(int *);\n-__device__ cudaError_t cudaGetDeviceCount(int *);\ncudaError_t cudaGetDeviceFlags(unsigned *);\ncudaError_t cudaGetDeviceProperties(cudaDeviceProp *, int);\ncudaError_t cudaInitDevice(int, unsigned, unsigned);\ncudaError_t cudaIpcCloseMemHandle(void *);\ncudaError_t cudaIpcGetEventHandle(cudaIpcEventHandle_t *, cudaEvent_t);\ncudaError_t cudaIpcGetMemHandle(cudaIpcMemHandle_t *, void *);\ncudaError_t cudaIpcOpenEventHandle(cudaEvent_t *, cudaIpcEventHandle_t);\ncudaError_t cudaIpcOpenMemHandle(void * *, cudaIpcMemHandle_t, unsigned);\ncudaError_t cudaSetDevice(int);\ncudaError_t cudaSetDeviceFlags(unsigned);\ncudaError_t cudaSetValidDevices(int *, int);\n</code></pre>"},{"location":"manual/api-runtime/#62-device-management-deprecated","title":"6.2. Device Management [DEPRECATED]","text":"<pre><code>cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig *);\n__device__ cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig *);\ncudaError_t cudaDeviceSetSharedMemConfig(cudaSharedMemConfig);\n</code></pre>"},{"location":"manual/api-runtime/#63-thread-management-deprecated","title":"6.3. Thread Management [DEPRECATED]","text":"<pre><code>cudaError_t cudaThreadExit();\ncudaError_t cudaThreadGetCacheConfig(cudaFuncCache *);\ncudaError_t cudaThreadGetLimit(size_t *, cudaLimit);\ncudaError_t cudaThreadSetCacheConfig(cudaFuncCache);\ncudaError_t cudaThreadSetLimit(cudaLimit, size_t);\ncudaError_t cudaThreadSynchronize();\n</code></pre>"},{"location":"manual/api-runtime/#64-error-handling","title":"6.4. Error Handling","text":"<pre><code>const char * cudaGetErrorName(cudaError_t);\n__device__ const char * cudaGetErrorName(cudaError_t);\nconst char * cudaGetErrorString(cudaError_t);\n__device__ const char * cudaGetErrorString(cudaError_t);\ncudaError_t cudaGetLastError();\n-__device__ cudaError_t cudaGetLastError();\ncudaError_t cudaPeekAtLastError();\n-__device__ cudaError_t cudaPeekAtLastError();\n</code></pre>"},{"location":"manual/api-runtime/#65-stream-management","title":"6.5. Stream Management","text":"<pre><code>typedef void(* cudaStreamCallback_t)(cudaStream_t, cudaError_t, void *);\ncudaError_t cudaCtxResetPersistingL2Cache();\ncudaError_t cudaStreamAddCallback(cudaStream_t, cudaStreamCallback_t, void *, unsigned);\n-cudaError_t cudaStreamAttachMemAsync(cudaStream_t, void *, size_t = 0, unsigned = cudaMemAttachSingle);\n-cudaError_t cudaStreamBeginCapture(cudaStream_t, cudaStreamCaptureMode);\n-cudaError_t cudaStreamBeginCaptureToGraph(cudaStream_t, cudaGraph_t, const cudaGraphNode_t *, const cudaGraphEdgeData *, size_t, cudaStreamCaptureMode);\n-cudaError_t cudaStreamCopyAttributes(cudaStream_t, cudaStream_t);\ncudaError_t cudaStreamCreate(cudaStream_t *);\ncudaError_t cudaStreamCreateWithFlags(cudaStream_t *, unsigned);\n-__device__ cudaError_t cudaStreamCreateWithFlags(cudaStream_t *, unsigned);\ncudaError_t cudaStreamCreateWithPriority(cudaStream_t *, unsigned, int);\ncudaError_t cudaStreamDestroy(cudaStream_t);\n-__device__ cudaError_t cudaStreamDestroy(cudaStream_t);\n-cudaError_t cudaStreamEndCapture(cudaStream_t, cudaGraph_t *);\n-cudaError_t cudaStreamGetAttribute(cudaStream_t, cudaStreamAttrID, cudaStreamAttrValue *);\n-cudaError_t cudaStreamGetCaptureInfo(cudaStream_t, cudaStreamCaptureStatus *, unsigned long long * = 0, cudaGraph_t * = 0, const cudaGraphNode_t * * = 0, size_t * = 0);\n-cudaError_t cudaStreamGetCaptureInfo_v3(cudaStream_t, cudaStreamCaptureStatus *, unsigned long long * = 0, cudaGraph_t * = 0, const cudaGraphNode_t * * = 0, const cudaGraphEdgeData * * = 0, size_t * = 0);\ncudaError_t cudaStreamGetFlags(cudaStream_t, unsigned *);\ncudaError_t cudaStreamGetId(cudaStream_t, unsigned long long *);\ncudaError_t cudaStreamGetPriority(cudaStream_t, int *);\n-cudaError_t cudaStreamIsCapturing(cudaStream_t, cudaStreamCaptureStatus *);\ncudaError_t cudaStreamQuery(cudaStream_t);\ncudaError_t cudaStreamSetAttribute(cudaStream_t, cudaStreamAttrID, const cudaStreamAttrValue *);\ncudaError_t cudaStreamSynchronize(cudaStream_t);\n-cudaError_t cudaStreamUpdateCaptureDependencies(cudaStream_t, cudaGraphNode_t *, size_t, unsigned = 0);\n-cudaError_t cudaStreamUpdateCaptureDependencies_v2(cudaStream_t, cudaGraphNode_t *, const cudaGraphEdgeData *, size_t, unsigned = 0);\ncudaError_t cudaStreamWaitEvent(cudaStream_t, cudaEvent_t, unsigned = 0);\n-__device__ cudaError_t cudaStreamWaitEvent(cudaStream_t, cudaEvent_t, unsigned = 0);\n-cudaError_t cudaThreadExchangeStreamCaptureMode(cudaStreamCaptureMode *);\n</code></pre>"},{"location":"manual/api-runtime/#66-event-management","title":"6.6. Event Management","text":"<pre><code>cudaError_t cudaEventCreate(cudaEvent_t *);\ncudaError_t cudaEventCreateWithFlags(cudaEvent_t *, unsigned);\n-__device__ cudaError_t cudaEventCreateWithFlags(cudaEvent_t *, unsigned);\ncudaError_t cudaEventDestroy(cudaEvent_t);\n-__device__ cudaError_t cudaEventDestroy(cudaEvent_t);\ncudaError_t cudaEventElapsedTime(float *, cudaEvent_t, cudaEvent_t);\ncudaError_t cudaEventQuery(cudaEvent_t);\ncudaError_t cudaEventRecord(cudaEvent_t, cudaStream_t = 0);\n-__device__ cudaError_t cudaEventRecord(cudaEvent_t, cudaStream_t = 0);\n-cudaError_t cudaEventRecordWithFlags(cudaEvent_t, cudaStream_t = 0, unsigned = 0);\ncudaError_t cudaEventSynchronize(cudaEvent_t);\n</code></pre>"},{"location":"manual/api-runtime/#67-external-resource-interoperability","title":"6.7. External Resource Interoperability","text":"<pre><code>-cudaError_t cudaDestroyExternalMemory(cudaExternalMemory_t);\n-cudaError_t cudaDestroyExternalSemaphore(cudaExternalSemaphore_t);\n-cudaError_t cudaExternalMemoryGetMappedBuffer(void * *, cudaExternalMemory_t, const cudaExternalMemoryBufferDesc *);\n-cudaError_t cudaExternalMemoryGetMappedMipmappedArray(cudaMipmappedArray_t *, cudaExternalMemory_t, const cudaExternalMemoryMipmappedArrayDesc *);\n-cudaError_t cudaImportExternalMemory(cudaExternalMemory_t *, const cudaExternalMemoryHandleDesc *);\n-cudaError_t cudaImportExternalSemaphore(cudaExternalSemaphore_t *, const cudaExternalSemaphoreHandleDesc *);\n-cudaError_t cudaSignalExternalSemaphoresAsync(const cudaExternalSemaphore_t *, const cudaExternalSemaphoreSignalParams *, unsigned, cudaStream_t = 0);\n-cudaError_t cudaWaitExternalSemaphoresAsync(const cudaExternalSemaphore_t *, const cudaExternalSemaphoreWaitParams *, unsigned, cudaStream_t = 0);\n</code></pre>"},{"location":"manual/api-runtime/#68-execution-control","title":"6.8. Execution Control","text":"<pre><code>cudaError_t cudaFuncGetAttributes(cudaFuncAttributes *, const void *);\n-__device__ cudaError_t cudaFuncGetAttributes(cudaFuncAttributes *, const void *);\ncudaError_t cudaFuncGetName(const char * *, const void *);\n-cudaError_t cudaFuncGetParamInfo(const void *, size_t, size_t *, size_t *);\ncudaError_t cudaFuncSetAttribute(const void *, cudaFuncAttribute, int);\ncudaError_t cudaFuncSetCacheConfig(const void *, cudaFuncCache);\n-__device__ void * cudaGetParameterBuffer(size_t, size_t);\n-__device__ void cudaGridDependencySynchronize();\ncudaError_t cudaLaunchCooperativeKernel(const void *, dim3, dim3, void * *, size_t, cudaStream_t);\n-cudaError_t cudaLaunchCooperativeKernelMultiDevice(cudaLaunchParams *, unsigned, unsigned = 0);\n-__device__ cudaError_t cudaLaunchDevice(void *, void *, dim3, dim3, unsigned, cudaStream_t);\ncudaError_t cudaLaunchHostFunc(cudaStream_t, cudaHostFn_t, void *);\ncudaError_t cudaLaunchKernel(const void *, dim3, dim3, void * *, size_t, cudaStream_t);\ncudaError_t cudaLaunchKernelExC(const cudaLaunchConfig_t *, const void *, void * *);\ncudaError_t cudaSetDoubleForDevice(double *);\ncudaError_t cudaSetDoubleForHost(double *);\n-__device__ void cudaTriggerProgrammaticLaunchCompletion();\n</code></pre>"},{"location":"manual/api-runtime/#69-execution-control-deprecated","title":"6.9. Execution Control [DEPRECATED]","text":"<pre><code>cudaError_t cudaFuncSetSharedMemConfig(const void *, cudaSharedMemConfig);\n</code></pre>"},{"location":"manual/api-runtime/#610-occupancy","title":"6.10. Occupancy","text":"<pre><code>cudaError_t cudaOccupancyAvailableDynamicSMemPerBlock(size_t *, const void *, int, int);\ncudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessor(int *, const void *, int, size_t);\n-__device__ cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessor(int *, const void *, int, size_t);\ncudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int *, const void *, int, size_t, unsigned);\n-cudaError_t cudaOccupancyMaxActiveClusters(int *, const void *, const cudaLaunchConfig_t *);\n-cudaError_t cudaOccupancyMaxPotentialClusterSize(int *, const void *, const cudaLaunchConfig_t *);\n</code></pre>"},{"location":"manual/api-runtime/#611-memory-management","title":"6.11. Memory Management","text":"<pre><code>cudaError_t cudaArrayGetInfo(cudaChannelFormatDesc *, cudaExtent *, unsigned *, cudaArray_t);\n-cudaError_t cudaArrayGetMemoryRequirements(cudaArrayMemoryRequirements *, cudaArray_t, int);\n-cudaError_t cudaArrayGetPlane(cudaArray_t *, cudaArray_t, unsigned);\n-cudaError_t cudaArrayGetSparseProperties(cudaArraySparseProperties *, cudaArray_t);\ncudaError_t cudaFree(void *);\n__device__ cudaError_t cudaFree(void *);\ncudaError_t cudaFreeArray(cudaArray_t);\ncudaError_t cudaFreeHost(void *);\n-cudaError_t cudaFreeMipmappedArray(cudaMipmappedArray_t);\n-cudaError_t cudaGetMipmappedArrayLevel(cudaArray_t *, cudaMipmappedArray_const_t, unsigned);\ncudaError_t cudaGetSymbolAddress(void * *, const void *);\ncudaError_t cudaGetSymbolSize(size_t *, const void *);\ncudaError_t cudaHostAlloc(void * *, size_t, unsigned);\ncudaError_t cudaHostGetDevicePointer(void * *, void *, unsigned);\n-cudaError_t cudaHostGetFlags(unsigned *, void *);\ncudaError_t cudaHostRegister(void *, size_t, unsigned);\ncudaError_t cudaHostUnregister(void *);\ncudaError_t cudaMalloc(void * *, size_t);\n__device__ cudaError_t cudaMalloc(void * *, size_t);\n-cudaError_t cudaMalloc3D(cudaPitchedPtr *, cudaExtent);\ncudaError_t cudaMalloc3DArray(cudaArray_t *, const cudaChannelFormatDesc *, cudaExtent, unsigned = 0);\ncudaError_t cudaMallocArray(cudaArray_t *, const cudaChannelFormatDesc *, size_t, size_t = 0, unsigned = 0);\ncudaError_t cudaMallocHost(void * *, size_t);\ncudaError_t cudaMallocManaged(void * *, size_t, unsigned = cudaMemAttachGlobal);\n-cudaError_t cudaMallocMipmappedArray(cudaMipmappedArray_t *, const cudaChannelFormatDesc *, cudaExtent, unsigned, unsigned = 0);\ncudaError_t cudaMallocPitch(void * *, size_t *, size_t, size_t);\ncudaError_t cudaMemAdvise(const void *, size_t, cudaMemoryAdvise, int);\ncudaError_t cudaMemAdvise_v2(const void *, size_t, cudaMemoryAdvise, cudaMemLocation);\ncudaError_t cudaMemGetInfo(size_t *, size_t *);\ncudaError_t cudaMemPrefetchAsync(const void *, size_t, int, cudaStream_t = 0);\n-cudaError_t cudaMemPrefetchAsync_v2(const void *, size_t, cudaMemLocation, unsigned, cudaStream_t = 0);\n-cudaError_t cudaMemRangeGetAttribute(void *, size_t, cudaMemRangeAttribute, const void *, size_t);\n-cudaError_t cudaMemRangeGetAttributes(void * *, size_t *, cudaMemRangeAttribute *, size_t, const void *, size_t);\ncudaError_t cudaMemcpy(void *, const void *, size_t, cudaMemcpyKind);\ncudaError_t cudaMemcpy2D(void *, size_t, const void *, size_t, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaMemcpy2DArrayToArray(cudaArray_t, size_t, size_t, cudaArray_const_t, size_t, size_t, size_t, size_t, cudaMemcpyKind = cudaMemcpyDeviceToDevice);\ncudaError_t cudaMemcpy2DAsync(void *, size_t, const void *, size_t, size_t, size_t, cudaMemcpyKind, cudaStream_t = 0);\n-__device__ cudaError_t cudaMemcpy2DAsync(void *, size_t, const void *, size_t, size_t, size_t, cudaMemcpyKind, cudaStream_t = 0);\n-cudaError_t cudaMemcpy2DFromArray(void *, size_t, cudaArray_const_t, size_t, size_t, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaMemcpy2DFromArrayAsync(void *, size_t, cudaArray_const_t, size_t, size_t, size_t, size_t, cudaMemcpyKind, cudaStream_t = 0);\ncudaError_t cudaMemcpy2DToArray(cudaArray_t, size_t, size_t, const void *, size_t, size_t, size_t, cudaMemcpyKind);\ncudaError_t cudaMemcpy2DToArrayAsync(cudaArray_t, size_t, size_t, const void *, size_t, size_t, size_t, cudaMemcpyKind, cudaStream_t = 0);\ncudaError_t cudaMemcpy3D(const cudaMemcpy3DParms *);\ncudaError_t cudaMemcpy3DAsync(const cudaMemcpy3DParms *, cudaStream_t = 0);\n-__device__ cudaError_t cudaMemcpy3DAsync(const cudaMemcpy3DParms *, cudaStream_t = 0);\n-cudaError_t cudaMemcpy3DPeer(const cudaMemcpy3DPeerParms *);\n-cudaError_t cudaMemcpy3DPeerAsync(const cudaMemcpy3DPeerParms *, cudaStream_t = 0);\ncudaError_t cudaMemcpyAsync(void *, const void *, size_t, cudaMemcpyKind, cudaStream_t = 0);\n-__device__ cudaError_t cudaMemcpyAsync(void *, const void *, size_t, cudaMemcpyKind, cudaStream_t = 0);\ncudaError_t cudaMemcpyFromSymbol(void *, const void *, size_t, size_t = 0, cudaMemcpyKind = cudaMemcpyDeviceToHost);\ncudaError_t cudaMemcpyFromSymbolAsync(void *, const void *, size_t, size_t, cudaMemcpyKind, cudaStream_t = 0);\n-cudaError_t cudaMemcpyPeer(void *, int, const void *, int, size_t);\n-cudaError_t cudaMemcpyPeerAsync(void *, int, const void *, int, size_t, cudaStream_t = 0);\ncudaError_t cudaMemcpyToSymbol(const void *, const void *, size_t, size_t = 0, cudaMemcpyKind = cudaMemcpyHostToDevice);\ncudaError_t cudaMemcpyToSymbolAsync(const void *, const void *, size_t, size_t, cudaMemcpyKind, cudaStream_t = 0);\ncudaError_t cudaMemset(void *, int, size_t);\ncudaError_t cudaMemset2D(void *, size_t, int, size_t, size_t);\ncudaError_t cudaMemset2DAsync(void *, size_t, int, size_t, size_t, cudaStream_t = 0);\n-__device__ cudaError_t cudaMemset2DAsync(void *, size_t, int, size_t, size_t, cudaStream_t = 0);\n-cudaError_t cudaMemset3D(cudaPitchedPtr, int, cudaExtent);\n-cudaError_t cudaMemset3DAsync(cudaPitchedPtr, int, cudaExtent, cudaStream_t = 0);\n-__device__ cudaError_t cudaMemset3DAsync(cudaPitchedPtr, int, cudaExtent, cudaStream_t = 0);\ncudaError_t cudaMemsetAsync(void *, int, size_t, cudaStream_t = 0);\n-__device__ cudaError_t cudaMemsetAsync(void *, int, size_t, cudaStream_t = 0);\n-cudaError_t cudaMipmappedArrayGetMemoryRequirements(cudaArrayMemoryRequirements *, cudaMipmappedArray_t, int);\n-cudaError_t cudaMipmappedArrayGetSparseProperties(cudaArraySparseProperties *, cudaMipmappedArray_t);\ncudaExtent make_cudaExtent(size_t, size_t, size_t);\ncudaPitchedPtr make_cudaPitchedPtr(void *, size_t, size_t, size_t);\ncudaPos make_cudaPos(size_t, size_t, size_t);\n</code></pre>"},{"location":"manual/api-runtime/#612-memory-management-deprecated","title":"6.12. Memory Management [DEPRECATED]","text":"<pre><code>-cudaError_t cudaMemcpyArrayToArray(cudaArray_t, size_t, size_t, cudaArray_const_t, size_t, size_t, size_t, cudaMemcpyKind = cudaMemcpyDeviceToDevice);\n-cudaError_t cudaMemcpyFromArray(void *, cudaArray_const_t, size_t, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaMemcpyFromArrayAsync(void *, cudaArray_const_t, size_t, size_t, size_t, cudaMemcpyKind, cudaStream_t = 0);\n-cudaError_t cudaMemcpyToArray(cudaArray_t, size_t, size_t, const void *, size_t, cudaMemcpyKind);\n-cudaError_t cudaMemcpyToArrayAsync(cudaArray_t, size_t, size_t, const void *, size_t, cudaMemcpyKind, cudaStream_t = 0);\n</code></pre>"},{"location":"manual/api-runtime/#613-stream-ordered-memory-allocator","title":"6.13. Stream Ordered Memory Allocator","text":"<pre><code>cudaError_t cudaFreeAsync(void *, cudaStream_t);\ncudaError_t cudaMallocAsync(void * *, size_t, cudaStream_t);\n-cudaError_t cudaMallocFromPoolAsync(void * *, size_t, cudaMemPool_t, cudaStream_t);\n-cudaError_t cudaMemPoolCreate(cudaMemPool_t *, const cudaMemPoolProps *);\n-cudaError_t cudaMemPoolDestroy(cudaMemPool_t);\n-cudaError_t cudaMemPoolExportPointer(cudaMemPoolPtrExportData *, void *);\n-cudaError_t cudaMemPoolExportToShareableHandle(void *, cudaMemPool_t, cudaMemAllocationHandleType, unsigned);\n-cudaError_t cudaMemPoolGetAccess(cudaMemAccessFlags *, cudaMemPool_t, cudaMemLocation *);\n-cudaError_t cudaMemPoolGetAttribute(cudaMemPool_t, cudaMemPoolAttr, void *);\n-cudaError_t cudaMemPoolImportFromShareableHandle(cudaMemPool_t *, void *, cudaMemAllocationHandleType, unsigned);\n-cudaError_t cudaMemPoolImportPointer(void * *, cudaMemPool_t, cudaMemPoolPtrExportData *);\n-cudaError_t cudaMemPoolSetAccess(cudaMemPool_t, const cudaMemAccessDesc *, size_t);\n-cudaError_t cudaMemPoolSetAttribute(cudaMemPool_t, cudaMemPoolAttr, void *);\n-cudaError_t cudaMemPoolTrimTo(cudaMemPool_t, size_t);\n</code></pre>"},{"location":"manual/api-runtime/#614-unified-addressing","title":"6.14. Unified Addressing","text":"<pre><code>cudaError_t cudaPointerGetAttributes(cudaPointerAttributes *, const void *);\n</code></pre>"},{"location":"manual/api-runtime/#615-peer-device-memory-access","title":"6.15. Peer Device Memory Access","text":"<pre><code>cudaError_t cudaDeviceCanAccessPeer(int *, int, int);\ncudaError_t cudaDeviceDisablePeerAccess(int);\ncudaError_t cudaDeviceEnablePeerAccess(int, unsigned);\n</code></pre>"},{"location":"manual/api-runtime/#616-opengl-interoperability","title":"6.16. OpenGL Interoperability","text":"<pre><code>enum cudaGLDeviceList;\n-cudaError_t cudaGLGetDevices(unsigned *, int *, unsigned, cudaGLDeviceList);\n-cudaError_t cudaGraphicsGLRegisterBuffer(cudaGraphicsResource * *, GLuint, unsigned);\n-cudaError_t cudaGraphicsGLRegisterImage(cudaGraphicsResource * *, GLuint, GLenum, unsigned);\n-cudaError_t cudaWGLGetDevice(int *, HGPUNV);\n</code></pre>"},{"location":"manual/api-runtime/#617-opengl-interoperability-deprecated","title":"6.17. OpenGL Interoperability [DEPRECATED]","text":"<pre><code>-enum cudaGLMapFlags;\n-cudaError_t cudaGLMapBufferObject(void * *, GLuint);\n-cudaError_t cudaGLMapBufferObjectAsync(void * *, GLuint, cudaStream_t);\n-cudaError_t cudaGLRegisterBufferObject(GLuint);\n-cudaError_t cudaGLSetBufferObjectMapFlags(GLuint, unsigned);\n-cudaError_t cudaGLSetGLDevice(int);\n-cudaError_t cudaGLUnmapBufferObject(GLuint);\n-cudaError_t cudaGLUnmapBufferObjectAsync(GLuint, cudaStream_t);\n-cudaError_t cudaGLUnregisterBufferObject(GLuint);\n</code></pre>"},{"location":"manual/api-runtime/#618-direct3d-9-interoperability","title":"6.18. Direct3D 9 Interoperability","text":"<pre><code>-enum cudaD3D9DeviceList;\n-cudaError_t cudaD3D9GetDevice(int *, const char *);\n-cudaError_t cudaD3D9GetDevices(unsigned *, int *, unsigned, IDirect3DDevice9 *, cudaD3D9DeviceList);\n-cudaError_t cudaD3D9GetDirect3DDevice(IDirect3DDevice9 * *);\n-cudaError_t cudaD3D9SetDirect3DDevice(IDirect3DDevice9 *, int = -1);\n-cudaError_t cudaGraphicsD3D9RegisterResource(cudaGraphicsResource * *, IDirect3DResource9 *, unsigned);\n</code></pre>"},{"location":"manual/api-runtime/#619-direct3d-9-interoperability-deprecated","title":"6.19. Direct3D 9 Interoperability [DEPRECATED]","text":"<pre><code>-enum cudaD3D9MapFlags;\n-enum cudaD3D9RegisterFlags;\n-cudaError_t cudaD3D9MapResources(int, IDirect3DResource9 * *);\n-cudaError_t cudaD3D9RegisterResource(IDirect3DResource9 *, unsigned);\n-cudaError_t cudaD3D9ResourceGetMappedArray(cudaArray * *, IDirect3DResource9 *, unsigned, unsigned);\n-cudaError_t cudaD3D9ResourceGetMappedPitch(size_t *, size_t *, IDirect3DResource9 *, unsigned, unsigned);\n-cudaError_t cudaD3D9ResourceGetMappedPointer(void * *, IDirect3DResource9 *, unsigned, unsigned);\n-cudaError_t cudaD3D9ResourceGetMappedSize(size_t *, IDirect3DResource9 *, unsigned, unsigned);\n-cudaError_t cudaD3D9ResourceGetSurfaceDimensions(size_t *, size_t *, size_t *, IDirect3DResource9 *, unsigned, unsigned);\n-cudaError_t cudaD3D9ResourceSetMapFlags(IDirect3DResource9 *, unsigned);\n-cudaError_t cudaD3D9UnmapResources(int, IDirect3DResource9 * *);\n-cudaError_t cudaD3D9UnregisterResource(IDirect3DResource9 *);\n</code></pre>"},{"location":"manual/api-runtime/#620-direct3d-10-interoperability","title":"6.20. Direct3D 10 Interoperability","text":"<pre><code>-enum cudaD3D10DeviceList;\n-cudaError_t cudaD3D10GetDevice(int *, IDXGIAdapter *);\n-cudaError_t cudaD3D10GetDevices(unsigned *, int *, unsigned, ID3D10Device *, cudaD3D10DeviceList);\n-cudaError_t cudaGraphicsD3D10RegisterResource(cudaGraphicsResource * *, ID3D10Resource *, unsigned);\n</code></pre>"},{"location":"manual/api-runtime/#621-direct3d-10-interoperability-deprecated","title":"6.21. Direct3D 10 Interoperability [DEPRECATED]","text":"<pre><code>-enum cudaD3D10MapFlags;\n-enum cudaD3D10RegisterFlags;\n-cudaError_t cudaD3D10GetDirect3DDevice(ID3D10Device * *);\n-cudaError_t cudaD3D10MapResources(int, ID3D10Resource * *);\n-cudaError_t cudaD3D10RegisterResource(ID3D10Resource *, unsigned);\n-cudaError_t cudaD3D10ResourceGetMappedArray(cudaArray * *, ID3D10Resource *, unsigned);\n-cudaError_t cudaD3D10ResourceGetMappedPitch(size_t *, size_t *, ID3D10Resource *, unsigned);\n-cudaError_t cudaD3D10ResourceGetMappedPointer(void * *, ID3D10Resource *, unsigned);\n-cudaError_t cudaD3D10ResourceGetMappedSize(size_t *, ID3D10Resource *, unsigned);\n-cudaError_t cudaD3D10ResourceGetSurfaceDimensions(size_t *, size_t *, size_t *, ID3D10Resource *, unsigned);\n-cudaError_t cudaD3D10ResourceSetMapFlags(ID3D10Resource *, unsigned);\n-cudaError_t cudaD3D10SetDirect3DDevice(ID3D10Device *, int = -1);\n-cudaError_t cudaD3D10UnmapResources(int, ID3D10Resource * *);\n-cudaError_t cudaD3D10UnregisterResource(ID3D10Resource *);\n</code></pre>"},{"location":"manual/api-runtime/#622-direct3d-11-interoperability","title":"6.22. Direct3D 11 Interoperability","text":"<pre><code>-enum cudaD3D11DeviceList;\n-cudaError_t cudaD3D11GetDevice(int *, IDXGIAdapter *);\n-cudaError_t cudaD3D11GetDevices(unsigned *, int *, unsigned, ID3D11Device *, cudaD3D11DeviceList);\n-cudaError_t cudaGraphicsD3D11RegisterResource(cudaGraphicsResource * *, ID3D11Resource *, unsigned);\n</code></pre>"},{"location":"manual/api-runtime/#623-direct3d-11-interoperability-deprecated","title":"6.23. Direct3D 11 Interoperability [DEPRECATED]","text":"<pre><code>-cudaError_t cudaD3D11GetDirect3DDevice(ID3D11Device * *);\n-cudaError_t cudaD3D11SetDirect3DDevice(ID3D11Device *, int = -1);\n</code></pre>"},{"location":"manual/api-runtime/#624-vdpau-interoperability","title":"6.24. VDPAU Interoperability","text":"<pre><code>-cudaError_t cudaGraphicsVDPAURegisterOutputSurface(cudaGraphicsResource * *, VdpOutputSurface, unsigned);\n-cudaError_t cudaGraphicsVDPAURegisterVideoSurface(cudaGraphicsResource * *, VdpVideoSurface, unsigned);\n-cudaError_t cudaVDPAUGetDevice(int *, VdpDevice, VdpGetProcAddress *);\n-cudaError_t cudaVDPAUSetVDPAUDevice(int, VdpDevice, VdpGetProcAddress *);\n</code></pre>"},{"location":"manual/api-runtime/#625-egl-interoperability","title":"6.25. EGL Interoperability","text":"<pre><code>-cudaError_t cudaEGLStreamConsumerAcquireFrame(cudaEglStreamConnection *, cudaGraphicsResource_t *, cudaStream_t *, unsigned);\n-cudaError_t cudaEGLStreamConsumerConnect(cudaEglStreamConnection *, EGLStreamKHR);\n-cudaError_t cudaEGLStreamConsumerConnectWithFlags(cudaEglStreamConnection *, EGLStreamKHR, unsigned);\n-cudaError_t cudaEGLStreamConsumerDisconnect(cudaEglStreamConnection *);\n-cudaError_t cudaEGLStreamConsumerReleaseFrame(cudaEglStreamConnection *, cudaGraphicsResource_t, cudaStream_t *);\n-cudaError_t cudaEGLStreamProducerConnect(cudaEglStreamConnection *, EGLStreamKHR, EGLint, EGLint);\n-cudaError_t cudaEGLStreamProducerDisconnect(cudaEglStreamConnection *);\n-cudaError_t cudaEGLStreamProducerPresentFrame(cudaEglStreamConnection *, cudaEglFrame, cudaStream_t *);\n-cudaError_t cudaEGLStreamProducerReturnFrame(cudaEglStreamConnection *, cudaEglFrame *, cudaStream_t *);\n-cudaError_t cudaEventCreateFromEGLSync(cudaEvent_t *, EGLSyncKHR, unsigned);\n-cudaError_t cudaGraphicsEGLRegisterImage(cudaGraphicsResource * *, EGLImageKHR, unsigned);\n-cudaError_t cudaGraphicsResourceGetMappedEglFrame(cudaEglFrame *, cudaGraphicsResource_t, unsigned, unsigned);\n</code></pre>"},{"location":"manual/api-runtime/#626-graphics-interoperability","title":"6.26. Graphics Interoperability","text":"<pre><code>cudaError_t cudaGraphicsMapResources(int, cudaGraphicsResource_t *, cudaStream_t = 0);\n-cudaError_t cudaGraphicsResourceGetMappedMipmappedArray(cudaMipmappedArray_t *, cudaGraphicsResource_t);\ncudaError_t cudaGraphicsResourceGetMappedPointer(void * *, size_t *, cudaGraphicsResource_t);\n-cudaError_t cudaGraphicsResourceSetMapFlags(cudaGraphicsResource_t, unsigned);\n-cudaError_t cudaGraphicsSubResourceGetMappedArray(cudaArray_t *, cudaGraphicsResource_t, unsigned, unsigned);\ncudaError_t cudaGraphicsUnmapResources(int, cudaGraphicsResource_t *, cudaStream_t = 0);\ncudaError_t cudaGraphicsUnregisterResource(cudaGraphicsResource_t);\n</code></pre>"},{"location":"manual/api-runtime/#627-texture-object-management","title":"6.27. Texture Object Management","text":"<pre><code>cudaChannelFormatDesc cudaCreateChannelDesc(int, int, int, int, cudaChannelFormatKind);\n-cudaError_t cudaCreateTextureObject(cudaTextureObject_t *, const cudaResourceDesc *, const cudaTextureDesc *, const cudaResourceViewDesc *);\ncudaError_t cudaDestroyTextureObject(cudaTextureObject_t);\n-cudaError_t cudaGetChannelDesc(cudaChannelFormatDesc *, cudaArray_const_t);\n-cudaError_t cudaGetTextureObjectResourceDesc(cudaResourceDesc *, cudaTextureObject_t);\n-cudaError_t cudaGetTextureObjectResourceViewDesc(cudaResourceViewDesc *, cudaTextureObject_t);\n-cudaError_t cudaGetTextureObjectTextureDesc(cudaTextureDesc *, cudaTextureObject_t);\n</code></pre>"},{"location":"manual/api-runtime/#628-surface-object-management","title":"6.28. Surface Object Management","text":"<pre><code>-cudaError_t cudaCreateSurfaceObject(cudaSurfaceObject_t *, const cudaResourceDesc *);\n-cudaError_t cudaDestroySurfaceObject(cudaSurfaceObject_t);\n-cudaError_t cudaGetSurfaceObjectResourceDesc(cudaResourceDesc *, cudaSurfaceObject_t);\n</code></pre>"},{"location":"manual/api-runtime/#629-version-management","title":"6.29. Version Management","text":"<pre><code>cudaError_t cudaDriverGetVersion(int *);\ncudaError_t cudaRuntimeGetVersion(int *);\n__device__ cudaError_t cudaRuntimeGetVersion(int *);\n</code></pre>"},{"location":"manual/api-runtime/#630-graph-management","title":"6.30. Graph Management","text":"<pre><code>-cudaError_t cudaDeviceGetGraphMemAttribute(int, cudaGraphMemAttributeType, void *);\n-cudaError_t cudaDeviceGraphMemTrim(int);\n-cudaError_t cudaDeviceSetGraphMemAttribute(int, cudaGraphMemAttributeType, void *);\n-__device__ cudaGraphExec_t cudaGetCurrentGraphExec();\n-cudaError_t cudaGraphAddChildGraphNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, cudaGraph_t);\n-cudaError_t cudaGraphAddDependencies(cudaGraph_t, const cudaGraphNode_t *, const cudaGraphNode_t *, size_t);\n-cudaError_t cudaGraphAddDependencies_v2(cudaGraph_t, const cudaGraphNode_t *, const cudaGraphNode_t *, const cudaGraphEdgeData *, size_t);\n-cudaError_t cudaGraphAddEmptyNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t);\n-cudaError_t cudaGraphAddEventRecordNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, cudaEvent_t);\n-cudaError_t cudaGraphAddEventWaitNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, cudaEvent_t);\n-cudaError_t cudaGraphAddExternalSemaphoresSignalNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const cudaExternalSemaphoreSignalNodeParams *);\n-cudaError_t cudaGraphAddExternalSemaphoresWaitNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const cudaExternalSemaphoreWaitNodeParams *);\n-cudaError_t cudaGraphAddHostNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const cudaHostNodeParams *);\n-cudaError_t cudaGraphAddKernelNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const cudaKernelNodeParams *);\n-cudaError_t cudaGraphAddMemAllocNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, cudaMemAllocNodeParams *);\n-cudaError_t cudaGraphAddMemFreeNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, void *);\n-cudaError_t cudaGraphAddMemcpyNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const cudaMemcpy3DParms *);\n-cudaError_t cudaGraphAddMemcpyNode1D(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, void *, const void *, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphAddMemcpyNodeFromSymbol(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, void *, const void *, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphAddMemcpyNodeToSymbol(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const void *, const void *, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphAddMemsetNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const cudaMemsetParams *);\n-cudaError_t cudaGraphAddNode(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, cudaGraphNodeParams *);\n-cudaError_t cudaGraphAddNode_v2(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, const cudaGraphEdgeData *, size_t, cudaGraphNodeParams *);\n-cudaError_t cudaGraphChildGraphNodeGetGraph(cudaGraphNode_t, cudaGraph_t *);\n-cudaError_t cudaGraphClone(cudaGraph_t *, cudaGraph_t);\n-cudaError_t cudaGraphConditionalHandleCreate(cudaGraphConditionalHandle *, cudaGraph_t, unsigned = 0, unsigned = 0);\n-cudaError_t cudaGraphCreate(cudaGraph_t *, unsigned);\n-cudaError_t cudaGraphDebugDotPrint(cudaGraph_t, const char *, unsigned);\n-cudaError_t cudaGraphDestroy(cudaGraph_t);\n-cudaError_t cudaGraphDestroyNode(cudaGraphNode_t);\n-cudaError_t cudaGraphEventRecordNodeGetEvent(cudaGraphNode_t, cudaEvent_t *);\n-cudaError_t cudaGraphEventRecordNodeSetEvent(cudaGraphNode_t, cudaEvent_t);\n-cudaError_t cudaGraphEventWaitNodeGetEvent(cudaGraphNode_t, cudaEvent_t *);\n-cudaError_t cudaGraphEventWaitNodeSetEvent(cudaGraphNode_t, cudaEvent_t);\n-cudaError_t cudaGraphExecChildGraphNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, cudaGraph_t);\n-cudaError_t cudaGraphExecDestroy(cudaGraphExec_t);\n-cudaError_t cudaGraphExecEventRecordNodeSetEvent(cudaGraphExec_t, cudaGraphNode_t, cudaEvent_t);\n-cudaError_t cudaGraphExecEventWaitNodeSetEvent(cudaGraphExec_t, cudaGraphNode_t, cudaEvent_t);\n-cudaError_t cudaGraphExecExternalSemaphoresSignalNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, const cudaExternalSemaphoreSignalNodeParams *);\n-cudaError_t cudaGraphExecExternalSemaphoresWaitNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, const cudaExternalSemaphoreWaitNodeParams *);\n-cudaError_t cudaGraphExecGetFlags(cudaGraphExec_t, unsigned long long *);\n-cudaError_t cudaGraphExecHostNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, const cudaHostNodeParams *);\n-cudaError_t cudaGraphExecKernelNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, const cudaKernelNodeParams *);\n-cudaError_t cudaGraphExecMemcpyNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, const cudaMemcpy3DParms *);\n-cudaError_t cudaGraphExecMemcpyNodeSetParams1D(cudaGraphExec_t, cudaGraphNode_t, void *, const void *, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphExecMemcpyNodeSetParamsFromSymbol(cudaGraphExec_t, cudaGraphNode_t, void *, const void *, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphExecMemcpyNodeSetParamsToSymbol(cudaGraphExec_t, cudaGraphNode_t, const void *, const void *, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphExecMemsetNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, const cudaMemsetParams *);\n-cudaError_t cudaGraphExecNodeSetParams(cudaGraphExec_t, cudaGraphNode_t, cudaGraphNodeParams *);\n-cudaError_t cudaGraphExecUpdate(cudaGraphExec_t, cudaGraph_t, cudaGraphExecUpdateResultInfo *);\n-cudaError_t cudaGraphExternalSemaphoresSignalNodeGetParams(cudaGraphNode_t, cudaExternalSemaphoreSignalNodeParams *);\n-cudaError_t cudaGraphExternalSemaphoresSignalNodeSetParams(cudaGraphNode_t, const cudaExternalSemaphoreSignalNodeParams *);\n-cudaError_t cudaGraphExternalSemaphoresWaitNodeGetParams(cudaGraphNode_t, cudaExternalSemaphoreWaitNodeParams *);\n-cudaError_t cudaGraphExternalSemaphoresWaitNodeSetParams(cudaGraphNode_t, const cudaExternalSemaphoreWaitNodeParams *);\n-cudaError_t cudaGraphGetEdges(cudaGraph_t, cudaGraphNode_t *, cudaGraphNode_t *, size_t *);\n-cudaError_t cudaGraphGetEdges_v2(cudaGraph_t, cudaGraphNode_t *, cudaGraphNode_t *, cudaGraphEdgeData *, size_t *);\n-cudaError_t cudaGraphGetNodes(cudaGraph_t, cudaGraphNode_t *, size_t *);\n-cudaError_t cudaGraphGetRootNodes(cudaGraph_t, cudaGraphNode_t *, size_t *);\n-cudaError_t cudaGraphHostNodeGetParams(cudaGraphNode_t, cudaHostNodeParams *);\n-cudaError_t cudaGraphHostNodeSetParams(cudaGraphNode_t, const cudaHostNodeParams *);\n-cudaError_t cudaGraphInstantiate(cudaGraphExec_t *, cudaGraph_t, unsigned long long = 0);\n-cudaError_t cudaGraphInstantiateWithFlags(cudaGraphExec_t *, cudaGraph_t, unsigned long long = 0);\n-cudaError_t cudaGraphInstantiateWithParams(cudaGraphExec_t *, cudaGraph_t, cudaGraphInstantiateParams *);\n-cudaError_t cudaGraphKernelNodeCopyAttributes(cudaGraphNode_t, cudaGraphNode_t);\n-cudaError_t cudaGraphKernelNodeGetAttribute(cudaGraphNode_t, cudaKernelNodeAttrID, cudaKernelNodeAttrValue *);\n-cudaError_t cudaGraphKernelNodeGetParams(cudaGraphNode_t, cudaKernelNodeParams *);\n-cudaError_t cudaGraphKernelNodeSetAttribute(cudaGraphNode_t, cudaKernelNodeAttrID, const cudaKernelNodeAttrValue *);\n-__device__ cudaError_t cudaGraphKernelNodeSetEnabled(cudaGraphDeviceNode_t, bool);\n-__device__ cudaError_t cudaGraphKernelNodeSetGridDim(cudaGraphDeviceNode_t, dim3);\n-template &lt; typename T &gt; __device__ cudaError_t cudaGraphKernelNodeSetParam(cudaGraphDeviceNode_t, size_t, const T &amp;);\n-__device__ cudaError_t cudaGraphKernelNodeSetParam(cudaGraphDeviceNode_t, size_t, const void *, size_t);\n-cudaError_t cudaGraphKernelNodeSetParams(cudaGraphNode_t, const cudaKernelNodeParams *);\n-__device__ cudaError_t cudaGraphKernelNodeUpdatesApply(const cudaGraphKernelNodeUpdate *, size_t);\n-cudaError_t cudaGraphLaunch(cudaGraphExec_t, cudaStream_t);\n-__device__ cudaError_t cudaGraphLaunch(cudaGraphExec_t, cudaStream_t);\n-cudaError_t cudaGraphMemAllocNodeGetParams(cudaGraphNode_t, cudaMemAllocNodeParams *);\n-cudaError_t cudaGraphMemFreeNodeGetParams(cudaGraphNode_t, void *);\n-cudaError_t cudaGraphMemcpyNodeGetParams(cudaGraphNode_t, cudaMemcpy3DParms *);\n-cudaError_t cudaGraphMemcpyNodeSetParams(cudaGraphNode_t, const cudaMemcpy3DParms *);\n-cudaError_t cudaGraphMemcpyNodeSetParams1D(cudaGraphNode_t, void *, const void *, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphMemcpyNodeSetParamsFromSymbol(cudaGraphNode_t, void *, const void *, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphMemcpyNodeSetParamsToSymbol(cudaGraphNode_t, const void *, const void *, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphMemsetNodeGetParams(cudaGraphNode_t, cudaMemsetParams *);\n-cudaError_t cudaGraphMemsetNodeSetParams(cudaGraphNode_t, const cudaMemsetParams *);\n-cudaError_t cudaGraphNodeFindInClone(cudaGraphNode_t *, cudaGraphNode_t, cudaGraph_t);\n-cudaError_t cudaGraphNodeGetDependencies(cudaGraphNode_t, cudaGraphNode_t *, size_t *);\n-cudaError_t cudaGraphNodeGetDependencies_v2(cudaGraphNode_t, cudaGraphNode_t *, cudaGraphEdgeData *, size_t *);\n-cudaError_t cudaGraphNodeGetDependentNodes(cudaGraphNode_t, cudaGraphNode_t *, size_t *);\n-cudaError_t cudaGraphNodeGetDependentNodes_v2(cudaGraphNode_t, cudaGraphNode_t *, cudaGraphEdgeData *, size_t *);\n-cudaError_t cudaGraphNodeGetEnabled(cudaGraphExec_t, cudaGraphNode_t, unsigned *);\n-cudaError_t cudaGraphNodeGetType(cudaGraphNode_t, cudaGraphNodeType *);\n-cudaError_t cudaGraphNodeSetEnabled(cudaGraphExec_t, cudaGraphNode_t, unsigned);\n-cudaError_t cudaGraphNodeSetParams(cudaGraphNode_t, cudaGraphNodeParams *);\n-cudaError_t cudaGraphReleaseUserObject(cudaGraph_t, cudaUserObject_t, unsigned = 1);\n-cudaError_t cudaGraphRemoveDependencies(cudaGraph_t, const cudaGraphNode_t *, const cudaGraphNode_t *, size_t);\n-cudaError_t cudaGraphRemoveDependencies_v2(cudaGraph_t, const cudaGraphNode_t *, const cudaGraphNode_t *, const cudaGraphEdgeData *, size_t);\n-cudaError_t cudaGraphRetainUserObject(cudaGraph_t, cudaUserObject_t, unsigned = 1, unsigned = 0);\n-__device__ void cudaGraphSetConditional(cudaGraphConditionalHandle, unsigned);\n-cudaError_t cudaGraphUpload(cudaGraphExec_t, cudaStream_t);\n-cudaError_t cudaUserObjectCreate(cudaUserObject_t *, void *, cudaHostFn_t, unsigned, unsigned);\n-cudaError_t cudaUserObjectRelease(cudaUserObject_t, unsigned = 1);\n-cudaError_t cudaUserObjectRetain(cudaUserObject_t, unsigned = 1);\n</code></pre>"},{"location":"manual/api-runtime/#631-driver-entry-point-access","title":"6.31. Driver Entry Point Access","text":"<pre><code>-cudaError_t cudaGetDriverEntryPoint(const char *, void * *, unsigned long long, cudaDriverEntryPointQueryResult * = NULL);\n-cudaError_t cudaGetDriverEntryPointByVersion(const char *, void * *, unsigned, unsigned long long, cudaDriverEntryPointQueryResult * = NULL);\n</code></pre>"},{"location":"manual/api-runtime/#632-c-api-routines","title":"6.32. C++ API Routines","text":"<pre><code>-class __cudaOccupancyB2DHelper;\ntemplate &lt; typename T &gt; cudaChannelFormatDesc cudaCreateChannelDesc();\ncudaError_t cudaEventCreate(cudaEvent_t *, unsigned);\ntemplate &lt; typename T &gt; cudaError_t cudaFuncGetAttributes(cudaFuncAttributes *, T *);\n-template &lt; typename T &gt; cudaError_t cudaFuncGetName(const char * *, T *);\ntemplate &lt; typename T &gt; cudaError_t cudaFuncSetAttribute(T *, cudaFuncAttribute, int);\ntemplate &lt; typename T &gt; cudaError_t cudaFuncSetCacheConfig(T *, cudaFuncCache);\n-template &lt; typename T &gt; cudaError_t cudaGetKernel(cudaKernel_t *, T *);\ntemplate &lt; typename T &gt; cudaError_t cudaGetSymbolAddress(void * *, const T &amp;);\ntemplate &lt; typename T &gt; cudaError_t cudaGetSymbolSize(size_t *, const T &amp;);\n-template &lt; typename T &gt; cudaError_t cudaGraphAddMemcpyNodeFromSymbol(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, void *, const T &amp;, size_t, size_t, cudaMemcpyKind);\n-template &lt; typename T &gt; cudaError_t cudaGraphAddMemcpyNodeToSymbol(cudaGraphNode_t *, cudaGraph_t, const cudaGraphNode_t *, size_t, const T &amp;, const void *, size_t, size_t, cudaMemcpyKind);\n-template &lt; typename T &gt; cudaError_t cudaGraphExecMemcpyNodeSetParamsFromSymbol(cudaGraphExec_t, cudaGraphNode_t, void *, const T &amp;, size_t, size_t, cudaMemcpyKind);\n-template &lt; typename T &gt; cudaError_t cudaGraphExecMemcpyNodeSetParamsToSymbol(cudaGraphExec_t, cudaGraphNode_t, const T &amp;, const void *, size_t, size_t, cudaMemcpyKind);\n-cudaError_t cudaGraphInstantiate(cudaGraphExec_t *, cudaGraph_t, cudaGraphNode_t *, char *, size_t);\n-template &lt; typename T &gt; cudaError_t cudaGraphMemcpyNodeSetParamsFromSymbol(cudaGraphNode_t, void *, const T &amp;, size_t, size_t, cudaMemcpyKind);\n-template &lt; typename T &gt; cudaError_t cudaGraphMemcpyNodeSetParamsToSymbol(cudaGraphNode_t, const T &amp;, const void *, size_t, size_t, cudaMemcpyKind);\n-template &lt; typename T &gt; cudaError_t cudaLaunchCooperativeKernel(T *, dim3, dim3, void * *, size_t = 0, cudaStream_t = 0);\n-template &lt; typename T &gt; cudaError_t cudaLaunchKernel(T *, dim3, dim3, void * *, size_t = 0, cudaStream_t = 0);\ntemplate &lt; typename ... ExpTypes, typename ... ActTypes &gt; cudaError_t cudaLaunchKernelEx(const cudaLaunchConfig_t *, void(*)(ExpTypes ...), ActTypes &amp; &amp; ...);\n-cudaError_t cudaMallocAsync(void * *, size_t, cudaMemPool_t, cudaStream_t);\ncudaError_t cudaMallocHost(void * *, size_t, unsigned);\ntemplate &lt; typename T &gt; cudaError_t cudaMallocManaged(T * *, size_t, unsigned = cudaMemAttachGlobal);\n-template &lt; typename T &gt; cudaError_t cudaMemAdvise(T *, size_t, cudaMemoryAdvise, cudaMemLocation);\ntemplate &lt; typename T &gt; cudaError_t cudaMemcpyFromSymbol(void *, const T &amp;, size_t, size_t = 0, cudaMemcpyKind = cudaMemcpyDeviceToHost);\ntemplate &lt; typename T &gt; cudaError_t cudaMemcpyFromSymbolAsync(void *, const T &amp;, size_t, size_t = 0, cudaMemcpyKind = cudaMemcpyDeviceToHost, cudaStream_t = 0);\ntemplate &lt; typename T &gt; cudaError_t cudaMemcpyToSymbol(const T &amp;, const void *, size_t, size_t = 0, cudaMemcpyKind = cudaMemcpyHostToDevice);\ntemplate &lt; typename T &gt; cudaError_t cudaMemcpyToSymbolAsync(const T &amp;, const void *, size_t, size_t = 0, cudaMemcpyKind = cudaMemcpyHostToDevice, cudaStream_t = 0);\n-template &lt; typename T &gt; cudaError_t cudaOccupancyAvailableDynamicSMemPerBlock(size_t *, T *, int, int);\ntemplate &lt; typename T &gt; cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessor(int *, T, int, size_t);\ntemplate &lt; typename T &gt; cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int *, T, int, size_t, unsigned);\n-template &lt; typename T &gt; cudaError_t cudaOccupancyMaxActiveClusters(int *, T *, const cudaLaunchConfig_t *);\ntemplate &lt; typename T &gt; cudaError_t cudaOccupancyMaxPotentialBlockSize(int *, int *, T, size_t = 0, int = 0);\ntemplate &lt; typename UnaryFunction, typename T &gt; cudaError_t cudaOccupancyMaxPotentialBlockSizeVariableSMem(int *, int *, T, UnaryFunction, int = 0);\ntemplate &lt; typename UnaryFunction, typename T &gt; cudaError_t cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int *, int *, T, UnaryFunction, int = 0, unsigned = 0);\ntemplate &lt; typename T &gt; cudaError_t cudaOccupancyMaxPotentialBlockSizeWithFlags(int *, int *, T, size_t = 0, int = 0, unsigned = 0);\n-template &lt; typename T &gt; cudaError_t cudaOccupancyMaxPotentialClusterSize(int *, T *, const cudaLaunchConfig_t *);\ntemplate &lt; typename T &gt; cudaError_t cudaStreamAttachMemAsync(cudaStream_t, T *, size_t = 0, unsigned = cudaMemAttachSingle);\n</code></pre>"},{"location":"manual/api-runtime/#633-interactions-with-the-cuda-driver-api","title":"6.33. Interactions with the CUDA Driver API","text":"<pre><code>-cudaError_t cudaGetFuncBySymbol(cudaFunction_t *, const void *);\n-cudaError_t cudaGetKernel(cudaKernel_t *, const void *);\n</code></pre>"},{"location":"manual/api-runtime/#634-profiler-control","title":"6.34. Profiler Control","text":"<pre><code>-cudaError_t cudaProfilerStart();\n-cudaError_t cudaProfilerStop();\n</code></pre>"},{"location":"manual/api-runtime/#635-data-types-used-by-cuda-runtime","title":"6.35. Data types used by CUDA Runtime","text":"<pre><code>-struct CUuuid_st;\nstruct cudaAccessPolicyWindow;\n-struct cudaArrayMemoryRequirements;\n-struct cudaArraySparseProperties;\n-struct cudaAsyncNotificationInfo_t;\nstruct cudaChannelFormatDesc;\n-struct cudaChildGraphNodeParams;\n-struct cudaConditionalNodeParams;\n-struct cudaDeviceProp;\n-struct cudaEglFrame;\n-struct cudaEglPlaneDesc;\n-struct cudaEventRecordNodeParams;\n-struct cudaEventWaitNodeParams;\n-struct cudaExtent;\n-struct cudaExternalMemoryBufferDesc;\n-struct cudaExternalMemoryHandleDesc;\n-struct cudaExternalMemoryMipmappedArrayDesc;\n-struct cudaExternalSemaphoreHandleDesc;\n-struct cudaExternalSemaphoreSignalNodeParams;\n-struct cudaExternalSemaphoreSignalNodeParamsV2;\n-struct cudaExternalSemaphoreSignalParams;\n-struct cudaExternalSemaphoreSignalParams_v1;\n-struct cudaExternalSemaphoreWaitNodeParams;\n-struct cudaExternalSemaphoreWaitNodeParamsV2;\n-struct cudaExternalSemaphoreWaitParams;\n-struct cudaExternalSemaphoreWaitParams_v1;\n-struct cudaFuncAttributes;\n-struct cudaGraphEdgeData;\n-struct cudaGraphExecUpdateResultInfo;\n-struct cudaGraphInstantiateParams;\n-struct cudaGraphKernelNodeUpdate;\n-struct cudaGraphNodeParams;\n-struct cudaHostNodeParams;\n-struct cudaHostNodeParamsV2;\n-struct cudaIpcEventHandle_t;\n-struct cudaIpcMemHandle_t;\n-struct cudaKernelNodeParams;\n-struct cudaKernelNodeParamsV2;\n-struct cudaLaunchAttribute;\n-union cudaLaunchAttributeValue;\n-struct cudaLaunchConfig_t;\n-struct cudaLaunchMemSyncDomainMap;\n-struct cudaLaunchParams;\n-struct cudaMemAccessDesc;\n-struct cudaMemAllocNodeParams;\n-struct cudaMemAllocNodeParamsV2;\n-struct cudaMemFreeNodeParams;\n-struct cudaMemLocation;\n-struct cudaMemPoolProps;\n-struct cudaMemPoolPtrExportData;\n-struct cudaMemcpy3DParms;\n-struct cudaMemcpy3DPeerParms;\n-struct cudaMemcpyNodeParams;\n-struct cudaMemsetParams;\n-struct cudaMemsetParamsV2;\n-struct cudaPitchedPtr;\n-struct cudaPointerAttributes;\n-struct cudaPos;\n-struct cudaResourceDesc;\n-struct cudaResourceViewDesc;\n-struct cudaTextureDesc;\n-#define CUDA_EGL_MAX_PLANES\n#define CUDA_IPC_HANDLE_SIZE\n-#define cudaArrayColorAttachment\n#define cudaArrayCubemap\n#define cudaArrayDefault\n-#define cudaArrayDeferredMapping\n#define cudaArrayLayered\n-#define cudaArraySparse\n-#define cudaArraySparsePropertiesSingleMipTail\n#define cudaArraySurfaceLoadStore\n#define cudaArrayTextureGather\n#define cudaCooperativeLaunchMultiDeviceNoPostSync\n#define cudaCooperativeLaunchMultiDeviceNoPreSync\n#define cudaCpuDeviceId\n#define cudaDeviceBlockingSync\n#define cudaDeviceLmemResizeToMax\n#define cudaDeviceMapHost\n#define cudaDeviceMask\n#define cudaDeviceScheduleAuto\n#define cudaDeviceScheduleBlockingSync\n#define cudaDeviceScheduleMask\n#define cudaDeviceScheduleSpin\n#define cudaDeviceScheduleYield\n-#define cudaDeviceSyncMemops\n#define cudaEventBlockingSync\n#define cudaEventDefault\n#define cudaEventDisableTiming\n#define cudaEventInterprocess\n#define cudaEventRecordDefault\n#define cudaEventRecordExternal\n#define cudaEventWaitDefault\n#define cudaEventWaitExternal\n-#define cudaExternalMemoryDedicated\n-#define cudaExternalSemaphoreSignalSkipNvSciBufMemSync\n-#define cudaExternalSemaphoreWaitSkipNvSciBufMemSync\n-#define cudaGraphKernelNodePortDefault\n-#define cudaGraphKernelNodePortLaunchCompletion\n-#define cudaGraphKernelNodePortProgrammatic\n#define cudaHostAllocDefault\n#define cudaHostAllocMapped\n#define cudaHostAllocPortable\n#define cudaHostAllocWriteCombined\n#define cudaHostRegisterDefault\n#define cudaHostRegisterIoMemory\n#define cudaHostRegisterMapped\n#define cudaHostRegisterPortable\n-#define cudaHostRegisterReadOnly\n#define cudaInitDeviceFlagsAreValid\n#define cudaInvalidDeviceId\n#define cudaIpcMemLazyEnablePeerAccess\n#define cudaMemAttachGlobal\n#define cudaMemAttachHost\n#define cudaMemAttachSingle\n-#define cudaNvSciSyncAttrSignal\n-#define cudaNvSciSyncAttrWait\n#define cudaOccupancyDefault\n#define cudaOccupancyDisableCachingOverride\n#define cudaPeerAccessDefault\n#define cudaStreamDefault\n#define cudaStreamLegacy\n#define cudaStreamNonBlocking\n#define cudaStreamPerThread\n-typedef cudaArray * cudaArray_const_t;\n-typedef cudaArray * cudaArray_t;\n-typedef cudaAsyncCallbackEntry * cudaAsyncCallbackHandle_t;\n-typedef CUeglStreamConnection_st * cudaEglStreamConnection;\n-typedef enumcudaError cudaError_t;\n-typedef CUevent_st * cudaEvent_t;\n-typedef CUexternalMemory_st * cudaExternalMemory_t;\n-typedef CUexternalSemaphore_st * cudaExternalSemaphore_t;\n-typedef CUfunc_st * cudaFunction_t;\n-typedef unsigned long long cudaGraphConditionalHandle;\n-typedef CUgraphDeviceUpdatableNode_st * cudaGraphDeviceNode_t;\n-typedef CUgraphExec_st * cudaGraphExec_t;\n-typedef CUgraphNode_st * cudaGraphNode_t;\n-typedef CUgraph_st * cudaGraph_t;\n-typedef cudaGraphicsResource * cudaGraphicsResource_t;\ntypedef void(* cudaHostFn_t)(void *);\n-typedef CUkern_st * cudaKernel_t;\n-typedef CUmemPoolHandle_st * cudaMemPool_t;\n-typedef cudaMipmappedArray * cudaMipmappedArray_const_t;\n-typedef cudaMipmappedArray * cudaMipmappedArray_t;\n-typedef CUstream_st * cudaStream_t;\n-typedef unsigned long long cudaSurfaceObject_t;\n-typedef unsigned long long cudaTextureObject_t;\n-typedef CUuserObject_st * cudaUserObject_t;\nenum cudaAccessProperty;\n-enum cudaAsyncNotificationType;\n-enum cudaCGScope;\n-enum cudaChannelFormatKind;\nenum cudaClusterSchedulingPolicy;\n-enum cudaComputeMode;\n-enum cudaDeviceAttr;\n-enum cudaDeviceNumaConfig;\n-enum cudaDeviceP2PAttr;\n-enum cudaDriverEntryPointQueryResult;\n-enum cudaEglColorFormat;\n-enum cudaEglFrameType;\n-enum cudaEglResourceLocationFlags;\n-enum cudaError;\n-enum cudaExternalMemoryHandleType;\n-enum cudaExternalSemaphoreHandleType;\n-enum cudaFlushGPUDirectRDMAWritesOptions;\n-enum cudaFlushGPUDirectRDMAWritesScope;\n-enum cudaFlushGPUDirectRDMAWritesTarget;\n-enum cudaFuncAttribute;\n-enum cudaFuncCache;\n-enum cudaGPUDirectRDMAWritesOrdering;\n-enum cudaGetDriverEntryPointFlags;\n-enum cudaGraphConditionalNodeType;\n-enum cudaGraphDebugDotFlags;\n-enum cudaGraphDependencyType;\n-enum cudaGraphExecUpdateResult;\n-enum cudaGraphInstantiateFlags;\n-enum cudaGraphInstantiateResult;\n-enum cudaGraphKernelNodeField;\n-enum cudaGraphMemAttributeType;\n-enum cudaGraphNodeType;\n-enum cudaGraphicsCubeFace;\nenum cudaGraphicsMapFlags;\nenum cudaGraphicsRegisterFlags;\n-enum cudaLaunchAttributeID;\nenum cudaLaunchMemSyncDomain;\n-enum cudaLimit;\n-enum cudaMemAccessFlags;\n-enum cudaMemAllocationHandleType;\n-enum cudaMemAllocationType;\n-enum cudaMemLocationType;\n-enum cudaMemPoolAttr;\n-enum cudaMemRangeAttribute;\n-enum cudaMemcpyKind;\n-enum cudaMemoryAdvise;\n-enum cudaMemoryType;\n-enum cudaResourceType;\n-enum cudaResourceViewFormat;\n-enum cudaSharedCarveout;\n-enum cudaSharedMemConfig;\nenum cudaStreamCaptureMode;\nenum cudaStreamCaptureStatus;\n-enum cudaStreamUpdateCaptureDependenciesFlags;\n-enum cudaSurfaceBoundaryMode;\n-enum cudaSurfaceFormatMode;\n-enum cudaTextureAddressMode;\n-enum cudaTextureFilterMode;\n-enum cudaTextureReadMode;\n-enum cudaUserObjectFlags;\n-enum cudaUserObjectRetainFlags;\n</code></pre>"},{"location":"manual/apis/","title":"Introduction to implemented APIs","text":"<p>This section of the manual lists CUDA APIs that are implemented by SCALE. It is split in three pages:</p> <ul> <li>List of Driver APIs</li> <li>List of Math APIs</li> <li>List of Runtime API</li> </ul> <p>The lists are based on the official Nvidia documentation and use the same layout. Every heading links back to the page where its entries originate from. We compare those entries against SCALE source code and identify the parts that are present or missing.</p> <p>There are several types of entries:</p> <ul> <li>Macros, for which we only compare the names, as some of the values differ naturally between NVIDIA CUDA and SCALE</li> <li>Types, for which we compare:<ul> <li>their names</li> <li>the exact way they are defined</li> </ul> </li> <li>Functions, for which we compare:<ul> <li>their names</li> <li>types and default values of their arguments (argument names are stripped among some other things)</li> <li>their type arguments if present</li> </ul> </li> </ul> <p>The rules for comparing functions are also applied to function pointer types which are sometimes <code>typedef</code>'d or are themselves listed as function arguments.</p>"},{"location":"manual/apis/#presentation","title":"Presentation","text":"<p>The lists are presented using <code>diff</code> syntax highlighting of code blocks. This allows seeing which entries are available and which may be missing. Missing entries are prefixed with <code>-</code> (a minus) which paints them red in the list.</p> <p>By default, <code>__host__</code> qualifier is assumed for functions, it is removed if present. Functions that are qualified as <code>__host__ __device__</code> are split into two separate entries.</p> <p>Here is an example:</p> <pre><code>const char * cudaGetErrorName(cudaError_t);\n__device__ const char * cudaGetErrorName(cudaError_t);\nconst char * cudaGetErrorString(cudaError_t);\n__device__ const char * cudaGetErrorString(cudaError_t);\ncudaError_t cudaGetLastError();\n-__device__ cudaError_t cudaGetLastError();\ncudaError_t cudaPeekAtLastError();\n-__device__ cudaError_t cudaPeekAtLastError();\n</code></pre> <p>In this example, functions <code>cudaGetErrorName</code> and <code>cudaGetErrorString</code> are available on both host and device. Functions <code>cudaGetLastError</code> and <code>cudaPeekAtLastError</code> are available on host, and are not available on device.</p>"},{"location":"manual/apis/#correctness","title":"Correctness","text":"<p>The lists may say that something is unavailable when it's not the case. This may happen for a few reasons.</p> <p>NVIDIA documentation may differ from what CUDA provides in reality. An example of that is differences in <code>const</code>-ness of some function arguments. In such cases SCALE may be forced to maintain \"bug compatibility\" and the functions stop matching what NVIDIA documentation promises.</p> <p>Many functions are called conditionally and may never get used in certain scenarios. For some of those functions, SCALE may provide an empty implementation. By doing this, SCALE allows more projects pass compilation and linking. We don't want to list such empty functions as available, so we manually mark them as missing to avoid confusion.</p> <p>The code that compares entries from SCALE against NVIDIA documentation may contain imperfections. For this reason, some successful matches may simply get missed.</p> <p>Note that these lists currently don't check members of types such as struct fields or enum variants.</p> <p>Reach out to us if you experience problems for any of these reasons. Possible problems with the entries require our attention on a case-by-case basis. Your feedback will help us find possible inconsistencies and prioritise our work to fix them.</p>"},{"location":"manual/comparison/","title":"Comparison to other solutions","text":""},{"location":"manual/comparison/#hip","title":"HIP","text":"<p>HIP is AMD's answer to CUDA. It is superficially similar to CUDA, providing a similar programming language and similar APIs.  An automatic <code>hipify</code> tool exists to partially automate the process of  rewriting your code from CUDA to HIP.</p> <p>We believe HIP does not solve the \"CUDA compatibility problem\" because:</p> <ul> <li>The CUDA dialect problem. HIP's language is almost    identical to LLVM-dialect CUDA, which is quite different from the dialect    of CUDA accepted by <code>nvcc</code>. Consequently, some CUDA programs fail in    strange ways after porting.</li> <li>HIP has no support for inline PTX <code>asm</code> blocks in CUDA code. These must be   manually removed or guarded by macros. SCALE simply accepts them and   compiles them for AMD.</li> <li>HIP's support for NVIDIA is via wrapper APIs rather than simply using    NVIDIA's tools directly as a SCALE-based solution does.</li> <li><code>hipify</code> is unable to handle many CUDA code constructs, such as complex    macros.</li> </ul> <p>To avoid these issues, many projects end up maintaining separate HIP and  CUDA codebases (or one codebase that converts to HIP or CUDA via complex preprocessor macros).</p>"},{"location":"manual/comparison/#zluda","title":"ZLUDA","text":"<p>ZLUDA is a PTX JIT for AMD GPUs. On program startup, ZLUDA grabs the PTX from the CUDA binary and compiles it for your AMD GPU.</p> <p>ZLUDA is a useful tool for end-users to run CUDA programs on  otherwise-unsupported GPUs, without the involvement of the authors of the  program (or even access to the source code!).</p> <p>There are some downsides:</p> <ul> <li>JIT on startup can lead to startup-time delays.</li> <li>Reliance on dll-injection is a bit \"hacky\", and tends to make antivirus    software angry.</li> <li>ZLUDA's approach to providing AMD support inherently depends on tools    provided by NVIDIA. NVIDIA controls the design of the PTX language and the    compilers that produce it, and manipulate both to optimise outcomes for    their hardware specifically.</li> <li>Compiling source code directly to AMDGPU machine code should   offer greater opportunities for optimisation than working backwards from   PTX that has already been optimised for a specific NVIDIA target.</li> </ul> <p>We believe that ZLUDA fills a useful niche, but that software distributors  should have the power to compile their CUDA source code directly to the  machine code of multiple GPU vendors, without reliance on tools maintained  by NVIDIA.</p>"},{"location":"manual/compute-capabilities/","title":"Compute Capability Mapping","text":"<p>\"Compute capability\" is a numbering system used by NVIDIA's CUDA tools to represent different GPU targets. The value of the <code>__CUDA_ARCH__</code> macro is derived from this, and it's how you communicate with <code>nvcc</code> to request the target to build for.</p> <p>GPUs from other vendors have their own numbering scheme, such as AMD's  <code>gfx1234</code> format.</p> <p>CUDA projects sometimes do numeric comparisons on the compute capability  value to enable/disable features using the preprocessor. This is a problem,  since those comparisons are inherently meaningless when targeting non-NVIDIA  hardware.</p> <p>There is no meaningful mapping between compute capability numbers and the  hardware of other vendors.</p> <p>SCALE addresses this problem by providing a \"CUDA installation directory\"  for each supported GPU target. By default, the <code>nvcc</code> in each of these  directories maps every compute capability number to the corresponding AMD  GPU target.</p> <p>This approach works, but has one obvious downside: it makes fat binaries  unrepresentable.</p> <p>To resolve that, your buildsystem must be at least somewhat SCALE-aware:  compute capabilities are not a sufficiently powerful abstraction to model  the needs of a cross-vendor fat binary.</p> <p>The special <code>gfxany</code> target directory is a \"CUDA installation directory\"  that does not perform this compute capability mapping at all. Instead, you  may provide your own arbitrary mapping from GPU targets to CC-number - or  use no such mapping at all (if your program doesn't use the CC-number for  metaprogramming). We recommend CUDA progras be written using more portable  and reliable means of detecting the existence of features: even within  NVIDIA's universe, the CC number is a rather blunt instrument.</p> <p>The remainder of this document explains how the compute capability mapping  configuration works for users of the <code>gfxany</code> target.</p>"},{"location":"manual/compute-capabilities/#configuration-file-format","title":"Configuration file format","text":"<p>The configuration file is a newline separated list of entries. Each entry consists of an ISA name, e.g: <code>gfx1030</code> and a compute capability represented as an integer, e.g: <code>86</code>, separated by a space. The entries are tried in order, so it's possible to map more than one ISA and compute capability to each other unambiguously. If the space and compute capability are omitted, then the compiler associates all NVIDIA compute capabilities with the specified GPU.</p> <p>If no entry is found for the given GPU or if no configuration file is found, then the library reports a compute capability with a large major version defined by the default numbering scheme.</p> <p>If no entry is found for the given GPU or if no configuration file is found, the compiler does not translate a compute capability to an AMD ISA.</p> <p>Lines starting with <code>#</code> and empty lines are ignored.</p>"},{"location":"manual/compute-capabilities/#example","title":"Example","text":"<pre><code># The library will report compute capability 6.1 for gfx900 devices. The compiler will use gfx900 for `sm_61` or\n# `compute_61`.\ngfx900 61\n\n# The library will report compute capability 8.6 for gfx1030 devices. The compiler will use gfx1030 for any of `sm_80`,\n# `compute_80`, `sm_86`, or `compute_86`.\ngfx1030 86\ngfx1030 80\n\n# The compiler will use gfx1100 for any compute capability other than 6.1, 8.0, or 8.6.\ngfx1100\n</code></pre>"},{"location":"manual/compute-capabilities/#search-locations-for-the-library","title":"Search locations for the library","text":"<p>The library searches for a compute capability map in the following order:</p> <ul> <li>The file pointed the <code>REDSCALE_CCMAP</code> environment variable. It is an error if   this environment variable is set but the   file to which it points does not exist.</li> <li><code>../share/redscale/ccmap.conf</code> relative to the directory   containing <code>libredscale.so</code>. This search location is intended   for users who build different installation trees for different GPUs. Packagers   should not place a configuration here.</li> <li><code>${HOME}/.redscale/ccmap.conf</code></li> <li><code>/etc/redscale/ccmap.conf</code></li> </ul>"},{"location":"manual/compute-capabilities/#search-locations-for-the-compiler","title":"Search locations for the compiler","text":"<p>The compiler searches for a compute capability map in the following order:</p> <ul> <li>The file pointed to by the <code>--cuda-ccmap</code> flag. It is an error if this flag is   given but the file to which it points   does not exist.</li> <li>The file pointed the <code>REDSCALE_CCMAP</code> environment variable. It is an error if   this environment variable is set but the   file to which it points does not exist.</li> <li><code>../share/redscale/ccmap.conf</code> relative to the directory containing the   compiler binary (e.g: <code>nvcc</code>) if that   directory is in a CUDA installation directory. This search location is   intended for users who build different   installation trees for different GPUs. Packagers should not place a   configuration here.</li> </ul>"},{"location":"manual/diagnostic-flags/","title":"Compiler warnings","text":"<p>There are some differences in how NVIDIA's <code>nvcc</code> and the SCALE  compiler in \"nvcc mode\" interpret compiler options relatig to warnings.</p>"},{"location":"manual/diagnostic-flags/#clang-flags","title":"<code>clang++</code> flags","text":"<p>The SCALE compiler accepts all of <code>clang++</code>'s usual flags in addition to those provided by nvcc, except where doing so would create an ambiguity.</p>"},{"location":"manual/diagnostic-flags/#compiler-warnings_1","title":"Compiler warnings","text":"<p>The SCALE compiler has the same default warning behaviour as <code>clang</code>, which  is somewhat more strict than <code>nvcc</code>. Warnings may be disabled with the usual  <code>-Wno-</code> flags documented in the clang diagnostics reference.</p> <p>There may be value in enabling even more warnings to find further issues and improve your code.</p> <p>Note that the end of every compiler warning message tells you the name of  the warning flag it is associated with, such as:</p> <pre><code>warning: implicit conversion from 'int' to 'float' changes value from \n2147483647 to 2147483648 [-Wimplicit-const-int-float-conversion]\n</code></pre> <p>By changing <code>-W</code> to <code>-Wno-</code>, you obtain the flag required to disable that  warning.</p> <p>The SCALE implementation of the CUDA runtime/driver APIs uses <code>[[nodiscard]]</code> for the error return codes, meaning you'll get a warning from code that  ignores potential errors from CUDA APIs. This warning can be disabled via  <code>-Wno-unused-result</code>.</p>"},{"location":"manual/diagnostic-flags/#-werror","title":"<code>-Werror</code>","text":"<p><code>nvcc</code>'s <code>-Werror</code> takes an argument specifying the types of warnings that  should be errors, such as:</p> <pre><code>-Werror reorder,default-stream-launch\n</code></pre> <p>This differs from clang's syntax, which consists of either a lone <code>-Werror</code>  to make all warnings into errors, or a set of <code>-Werror=name</code> flags to make  specific things into errors.</p> <p>In <code>nvcc</code> mode, the SCALE compiler accepts only the <code>nvcc</code> syntax, but  allows the same set of diagnostic names accepted by <code>clang</code> (as well  as the special names supported by NVIDIA's <code>nvcc</code>). For example:</p> <pre><code>nvcc -Werror=documentation,implicit-int-conversion foo.cu\nclang++ -Werror=documentation -Werror=implicit-int-conversion foo.cu\n</code></pre> <p>Since SCALE enables more warnings than nvcc does by default, many projects  using <code>-Werror</code> with nvcc will not compile without either disabling the flag  or fixing the underlying code issues.</p>"},{"location":"manual/diagnostic-flags/#diagnostic-control-pragmas","title":"Diagnostic control pragmas","text":"<p>The SCALE compiler does not currently support <code>#pragma nv_diag_suppress</code> or <code>#pragma diag_suppress</code> because the set of integers accepted by these pragmas does not appear to be documented, so we do not know which diagnostics should be controlled by which pragmas. Using these pragmas in your program will produce an \"unrecognised pragma ignored\" warning, which can itself be disabled with <code>-Wno-unknown-pragmas</code>.</p> <p>SCALE supports clang-style diagnostic pragmas, as documented [here] (https://clang.llvm.org/docs/UsersManual.html#controlling-diagnostics-via-pragmas). This can be combined with preprocessor macros to achieve the desired effect:</p> <pre><code>#if defined(__clang__) // All clang-like compilers, including SCALE.\n#pragma clang diagnostic ignored \"-Wunused-result\"\n#elif defined(__NVCC__) // NVCC, but not clang. AKA: nvidia's one.\n#pragma nv_diag_suppress ...\n#endif\n</code></pre>"},{"location":"manual/dialects/","title":"CUDA Dialects","text":"<p>The CUDA programming language is not formally specified. The \"standard\" is  therefore approximately \"whatever <code>nvcc</code> does\". Although <code>clang</code> supports  compiling CUDA, it supports a somewhat different dialect compared to <code>nvcc</code>.</p> <p>You can read more about (some of) the specific differences in the LLVM manual page about it.</p> <p>This leads to a problem: most CUDA code is written with <code>nvcc</code> in mind, but  the only open-source compiler available with a CUDA frontend is <code>clang</code>. Many  real-world CUDA programs cannot be successfully compiled with <code>clang</code>  because they depend on <code>nvcc</code>'s behaviour.</p> <p>HIP experiences the same problem: the HIP compiler is based on LLVM, so HIP  is closer to \"LLVM-dialect CUDA\" than it is to \"nvcc-dialect CUDA\". This  causes some CUDA programs to fail in \"interesting\" ways when ported to HIP. It's not really the case that you can remap all the API calls to the HIP ones and expect it to work: nvcc-CUDA, and LLVM-CUDA/HIP have quite different C++  semantics.</p> <p>SCALE resolves this issue by offering two compilers:</p> <ul> <li><code>\"nvcc\"</code>: a clang frontend that replicates the behaviour of NVIDIA's <code>nvcc</code>,    allowing existing CUDA programs to compile directly. This is similar to how   LLVM achieves MSVC compatibility by providing <code>clang-cl</code>.</li> <li><code>clang</code>: providing clang's usual clang-dialect-CUDA support, with our    opt-in language extensions.</li> </ul> <p>Existing projects can be compiled without modification using the  <code>nvcc</code>-equivalent compiler. Users of clang-dialect CUDA may use the provided  clang compiler to compile for either platform.</p>"},{"location":"manual/differences/","title":"Differences from NVIDIA CUDA","text":"<p>There are some areas where SCALE's implementation of a certain feature also found in NVIDIA CUDA has different behaviour. This document does not enumerate missing CUDA APIs/features.</p>"},{"location":"manual/differences/#defects","title":"Defects","text":""},{"location":"manual/differences/#nvrtc-differences","title":"NVRTC differences","text":"<p>SCALE's current implementation of the nvrtc API works by calling the compiler as a subprocess instead of a library. This differs from how NVIDIA's implementation works, and means that the library must be able to locate the compiler to invoke it.</p> <p>If your program uses the rtc APIs and fails with errors that relate to being  unable to locate the compiler, ensure that SCALE's <code>nvcc</code> is first in PATH.</p>"},{"location":"manual/differences/#stream-synchronization","title":"Stream synchronization","text":"<p>SCALE does not yet support per-thread default stream behaviour.</p> <p>Instead, the default stream is used in place of the per-thread default stream. This will not break programs, but is likely to reduce performance.</p> <p>A workaround which will also slightly improve the performance of your program when run on NVIDIA GPUs is to use nonblocking CUDA streams explicitly, rather than relying on the implicit CUDA stream.</p>"},{"location":"manual/differences/#host-side-__half-support","title":"Host-side <code>__half</code> support","text":"<p>The CUDA API allows many <code>__half</code> math functions to be used on both host and  device.</p> <p>When compiling non-CUDA translation units, you can include <code>&lt;cuda_fp16.h&gt;</code>  and use the <code>__half</code> math APIs in host code. When you do this, NVIDIA's CUDA  implementation converts the <code>__half</code> to 32-bit <code>float</code>, does the calculation, and converts back.</p> <p>SCALE only allows these functions to be used on the host when the host compiler  supports compiling fp16 code directly (via the <code>_Float16</code> type). Current versions of gcc and clang both support this.</p> <p>This difference only applies to non-CUDA translation units using compilers at least 2 years old.</p> <p>This means:</p> <ul> <li>All <code>__half</code> APIs work in both host and device code in <code>.cu</code> files.</li> <li><code>__half</code> APIs that perform floating point math will not compile in host    code in non-CUDA translation units if an old host compiler is used.</li> <li>The outcome of <code>__half</code> calculations on host/device will always be the same.</li> <li>APIs for using <code>__half</code> as a storage type are always supported.</li> </ul> <p>SCALE bundles a modern host compiler at <code>&lt;SCALE_DIR&gt;/targets/gfxXXX/bin/clang++</code>  you can use as a workaround if this edgecase becomes a problem.</p>"},{"location":"manual/differences/#enhancements","title":"Enhancements","text":""},{"location":"manual/differences/#contexts-where-cuda-apis-are-forbidden","title":"Contexts where CUDA APIs are forbidden","text":"<p>NVIDIA's implementation forbids CUDA APIs in various contexts, such as from host-side functions enqueued onto streams.</p> <p>This implementation allows CUDA API calls in such cases.</p>"},{"location":"manual/differences/#static-initialization-and-deinitialization","title":"Static initialization and deinitialization","text":"<p>This implementation permits the use of CUDA API functions during global static initialization and <code>thread_local</code> static initialization.</p> <p>It is not permitted to use CUDA API functions during static deinitialization.</p> <p>This is more permissive than what is allowed by NVIDIA's implementation.</p>"},{"location":"manual/differences/#device-printf","title":"Device <code>printf</code>","text":"<p>SCALE's device <code>printf</code> accepts an unlimited number of arguments if you compile with at least C++11.</p> <p>If you target an older version of C++ then it is limited to 32, like NVIDIA's implementation.</p>"},{"location":"manual/differences/#contexts","title":"Contexts","text":"<p>If <code>cuCtxDestroy()</code> is used to destroy the context that is current to a different CPU thread, and that CPU thread then issues an API call that depends on the context without first setting a different context to be current, the behaviour is undefined.</p> <p>In NVIDIA's implementation, this condition returns <code>CUDA_ERROR_CONTEXT_IS_DESTROYED</code>.</p> <p>Matching NVIDIA's behaviour would have incurred a small performance penalty on many operations to handle an edgecase that is not permitted.</p>"},{"location":"manual/differences/#kernel-argument-size","title":"Kernel argument size","text":"<p>SCALE accepts kernel arguments up to 2GB, whereas NVIDIA CUDA allows only  32kb (and 4kb before version 12.1).</p> <p>This is more an implementation quirk than a feature, since huge kernel  arguments are unlikely to perform well compared to achieving the same effect  with async copies, memory mapping, etc.</p>"},{"location":"manual/faq/","title":"Frequently asked questions","text":""},{"location":"manual/faq/#how-do-i-report-a-problem","title":"How do I report a problem?","text":"<p>Strange compiler errors? Performance not as great as expected? Something else not working as expected?</p> <p>Contact us</p> <p>Bug reports - no matter how small - accelerate the SCALE project.</p> <p>Let's work together to democratise the GPGPU market!</p>"},{"location":"manual/faq/#what-are-unstable-builds","title":"What are <code>unstable</code> builds?","text":"<p><code>unstable</code> builds give you access to our latest features and performance optimisations. <code>unstable</code> builds give you access to these features sooner than they would become available via our stable release channel.</p> <p>However, <code>unstable</code> builds do not pass through our full quality assurance process: they may contain regressions and other bugs. <code>unstable</code> builds are made available \"as is\", and no detailed changlogs are available for <code>unstable</code> builds.</p>"},{"location":"manual/faq/#when-will-some-gpu-be-supported","title":"When will <code>&lt;some GPU&gt;</code> be supported?","text":"<p>Expanding the set of supported GPUs is an ongoing process. At present we're  being very conservative with the set of GPUs enabled with SCALE to avoid  use on platforms we currently have zero ability to test on.</p> <p>If your GPU is supported by ROCm, it'll probably become available on SCALE a  little sooner than if it is not, since it won't break our \"CUDA-X\" library  delegation mechanism.</p>"},{"location":"manual/faq/#when-will-some-cuda-api-be-supported","title":"When will <code>&lt;some CUDA API&gt;</code> be supported?","text":"<p>We prioritise CUDA APIs based on the number and popularity of third-party  projects requiring the missing API.</p> <p>If you'd like to bring a missing API to our attention, Contact us</p>"},{"location":"manual/faq/#cant-nvidia-just-change-cuda-and-break-scale","title":"Can't NVIDIA just change CUDA and break SCALE?","text":"<p>Of course, we have no control over what NVIDIA does with the CUDA toolkit.</p> <p>Although it is possible for NVIDIA to change/remove APIs in CUDA or PTX,  doing so would break every CUDA program that uses these functions. Those programs would then be broken on both SCALE and NVIDIA's platform.</p> <p>NVIDIA can add new things to CUDA which we don't support. Projects are free to  choose whether or not to use any new features that are added in the future,  and may choose to use feature detection macros to conditionalise dependence  on non-essential new features. Projects face a similar choice when deciding  whether or not to use SCALE's steadily growing set of features that go beyond NVIDIA's CUDA.</p>"},{"location":"manual/faq/#does-scale-depend-on-nvidias-compilerassembleretc","title":"Does SCALE depend on NVIDIA's compiler/assembler/etc.?","text":"<p>No.</p> <p>Although much of this manual talks about \"nvcc\", it is important to  understand the distinction between the two things this can refer to:</p> <ul> <li>The SCALE compiler, which is named \"nvcc\" for compatibility. This is the    name build scripts expect, so if we named it anything else then nothing    would work!</li> <li>NVIDIA's proprietary CUDA compiler, <code>nvcc</code>.</li> </ul> <p>SCALE provides a thing called nvcc, which is in fact absolutely nothing to  do with NVIDIA's <code>nvcc</code>. Our \"nvcc\" is built on top of the open-source  clang/llvm compiler, and has no dependency on NVIDIA's compiler.</p> <p>SCALE does not make use of \"nvvm\", either.</p>"},{"location":"manual/how-to-install/","title":"Install SCALE","text":"<p>Select your operating system and version below to see installation instructions.</p> UbuntuRocky LinuxOther Distros 22.0424.04 <p>First, add ROCM 6.3.1's package repositories, as per AMD's instructions. Then, add our repository and install from it.</p> <pre><code># Replace with your credentials\nexport CUSTOMER_NAME=\"&lt;customer-username&gt;\"\nexport CUSTOMER_PASSWORD=\"&lt;customer-password&gt;\"\n\n# Tell apt to authenticate to the repo\nsudo tee /etc/apt/auth.conf.d/scale.conf &lt;&lt;EOF\nmachine unstable-nonfree-pkgs.scale-lang.com\nlogin $CUSTOMER_NAME\npassword $CUSTOMER_PASSWORD\nEOF\n# Add the scale deb repos.\nwget --http-user=\"$CUSTOMER_NAME\" --http-password=\"$CUSTOMER_PASSWORD\" https://unstable-nonfree-pkgs.scale-lang.com/$CUSTOMER_NAME/deb/dists/jammy/main/binary-all/scale-repos.deb\n\nsudo apt install ./scale-repos.deb\n\n# Install SCALE\nsudo apt update &amp;&amp; sudo apt install scale-unstable\n\n# Add your user to the `video` group:\nsudo usermod -a -G video $(whoami)\n</code></pre> <p>If you did not already have the <code>amdgpu-dkms</code> kernel driver installed prior to installing SCALE, you should now reboot.</p> <p>First, add ROCM 6.3.1's package repositories, as per AMD's instructions. Then, add our repository and install from it.</p> <pre><code># Replace with your credentials\nexport CUSTOMER_NAME=\"&lt;customer-username&gt;\"\nexport CUSTOMER_PASSWORD=\"&lt;customer-password&gt;\"\n\n# Tell apt to authenticate to the repo\nsudo tee /etc/apt/auth.conf.d/scale.conf &lt;&lt;EOF\nmachine unstable-nonfree-pkgs.scale-lang.com\nlogin $CUSTOMER_NAME\npassword $CUSTOMER_PASSWORD\nEOF\n# Add the scale deb repos.\nwget --http-user=\"$CUSTOMER_NAME\" --http-password=\"$CUSTOMER_PASSWORD\" https://unstable-nonfree-pkgs.scale-lang.com/$CUSTOMER_NAME/deb/dists/noble/main/binary-all/scale-repos.deb\n\nsudo apt install ./scale-repos.deb\n\n# Install SCALE\nsudo apt update &amp;&amp; sudo apt install scale-unstable\n\n# Add your user to the `video` group:\nsudo usermod -a -G video $(whoami)\n</code></pre> <p>If you did not already have the <code>amdgpu-dkms</code> kernel driver installed prior to installing SCALE, you should now reboot.</p> 9 <p>First, add ROCM 6.3.1's package repositories, as per AMD's instructions. Then, add our repository and install from it.</p> <pre><code># Replace with your credentials\nexport CUSTOMER_NAME=\"&lt;customer-username&gt;\"\nexport CUSTOMER_PASSWORD=\"&lt;customer-password&gt;\"\n\n# Add the scale rpm repos.\nwget --http-user=\"$CUSTOMER_NAME\" --http-password=\"$CUSTOMER_PASSWORD\" https://unstable-nonfree-pkgs.scale-lang.com/$CUSTOMER_NAME/rpm/el9/main/scale-repos.rpm\nsudo dnf install ./scale-repos.rpm\n\n# Tell dnf to authenticate to the repo\nsudo tee -a /etc/yum.repos.d/scale.repo &lt;&lt;EOF\nusername = $CUSTOMER_NAME\npassword = $CUSTOMER_PASSWORD\nEOF\n\n# Install SCALE\nsudo dnf install scale-unstable\n</code></pre> <p>If you did not already have the <code>amdgpu-dkms</code> kernel driver installed prior to installing SCALE, you should now reboot.</p> <p>Download and extract the SCALE tarball:</p> <pre><code># Replace with your credentials\nexport CUSTOMER_NAME=\"&lt;customer-username&gt;\"\nexport CUSTOMER_PASSWORD=\"&lt;customer-password&gt;\"\n\nwget --http-user=\"$CUSTOMER_NAME\" --http-password=\"$CUSTOMER_PASSWORD\" https://unstable-nonfree-pkgs.scale-lang.com/$CUSTOMER_NAME/tar/scale-unstable-2025.02.19-amd64.tar.xz\n\n# Extract it to the current directory\ntar xf scale-unstable-2025.02.19-amd64.tar.xz\n</code></pre> <p>The tarball is significantly larger than other options since it includes many dependent libraries directly instead of asking the system     package manager to install them.</p>"},{"location":"manual/how-to-use/","title":"Compile CUDA with SCALE","text":"<p>This guide covers the steps required to compile an existing CUDA project for an AMD GPU using SCALE.</p> <p>SCALE makes this as easy as possible by convincingly impersonating the  NVIDIA CUDA Toolkit (from the point of view of your build system).</p> <p>To use SCALE, we must simply cause your build system to use the \"CUDA  installation\" offered by SCALE.</p> <p>Install SCALE, if you haven't already.</p>"},{"location":"manual/how-to-use/#identifying-gpu-target","title":"Identifying GPU Target","text":"<p>If you don't already know which AMD GPU target you need to compile for, you can use the <code>scaleinfo</code> command provided by SCALE to find out:</p> <pre><code>scaleinfo | grep gfx\n</code></pre> <p>Example output:</p> <pre><code>Device 0 (00:23:00.0): AMD Radeon Pro W6800 - gfx1030 (AMD) &lt;amdgcn-amd-amdhsa--gfx1030&gt;\n</code></pre> <p>In this example, the GPU target ID is <code>gfx1030</code>.</p> <p>If your GPU is not listed in the output of this command, it is not currently supported by SCALE.</p> <p>If the <code>scaleinfo</code> command is not found, ensure that <code>&lt;SCALE install path&gt;/bin</code> is in <code>PATH</code>.</p>"},{"location":"manual/how-to-use/#the-easy-way-scaleenv","title":"The easy way: <code>scaleenv</code>","text":"<p>SCALE offers a \"<code>venv</code>-flavoured\" environment management script to allow  \"magically\" building CUDA projects.</p> <p>The concept is simple:</p> <ol> <li>Activate the <code>scaleenv</code> for the AMD GPU target you want to build for.</li> <li>Run the commands you normally use to build the project for an NVIDIA GPU.</li> <li>AMD binaries are sneakily produced instead of NVIDIA ones.</li> </ol> <p>To activate a scaleenv:</p> <pre><code>source /opt/scale/bin/scaleenv gfx1030\n</code></pre> <p>You can exit a <code>scaleenv</code> by typing <code>deactivate</code> or closing your terminal.</p> <p>While the environment is active: simply run the usual <code>cmake</code>/<code>make</code>/etc. commands needed to build the project, and it will build for whatever AMD  target you handed to <code>scaleenv</code>.</p>"},{"location":"manual/how-to-use/#how-it-really-works","title":"How it really works","text":"<p>To allow compilation without build system changes, SCALE provides a series of directories that are recognised by build systems as being CUDA Toolkit installations. One such directory is provided for each supported AMD GPU target. These directories can be found at <code>&lt;SCALE install  path&gt;/targets/gfxXXXX</code>, where <code>gfxXXXX</code> is the name of an AMD GPU target, such as <code>gfx1030</code>.</p> <p>To achieve the desired effect, we need the build system to use the \"CUDA  toolkit\" corresponding to the desired AMD GPU target.</p> <p>For example: to build for <code>gfx1030</code> you would tell your build system that CUDA is installed at <code>&lt;SCALE install path&gt;/targets/gfx1030</code>.</p> <p>All <code>scaleenv</code> is actually doing is setting various environment variables up  to make this happen. It's just a shell script: open it to see the variables  it is manipulating.</p>"},{"location":"manual/how-to-use/#finding-the-libraries-at-runtime","title":"Finding the libraries at runtime","text":"<p>For maximum compatibility with projects that depend on NVIDIA's \"compute  capability\" numbering scheme, SCALE provides one \"cuda mimic directory\" per  supported GPU target that maps the new target to \"sm_86\" in NVIDIA's  numbering scheme.</p> <p>This means that each of the <code>target</code> subdirectories contains  identically-named libraries, so SCALE cannot meaningfully add them to the  system's library search path when it is installed. The built executable/library therefore needs to be told how to find the libraries via another mechanism,  such as:</p> <ul> <li>rpath. With CMake, the simplest    thing that \"usually just works\" is to add    <code>-DCMAKE_INSTALL_RPATH_USE_LINK_PATH=ON</code> to your cmake incantation.</li> <li>Set <code>LD_LIBRARY_PATH</code> to include <code>${SCALE_DIR}/lib</code> at runtime. <code>scaleenv</code>    does this, so if you keep that enabled when running your programs things    will just work.</li> </ul> <p>Support for multiple GPU architectures in a single binary (\"Fat binaries\")  is in development.</p>"},{"location":"manual/how-to-use/#next-steps","title":"Next steps","text":"<ul> <li>Learn about CUDA dialects and SCALE language extensions</li> <li>Report a bug</li> </ul>"},{"location":"manual/inline-ptx/","title":"Inline PTX support","text":"<p>SCALE accepts inline PTX <code>asm</code> blocks in CUDA programs and will attempt to  compile it for AMD along with the rest of your program.</p>"},{"location":"manual/inline-ptx/#wave64-considerations","title":"Wave64 considerations","text":"<p>A small number of PTX instructions depend on the warp size of the GPU being  used. Since all NVIDIA GPUs and many AMD ones have a warp size of 32, much  code implicitly relies on this. As a result, issues can appear when  targeting wave64 devices.</p> <p>SCALE provides several tools and compiler warnings to help you write  portable PTX code. In most cases only small tweaks are required to get things working. Since so little PTX actually depends on the warp size, most  projects are unaffected by the issues documented in this section.  Nevertheless, it is useful to adjust your code to be warp-size-agnostic,  since doing so can be done with no downsides.</p>"},{"location":"manual/inline-ptx/#querying-warp-size","title":"Querying warp size","text":"<p>PTX defines the <code>WARP_SZ</code> global constant which can be used to access the warp size directly. It's a compile-time constant in nvidia's implementation  as well as in SCALE, so there is no cost to using this and doing arithmetic  with it (like with <code>warpSize</code> in CUDA code).</p>"},{"location":"manual/inline-ptx/#lanemask-inputs","title":"Lanemask inputs","text":"<p>The length of lanemask operands on instructions will always have a number of  bits equal to the warp size on the target GPU. For  example, when compiling for a wave64 GPU, the lanemask argument to <code>shfl.sync</code> is a <code>b64</code>, not <code>b32</code>.</p> <p>The following rules are applied to help detect problems with such operands:</p> <ul> <li>If a non-constant lanemask operand is used, and its bit-length is &lt;= the    warp size, an error is raised.</li> <li>If a constant lanemask operand is used with no 1-bits in the high 32 bits,    while compiling for a wave64 architecture, a warning is raised (which can    be disabled). This catches the common case of hardcoded lanemasks like    <code>0xFFFFFFFF</code> which will typecheck as <code>b64</code>, but will probably not do what    you want.</li> </ul> <p>In the common case where you want an all-ones lanemask, the most convenient  thing to do is write <code>-1</code> instead of <code>0xFFFFFFFF</code>: this will give you the  correct number of 1-bits in all cases, including on nvidia platforms.</p>"},{"location":"manual/inline-ptx/#the-c-argument-to-shfl-instructions","title":"The <code>c</code> argument to <code>shfl</code> instructions","text":"<p>The <code>shfl</code> PTX instruction has a funky operand, <code>c</code>, used for clamping etc. See the documentation.</p> <p>The <code>c</code> operand is really two operands packed together: <code>cval</code> in bits 0-4, and <code>segmask</code> in bits 8-12. For wave64, an extra bit is needed. Since there is space for an extra bit in each of these values, we simply add it in the obvious place.</p> <p>A portable way of reasoning about this is to assume that <code>cval</code> is in bits 0-7 and <code>segmask</code> in bits 8-15.</p> <p>Here's a concrete example of a reverse cumsum that works on either warp size:</p> <pre><code>__global__ void shuffleRevCumsumKernel(float *dst)\n{\n    float out;\n    const int C = warpSize - 1;\n    asm(\n    \".reg .f32 Rx;\"\n    \".reg .f32 Ry;\"\n    \".reg .pred p;\"\n    \"mov.b32 Rx, %1;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x1,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x2,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x4,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x8,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x10, %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n\n    // One extra shuffle is needed for the larger warp size.\n    #if __SCALE_WARP_SIZE__ &gt; 32\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x20, %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    #endif // __SCALE_WARP_SIZE__\n    \"mov.b32 %0, Rx;\"\n    : \"=f\"(out) : \"f\"(1.0f), \"n\"(C)\n    );\n\n    dst[threadIdx.x] = out;\n}\n</code></pre> <p>And here's how to do a portable butterfly shuffle reduction:</p> <pre><code>__global__ void shuffleBflyKernel(float *dst)\n{\n    const int C = warpSize - 1;\n\n    float out;\n    asm(\n    \".reg .f32 Rx;\"\n    \".reg .f32 Ry;\"\n    \".reg .pred p;\"\n    \"mov.b32 Rx, %1;\"\n    #if __SCALE_WARP_SIZE__ &gt; 32\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x20, %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    #endif // __SCALE_WARP_SIZE__\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x10, %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x8,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x4,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x2,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x1,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"mov.b32 %0, Rx;\"\n    : \"=f\"(out) : \"f\"((float) threadIdx.x), \"n\"(C)\n    );\n\n    dst[threadIdx.x] = out;\n}\n</code></pre>"},{"location":"manual/inline-ptx/#dialect-differences","title":"Dialect differences","text":"<p>The SCALE compiler accepts a more permissive dialect of PTX than NVIDIA's  implementation does. </p>"},{"location":"manual/inline-ptx/#integer-lengths","title":"Integer lengths","text":"<p>Most PTX instructions are defined to work only for a specific, arbitrary set  of integer types. We didn't bother to implement such restrictions except in  cases where they are needed for correctness, so many PTX instructions accept  a wider selection of types than nvcc accepts.</p> <p>One amusing consequence of this is that most of the simple instructions work  for any bit-length: <code>add.s17</code> is allowed (but will of course lead to  extra sext/trunc instructions, so isn't necessarily a good idea).</p>"},{"location":"manual/inline-ptx/#divergent-exit","title":"Divergent <code>exit</code>","text":"<p>AMD hardware does not seem to have a mechanism by which individual threads  can terminate early (only entire warps). As a result, the <code>exit</code>  instruction may be used only in converged contexts. We transform it into approximately:</p> <pre><code>if (__activemask() == -1) {\n    exit_entire_warp();\n} else {\n    // This situation is unrepresentable\n    __trap();\n}\n</code></pre> <p>Code that uses <code>exit</code> as a performance optimisation for nvidia hardware may  benefit from being adjusted for AMD.</p>"},{"location":"manual/inline-ptx/#empty-asm-volatile-blocks","title":"Empty <code>asm volatile</code> blocks","text":"<p>To cater to \"interesting\" user code, the SCALE compiler will not touch <code>asm volatile</code> blocks containing no instructions. We've seen real-world CUDA code that uses these as a kind of ad-hoc optimisation barrier to prevent the compiler breaking programs that contain undefined behaviour. This pragmatic choice should reduce how often such broken programs fail to function, but such code is broken by definition.</p> <p>Note that the <code>volatile</code> on non-empty <code>volatile asm</code> blocks has no effect on the behaviour of the SCALE compiler. <code>volatile</code> asm is a conservative feature that allows the compiler to model \"unknowable\" implicit dependencies of the actions takenby the inline asm. Since we're compiling the asm to IR, the actual dependencies and properties of everything it does are known and modelled. This can improve optimisation, but may break programs that have undefined behaviour that was being hidden by the optimisation barrier effect of the volatile asm  block.</p>"},{"location":"manual/inline-ptx/#returning-the-carry-bit","title":"Returning the carry-bit","text":"<p>The PTX carry-bit may not be implicitly returned from functions.</p> <p>Some PTX instructions have a carry flag bit, used to perform extended-precision integer arithmetic across multiple instructions. The PTX manual notes:</p> <p>The condition code register is not preserved across calls and is mainly intended for use in straight-line code sequences for computing extended-precision integer addition, subtraction, and multiplication.</p> <p>It is, therefore, undefined behaviour to write code that writes to the  carry-bit in one function and then attempts to read it from another. The  correct execution of such code is optimiser-dependent on NVIDIA's platform  (depending on the function to inline), so will likely fail in <code>-O0</code> builds. </p> <p>For example:</p> <pre><code>void createCarryBit(int x, int y) {\n    // Add with carry-out.\n    asm(\"add.cc.u32 %0, %0, %1\": \"+r\"(x) : \"r\"(y));\n}\n\nvoid useCarryBit(int x, int y) {\n    createCarryBit(x, y);\n\n    // Add with carry-in. Not allowed, since it's trying to read the\n    // carry-bit across a function boundary.\n    int z;\n    asm(\"add.cc.u32 %0, %1, %2\" : \"=r\"(z) : \"r\"(x), \"r\"(y));\n}\n</code></pre> <p>Due to how SCALE's PTX support works, it can't support this pattern, so this situation is a compiler error instead. To write code like this in a portable  way, you can refactor it to use macros instead.</p> <p>It may also be worth considering if you truly still need this inline asm: both NVIDIA nvcc and SCALE have good support for <code>int128_t</code> now, which makes many  common uses of these asm constructs redundant.</p>"},{"location":"manual/inline-ptx/#asm-inputoutput-types","title":"<code>asm</code> input/output types","text":"<p><code>nvcc</code> doesn't appear to consistently follow its own tying rules for PTX asm  inputs/outputs. It allows the following invalid things to occur in some cases  (and real programs depend on this):</p> <ul> <li>Writes to read-only asm bindings are permitted (such as writing to an \"r\")    constraint. The result of the write is not visible to the caller: it's    effectively a temporary inside the scope of the asm block.</li> <li><code>=r</code> (write-only) constraints can be used in a read-write fashion (as if    they were <code>+r</code>).</li> <li>Values passed to the asm block are sometimes, but not always, type checked,   implicitly widened, or implicitly truncated.</li> </ul> <p>To avoid having to characterise and define the perimeter of this buggy  behaviour, SCALE's implementation defines the following consistent rules  which are intended to maximise compatibility (and minimise \"weirdness\"):</p> <ul> <li>All read-only inputs may be written to. The results of these writes are    visible only within the scope of the asm block (as if they were local    variables being passed by value into a function).</li> <li>All write-only outputs are implicitly read-write. ie.: there is no    difference between <code>+r</code> and <code>=r</code>.</li> <li>The type of an input or output binding is governed by the type of the    expression, not the constraint letter. Once \"in PTX\", the usual PTX rules    about implicit truncation/widening/etc. apply. This nuance won't change    the behaviour of programs unless they rely on using a \"too short\" PTX    constraint type to truncate a value, and then implicitly widen it within    PTX (hence zeroing out some of the bits). Since such truncations are    inconsistently applied even with nvidia nvcc mode, they are probably best    achieved with an explicit cast.</li> </ul>"},{"location":"manual/inline-ptx/#performance-considerations","title":"Performance considerations","text":"<p>In most cases, there isn't a performance penalty from using PTX asm in CUDA  code: it will usually convert to the same IR as the C++ you could have  written instead, and may actually be faster due to not needing to be as conservative about optimisation compared to the usual rules of asm blocks.</p> <p>Since the compiler effectively converts it to the CUDA code you could have  written to achieve the same effect without the use of the PTX asm, it  doesn't come with the optimisation-hindering downsides asm blocks  normally imply. The compiler will respect the ordering/synchronisation/etc.  requirements of each operation individually, rather than having to regard an  entire <code>asm volatile</code> block as an opaque, immutable unit.</p> <p>Programs that have already added support for HIP might have multiple  codepaths: one for CUDA that uses inline PTX, and one for AMD which doesn't.  In such cases, it is worth testing both to see which is fastest.</p>"},{"location":"manual/inline-ptx/#supported-constraints","title":"Supported constraints","text":"<p>The following PTX constraint letters are supported. See above commentary on  nuances regarding how they are interpreted.</p> <p><code>h</code>: u16 <code>r</code>: u32 <code>l</code>: u64 <code>f</code>: f32 <code>d</code>: f64 <code>n</code>: constants <code>C</code>: dynamic asm strings</p>"},{"location":"manual/inline-ptx/#supported-instructions","title":"Supported instructions","text":"<p>The following instructions are currently supported.</p> <p>Caveat: since the <code>bf16</code>, <code>fp8</code> and <code>tf32</code> floating point formats are not  currently supported in SCALE, they are also not supported here.</p> Instruction Notes abs activemask add addc and atom bfe bfi bfind bfind.shiftamt bmsk bra brev brkpt Currently a no-op clz cnot copysign cvt cvt.pack discard Currently a no-op div dp2a dp4a elect exit Only from convergent code fence Memory ranges unsupported fma fns griddepcontrol.launch_dependents Currently a no-op griddepcontrol.wait Currently a no-op isspacep ld ld.nc ldmatrix ldu lop3 mad mad24 madc match.all match.any max max.xorsign.abs membar min min.xorsign.abs mma <code>wmma.mma</code> likely faster mov mul mul24 nanosleep neg not or pmevent Currently a no-op popc prefetch prefetchu prmt prmt.b4e prmt.ecl prmt.ecr prmt.f4e prmt.rc16 prmt.rc8 rcp red redux rem sad selp set setp shf.l shfl.bfly shfl.down shfl.idx shfl.up shf.r shl shr slct st stmatrix sub subc szext testp.finite testp.infinite testp.normal testp.notanumber testp.number testp.subnormal trap vabsdiff vadd vmax vmin vote.all vote.any vote.ballot vote.uni vshl vshr vsub wmma.load wmma.store wmma.mma xor"},{"location":"manual/language-extensions/","title":"Language Extensions","text":"<p>SCALE has various opt-in language extensions that aim to improve the  experience of writing GPU code. More language extensions are in development.</p> <p>Unless otherwise noted, SCALE accepts these language extensions in both  <code>clang</code> and <code>nvcc</code> modes. Note that there are dialect differences between the two modes.</p> <p>Since NVIDIA's compiler does not support SCALE language extensions, if you  want to retain the ability to compile for NVIDIA GPUs you must do one of two  things:</p> <ul> <li>Guard use of language extensions behind the <code>__REDSCALE__</code> macro, hiding    it from NVIDIA's <code>nvcc</code>.</li> <li>Use SCALE's <code>clang</code> compiler to compile for both NVIDIA and AMD targets.    This will require changes to your build system.</li> </ul>"},{"location":"manual/language-extensions/#clangloop_unroll","title":"<code>[[clang::loop_unroll]]</code>","text":"<p>GPU code frequently contains loops that need to be partially unrolled, and which have the property that the degree of unrolling is a tradeoff between ILP and register usage.</p> <p>Finding the optimal amount to unroll is not usually possible in the compiler because the number of threads to be used is a runtime value. Programmers therefore usually want to set unroll depth by hand.</p> <p>The existing <code>#pragma unroll N</code> allows this to be set at the preprocessor level. The new <code>[[clang::loop_unroll N]]</code> allows doing this in a template-dependent way:</p> <pre><code>template&lt;int UnrollAmount&gt;\n__device__ void example(int n) {\n    [[clang::loop_unroll UnrollAmount]]\n    for (int i = 0; i &lt; n; i++) {\n        // ...\n    }\n}\n</code></pre>"},{"location":"manual/language-extensions/#__builtin_provablebool-x","title":"<code>__builtin_provable(bool X)</code>","text":"<p><code>__builtin_provable(X)</code> accepts a boolean, <code>X</code>, and:</p> <ul> <li>If the compiler is able to prove, during optimisation, that <code>X</code> is a    compile-time constant true, the entire expression evaluates to a    compile-time constant true.</li> <li>Otherwise (if <code>X</code> is unknown, or provably false), the entire expression    evaluates to a compile-time constant false.</li> </ul> <p>This allows you to write code that opportunistically optimises for a special  case, without the risk of runtime branching overhead or the inconvenience of  propagating this information through your entire program using templates.  For example:</p> <pre><code>__device__ int myCleverFunction(int input) {\n    if (__builtin_provable(input % 2 == 0)) {\n        // Special fast code for the case where `input` is divisible\n        // by 2 goes here.\n    } else {\n        // Slow, general case goes here.\n    }\n}\n</code></pre> <p>During optimisation, as calls to <code>myCleverFunction</code> get inlined, the  compiler may be able to prove that <code>input % 2 == 0</code> for specific calls to  this function. Those cases will be compiled with the \"fast path\", while all  others will be compiled to the \"slow path\". The <code>if</code> statement will never  compile to an actual conditional.</p> <p>Since there are no guarantees that the optimiser is able to prove the  condition, the program must produce identical outputs from either path, or  the behaviour is undefined.</p> <p>This feature differs from the standard c++17 <code>if constexpr</code> in that it is  not required that the input boolean be <code>constexpr</code>. <code>__builtin_provable()</code>  communicates with the optimiser, not the template system. Consequently:</p> <ul> <li>You don't need to use templates to propagate \"optimisation knowledge\"    throughout the program.</li> <li>Compilation may be faster, as a result of not having to template everything.</li> <li>Some cases may be missed where optimisation fails. Such cases are probably    independently worth investigating (Why did optimisation fail? That's a    source of additional slowness).</li> </ul>"},{"location":"manual/language-extensions/#improved-support-for-non-32-warpsize","title":"Improved support for non-32 warpSize","text":"<p>Not all AMD GPUs have a warp size of 32. To mitigate this, we offer a variety of compiler and API features:</p> <ul> <li><code>cudaLaneMask_t</code>: A type that is an integer with the number of bits as a CUDA   warp. This should be used when using functions such as <code>__ballot()</code> to avoid   discarding half the bits.</li> <li>Use of <code>cudaLaneMask_t</code> in appropriate places in the CUDA APIs (such as    the return value of <code>__ballot()</code>)</li> <li>Diagnostics to catch implicit casts from <code>cudaLaneMask_t</code> to narrower types.</li> </ul> <p>In practice, this means the compiler detects the majority of cases where code is written in a way that will break on a device with a warp size of 64.</p> <p>Programmers should modify their CUDA code to be agnostic to warp size.  NVIDIA's documentation recommends this practice, but a lot of real-world CUDA  code does it incorrectly because no current NVIDIA hardware has a warp size  other than 32.</p> <p>Since NVIDIA's <code>nvcc</code> does not have <code>cudaLaneMask_t</code>, programmers should use <code>auto</code> to declare the return types of functions such as <code>__ballot()</code> that return it. This will compile correctly on all platforms.</p>"},{"location":"manual/optimisation-flags/","title":"Compiler Optimisation Flags","text":"<p>When using the <code>nvcc</code> frontend to SCALE, it matches the behaviour of  NVIDIA's compiler as closely as possible.</p> <p>This means disabling some optimisations that are enabled by default by clang, since those break certain programs which rely on the behaviour of NVIDIA's  compiler. In some cases, such reliance represents undefined behaviour in the affected program.</p> <p>This page documents some of these differences, and how to opt-in to these  optimisations on a case-by-case basis. In general, all <code>clang++</code> flags are  also accepted by SCALE in <code>nvcc</code> mode.</p> <p>You may be able to simply switch some of these features on and immediately  gain performance.</p> <p>tl;dr: try <code>-ffast-math -fstrict-aliasing</code> and see if your program explodes.</p>"},{"location":"manual/optimisation-flags/#floating-point-optimisation","title":"Floating Point Optimisation","text":"<p>NVIDIA's compiler provides a <code>-use_fast_math</code> flag that relaxes some  floating point rules to improve optimisation. It's documented to do exactly these things:</p> <ul> <li>Use some less precise math functions (eg. <code>__sinf()</code> instead of <code>sinf()</code>).</li> <li>Enables less precise sqrt/division</li> <li>Flushes denorms to zero.</li> <li>Fuse multiply-adds into FMA operations.</li> </ul> <p>SCALE mirrors this behaviour when you use this flag in our <code>nvcc</code> emulation  mode, aiming to produce the same results as NVIDIA's compiler.</p> <p>SCALE also provides all of <code>clang</code>'s flags, allowing access to its more aggressive floating point optimisations,  such as:</p> <ul> <li>Assume infinities and NaNs never happen</li> <li>Allow algebraic rearrangement of floating point calculations</li> </ul> <p>Full details about these flags are available in the clang user manal.</p> <p>These optimisations can be controlled per-line using the fp control pragmas.</p> <p>This allows you to either:</p> <ul> <li>Specify the compiler flag (to enable an optimisation by default) and then    switch it off for special code regions (ie. opt-out mode).</li> <li>Opt-in to the optimisation in regions of code where you know it to be safe.</li> </ul> <p>These flags will affect the performance of functions  in the SCALE implementation of the CUDA Math API.</p> <p>These flags do not affect the accuracy of the results of the Math API, but  do apply assumptions about the range of possible inputs. For example: if  you enable \"assume no infinities\", all infinity-handling logic will be removed from the Math API functions, making them slightly more efficient. Flags like \"reassociate\" and \"use reciprocal math\" do not affect the behaviour of the math functions.</p> <p>Each call to a math function will be optimised separately, using the set of  fp optimisation flags in effect at that point in that file. You can use  pragmas to mix different optimisation flags at different points within the  same file. It is OK to compile different source files with different fp  optimisation flags and then link them together.</p>"},{"location":"manual/optimisation-flags/#strict-aliasing","title":"Strict aliasing","text":"<p>By default, in C++, the compiler assumes that pointers to unrelated types  (eg <code>float</code> and <code>int</code>) never point to the same place. This can significantly improve optimisation by improving instruction reordering and ILP.</p> <p>Unfortunately, NVIDIA's <code>nvcc</code> does not do strict-aliasing optimisations,  and enabling it breaks some CUDA programs. SCALE-nvcc therefore disables  this by default.</p> <p>You can explicitly enable this class of optimisations in SCALE by adding  <code>-fstrict-aliasing</code>. This may break your program if it contains TBAA violations. We recommend you find and fix such violations, since they are undefined behaviour. This would mean your code is only working correctly because NVIDIA's compiler doesn't currently exploit this type of optimisation: something which may change in the future!</p>"},{"location":"manual/runtime-extensions/","title":"API Extensions","text":"<p>SCALE has some runtime/library features not found in NVIDIA's CUDA Toolkit.</p>"},{"location":"manual/runtime-extensions/#environment-variables","title":"Environment variables","text":"<p>Some extra features can be enabled by environment variables.</p>"},{"location":"manual/runtime-extensions/#scale_exceptions","title":"<code>SCALE_EXCEPTIONS</code>","text":"<p>Errors from the CUDA API can be hard to debug, since they simply return an error code that the host program has to do something with.</p> <p>SCALE provides an environment variable to make any error from the CUDA API produce a observable result.</p> <p>Setting <code>SCALE_EXCEPTIONS=1</code> will cause all CUDA APIs to throw descriptive  exceptions instead of returning C-style error codes.</p> <p>Setting <code>SCALE_EXCEPTIONS=2</code> will print the error messages to stderr, but not throw them. This is helpful for programs that deliberately create CUDA errors as part of their processing.</p> <p>In cases where CUDA APIs are expected to return a value other than  <code>cudaSuccess</code> during normal operation (such as <code>cudaStreamQuery()</code>, an  exception will not be thrown except if an exceptional case arises.</p>"},{"location":"manual/runtime-extensions/#api-extensions_1","title":"API Extensions","text":"<p>Some of SCALE's API extensions require the <code>scale.h</code> header to be included. </p>"},{"location":"manual/runtime-extensions/#programmatic-exception-enablement","title":"Programmatic Exception Enablement","text":"<p>SCALE exceptions (see documentation of <code>SCALE_EXCEPTIONS</code> environment  variable above) may also be enabled/disabled programmatically using:</p> <pre><code>scale::Exception::enable(); // To enable.\nscale::Exception::enable(false); // To disable.\n</code></pre> <p>Even when exceptions are disabled, you can access a <code>scale::Exception</code> object containing the descriptive error message from the most recent failure using <code>scale::Exception::last()</code>:</p> <pre><code>cudaError_t e = cudaSomething();\nif (e != cudaSuccess) {\n    const scale::Exception &amp;ex = scale::Exception::last();\n    std::cerr &lt;&lt; \"CUDA error: \" &lt;&lt; ex.what() &lt;&lt; '\\n';\n}\n</code></pre> <p>The error accessed by this API is the same one you'd get from using the CUDA API <code>cudaGetLastError()</code>, just more descriptive.</p>"},{"location":"manual/troubleshooting/","title":"Troubleshooting","text":"<p>This page provides tips for solving common problems encountered when trying  to compile or run CUDA programs with SCALE.</p>"},{"location":"manual/troubleshooting/#crashes","title":"Crashes","text":"<p>Please report a bug.</p>"},{"location":"manual/troubleshooting/#no-such-function-cublascufftcusolversomethingsomething","title":"\"No such function: cuBlas/cuFFt/cuSolverSomethingSomething()\"","text":"<p>If your project needs a missing \"CUDA-X\" API (cuBLAS, cuFFT, cuSOLVER and friends), this is most likely something you can fix yourself by submitting a patch to the open-source library wrapper project. So long as an equivalent function is available in a ROCm library, the wrapper code is trivial.</p>"},{"location":"manual/troubleshooting/#cuda-api-errors","title":"CUDA API errors","text":"<p>The <code>SCALE_EXCEPTIONS</code> feature can be helpful for getting more information about many failures.</p>"},{"location":"manual/troubleshooting/#wave64-issues","title":"wave64 issues","text":"<p>All current NVIDIA GPUs have a warp size of 32, so many CUDA programs are  written in a way that assumes this is always the case.</p> <p>Some AMD GPUs have a warp size of 64, which can cause problems for CUDA code  written in this way.</p> <p>SCALE offers tools to address this problem:</p> <ul> <li>APIs that operate on warp masks accept and return a new type:    <code>cudaWarpSize_t</code>. This is an integer with as many bits as there are    threads in a warp on the target GPU.</li> <li>Some APIs (such as <code>__ffs()</code>) have extra overloads for <code>cudaWarpSize_t</code>, so   common patterns (such as <code>__ffs(__ballot(...))</code>) just work.</li> <li>The SCALE compiler will emit compiler warnings when values that represent    warp masks are implicitly truncated to 32 bits.</li> </ul> <p>To write code that works correctly on both platforms:</p> <ul> <li>Use <code>auto</code> instead of <code>uint32_t</code> when declaring a variable that is    intended to contain a warp mask. With NVIDIA <code>nvcc</code> this will map to   <code>uint32_t</code>, and with SCALE this will map to <code>cudaWarpSize_t</code>, producing    correct behaviour on both platforms.</li> <li>Avoid hardcoding the constant \"32\" to represent warp size, instead using    the global <code>warpSize</code> available on all platforms.</li> </ul>"},{"location":"manual/troubleshooting/#initialization-errors-or-no-devices-found","title":"Initialization errors or no devices found","text":"<p>The SCALE runtime can fail to initialise if:</p> <ul> <li>The AMD kernel module is out of date.</li> <li><code>/dev/kfd</code> is not writable by the user running the program.</li> <li>There are no supported GPUs attached.</li> </ul> <p>This situation produces error messages such as:</p> <pre><code>$ SCALE_EXCEPTIONS=1 ./myProgram\nterminate called after throwing an instance of 'redscale::SimpleException'\n  what():  cudaDeviceSynchronize: No usable CUDA devices found., CUDA error: \"no device\"\nAborted (core dumped)\n</code></pre> <pre><code>$ /opt/scale/bin/scaleinfo\nError getting device count: initialization error\n</code></pre> <pre><code>$ /opt/scale/bin/hsakmtsysinfo\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  HSAKMT Error 20: Could not open KFD\nAborted (core dumped)\n</code></pre>"},{"location":"manual/troubleshooting/#verify-you-have-a-supported-gpu","title":"Verify you have a supported gpu","text":"<p>Run <code>/opt/scale/bin/hsasysinfo | grep 'Name: gfx</code> to determine the  architecture of your GPU, and determine if it is one of the supported  architectures listed here.</p>"},{"location":"manual/troubleshooting/#ensure-devkfd-is-writable","title":"Ensure <code>/dev/kfd</code> is writable","text":"<p>Ensure your user is in the group that grants access to <code>/dev/kfd</code>. On Ubuntu, this is via membership of the <code>render</code> group: <code>sudo usermod -a -G render USERNAME</code>.</p> <p>You could temporarily make <code>/dev/kfd</code> world-writable via: <code>sudo chmod 666  /dev/kfd</code>.</p>"},{"location":"manual/troubleshooting/#cannot-find-shared-object","title":"Cannot find shared object","text":"<p>The correct library search path for a SCALE binary can be target dependent due to compute capability mapping. This can lead to runtime errors where the SCALE libraries cannot be found, such as:</p> <pre><code>error while loading shared libraries: libredscale.so: cannot open shared object file: No such file or directory\n</code></pre> <p>Two ways to solve this problem are:</p> <ul> <li>Set <code>LD_LIBRARY_PATH</code> to the SCALE target library directory, such as:    <code>LD_LIBRARY_PATH=/opt/scale/targets/gfx1030/lib:$LD_LIBRARY_PATH</code> for <code>gfx1030</code>.</li> <li>Compile your program is compiled with that directory in RPATH:    rpath.</li> </ul>"},{"location":"manual/troubleshooting/#cannot-compile-using-the-nvrtc-api-or-reported-compute-capabilities-are-huge","title":"Cannot compile using the nvrtc API or reported compute capabilities are huge","text":"<p>Both of these problems are caused by using a <code>libredscale.so</code> that is not located in the correct place relative to its support files when running a program. In the case of the nvrtc API, it's because the compiler cannot be found. In the case of reported huge compute capabilities, it's because the compute capability map cannot be found.</p> <p>The solution is to make sure to use the <code>lib</code> subdirectories for one of the targets, rather than the <code>lib</code> directory of the SCALE installation directory. For example, <code>/opt/scale/targets/gfx1030/lib</code> rather than <code>/opt/scale/lib</code>. The <code>gfxany</code> target is suitable for using the nvrtc API, but it does not contain a compute capability map so it will not report small compute capabilities.</p> <p>As with being unable to find the shared object at all, this can be solved either by setting <code>LD_LIBRARY_PATH</code> or by setting the binary's rpath.</p>"},{"location":"manual/troubleshooting/#example-error","title":"Example error:","text":"<pre><code>$ SCALE_EXCEPTIONS=1 ./rtc\nterminate called after throwing an instance of 'redscale::RtcException'\n  what():  nvrtcCompileProgram: Could not find clang-nvcc or nvcc., CUDA error: \"JIT compiler not found\", NVRTC error: \"Compilation\"\nAborted (core dumped)\n</code></pre>"},{"location":"manual/troubleshooting/#nvcc-cannot-find-libdevice-for-sm_52-and-cannot-find-cuda-installation","title":"nvcc: cannot find libdevice for sm_52 and cannot find CUDA installation","text":"<p>If <code>targets/gfxany</code> rather than a specific target like <code>targets/gfx1030</code> is used, then there is no default GPU to target. This leads to an error like the example below. The solution is to either use a target-specific directory like <code>targets/gfx1030</code>, or to specify a specific target such as with <code>-arch gfx1030</code>.</p>"},{"location":"manual/troubleshooting/#example-error_1","title":"Example error","text":"<pre><code>nvcc: error: cannot find libdevice for sm_52; provide path to different CUDA installation via '--cuda-path', or pass '-nocudalib' to build without linking with libdevice\nnvcc: error: cannot find CUDA installation; provide its path via '--cuda-path', or pass '-nocudainc' to build without CUDA includes\n</code></pre>"},{"location":"manual/troubleshooting/#cannot-find-c-standard-library-include","title":"Cannot find C++ standard library include","text":"<p>Some distributions, such as Ubuntu, permit multiple versions of <code>gcc</code> and <code>g++</code> to be installed separately. It is possible to have a version of <code>gcc</code> installed without the corresponding version of <code>g++</code>. This can cause our compiler to be unable to find the C++ standard library headers.</p> <p>The solution is to ensure the corresponding version of <code>g++</code> is installed. For example: if the latest version of <code>gcc</code> you have installed is <code>gcc-12</code>, but you do not have <code>g++-12</code> installed, run: <code>sudo apt-get install g++-12</code>.</p>"},{"location":"manual/troubleshooting/#example-error_2","title":"Example error","text":"<pre><code>  In file included from &lt;built-in&gt;:1:\n\n  In file included from\n  /opt/scale/targets/gfx1100/include/redscale_impl/device.h:6:\n\n  In file included from\n  /opt/scale/targets/gfx1100/include/redscale_impl/common.h:40:\n\n  /opt/scale/targets/gfx1100/include/redscale_impl/../cuda.h:15:10: fatal\n  error: 'cstddef' file not found\n\n  #include &lt;cstddef&gt;\n\n           ^~~~~~~~~\n\n  1 error generated when compiling for gfx1100.\n</code></pre>"},{"location":"manual/troubleshooting/#cmake-error-running-link-command-no-such-file-or-directory","title":"CMake: Error running link command: no such file or directory","text":"<p>CMake tries to detect the linker to use based on the compiler. For SCALE's <code>nvcc</code>, it uses <code>clang++</code> as the linker. If this does not exist in your <code>PATH</code>, the result is an error like the one in the example below.</p> <p>A good solution is to make sure SCALE's <code>nvcc</code> is at the start of your <code>PATH</code>. This will place our <code>clang++</code> on your path too, avoiding the problem.</p> <pre><code># Adjust for the target you want to use.\nexport PATH=/opt/scale/targets/gfx1030/bin:$PATH\n</code></pre>"},{"location":"manual/troubleshooting/#example-error_3","title":"Example error","text":"<pre><code>-- The CUDA compiler identification is NVIDIA 12.5.999\n-- Detecting CUDA compiler ABI info\n-- Detecting CUDA compiler ABI info - failed\n-- Check for working CUDA compiler: /opt/scale/targets/gfx1030/bin/nvcc\n-- Check for working CUDA compiler: /opt/scale/targets/gfx1030/bin/nvcc - broken\nCMake Error at /usr/local/share/cmake-3.29/Modules/CMakeTestCUDACompiler.cmake:59 (message):\n  The CUDA compiler\n\n    \"/opt/scale/targets/gfx1030/bin/nvcc\"\n\n  is not able to compile a simple test program.\n\n  It fails with the following output:\n\n    Change Dir: '/home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV'\n\n    Run Build Command(s): /usr/local/bin/cmake -E env VERBOSE=1 /usr/bin/gmake -f Makefile cmTC_185e7/fast\n    /usr/bin/gmake  -f CMakeFiles/cmTC_185e7.dir/build.make CMakeFiles/cmTC_185e7.dir/build\n    gmake[1]: Entering directory '/home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV'\n    Building CUDA object CMakeFiles/cmTC_185e7.dir/main.cu.o\n    /opt/scale/targets/gfx1030/bin/nvcc -forward-unknown-to-host-compiler   \"--generate-code=arch=compute_86,code=[compute_86,sm_86]\" -MD -MT CMakeFiles/cmTC_185e7.dir/main.cu.o -MF CMakeFiles/cmTC_185e7.dir/main.cu.o.d -x cu -c /home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV/main.cu -o CMakeFiles/cmTC_185e7.dir/main.cu.o\n    Linking CUDA executable cmTC_185e7\n    /usr/local/bin/cmake -E cmake_link_script CMakeFiles/cmTC_185e7.dir/link.txt --verbose=1\n    clang++ @CMakeFiles/cmTC_185e7.dir/objects1.rsp -o cmTC_185e7 @CMakeFiles/cmTC_185e7.dir/linkLibs.rsp -L\"/opt/scale/targets/gfx1030/lib\"\n    Error running link command: no such file or directorygmake[1]: *** [CMakeFiles/cmTC_185e7.dir/build.make:102: cmTC_185e7] Error 2\n    gmake[1]: Leaving directory '/home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV'\n    gmake: *** [Makefile:127: cmTC_185e7/fast] Error 2\n\n\n\n\n\n  CMake will not be able to correctly generate this project.\nCall Stack (most recent call first):\n  CMakeLists.txt:2 (project)\n\n\n-- Configuring incomplete, errors occurred!\n</code></pre>"},{"location":"manual/troubleshooting/#half-precision-intrinsics-not-defined-in-c","title":"Half precision intrinsics not defined in C++","text":"<p>If you're using <code>__half</code> in host code in a non-CUDA translation unit, you  might get an error claiming the function you want does not exist:</p> <pre><code>error: \u2018__half2float\u2019 was not declared in this scope\n</code></pre> <p>This problem can be resolved by using newer C++ compiler.</p> <p>This issue is discussed in more detail in the Differences from NVIDIA CUDA  section.</p>"}]}