{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SCALE by Spectral Compute","text":""},{"location":"#what-is-scale","title":"What is SCALE?","text":"<p>SCALE is a GPGPU programming toolkit that can natively compile CUDA applications for AMD GPUs.</p> <p>SCALE does not require the CUDA program or its build system to be modified.</p> <p>Support for more GPU vendors and CUDA APIs is in development.</p>"},{"location":"#how-do-i-use-scale","title":"How do I use SCALE?","text":"<ol> <li>Install SCALE.</li> <li>Activate SCALE, eg. <code>. /opt/SCALE/scaleenv gfx1100</code></li> <li>Compile your CUDA code with SCALE, following the same steps as for NVIDIA CUDA.</li> </ol>"},{"location":"#what-projects-have-been-tested","title":"What projects have been tested?","text":"<p>We validate SCALE by compiling open-source CUDA projects and running their tests. The list of currently-tested projects and their compatibility status can be found here.</p> <p>Join our Discord to let us know what projects are missing (or support our mission by contributing yourself).</p>"},{"location":"#what-are-examples-of-using-scale","title":"What are examples of using SCALE?","text":"<p>Our SCALE Examples section show very basic examples of using SCALE and are a great place to start your hands-on experience.</p> <p>We welcome contributions from our developer community. Join our Discord to share your SCALE projects.</p>"},{"location":"#which-gpus-are-supported","title":"Which GPUs are supported?","text":"<p>The following GPU targets are supported:</p>"},{"location":"#amd","title":"AMD","text":"<p>Full technical specifications available here.</p>"},{"location":"#enterprise","title":"Enterprise","text":"Name Architecture LLVM target MI350X / MI355X CDNA 4 gfx950 MI300A / MI300X / MI325X CDNA 3 gfx942 MI210 / MI250 / MI250X CDNA 2 gfx90a MI100 CDNA 1 gfx908 MI50 / MI60 GCN 5.1 gfx906 MI25 GCN 5.0 gfx900"},{"location":"#consumer","title":"Consumer","text":"Name Architecture LLVM target Radeon AI PRO R9600D / R9700 RDNA 4 gfx1201 Radeon RX 9070 RDNA 4 gfx1201 Radeon RX 9060 RDNA 4 gfx1200 Ryzen AI Max+ PRO 395 RDNA 3.5 gfx1151 Radeon RX 7600 RDNA 3 gfx1102 Radeon PRO v710 / W7700 RDNA 3 gfx1101 Radeon RX 7700 / 7800 RDNA 3 gfx1101 Radeon PRO W7800 / W7900 RDNA 3 gfx1100 Radeon RX W7900 XT / XTX RDNA 3 gfx1100 Radeon PRO v620 / W6800 RDNA 2 gfx1030 Radeon RX 6800 / 6900 / 6950 RDNA 2 gfx1030 Radeon Pro W5700 RDNA 1 gfx1010 <p>Contact us if you want us to expedite support for a particular AMD GPU architecture.</p>"},{"location":"#what-are-the-components-of-scale","title":"What are the components of SCALE?","text":"<p>SCALE consists of:</p> <ul> <li>An <code>nvcc</code>-compatible compiler capable of compiling nvcc-dialect CUDA for AMD   GPUs, including PTX asm.</li> <li>An implementation of the CUDA runtime, driver and math APIs for AMD GPUs.</li> <li>Wrapper libraries providing the \"CUDA-X\" APIs by delegating to the   corresponding ROCm libraries.   This is how libraries such as <code>cuBLAS</code> and <code>cuSOLVER</code> are handled.</li> </ul>"},{"location":"#what-are-the-differences-between-scale-and-other-solutions","title":"What are the differences between SCALE and other solutions?","text":"<p>Instead of providing a new way to write GPGPU software, SCALE allows programs written using the widely-popular CUDA language to be directly compiled for AMD GPUs.</p> <p>SCALE aims to be fully compatible with NVIDIA CUDA. We believe that users should not have to maintain multiple codebases or compromise on performance to support multiple GPU vendors.</p> <p>SCALE's language is a superset of NVIDIA CUDA, offering some opt-in language extensions that can make writing GPU code easier and more efficient for users who wish to move away from <code>nvcc</code>.</p> <p>SCALE is a work in progress. If there is a missing API that is blocking your attempt to use SCALE, please contact us so we can prioritise its development.</p>"},{"location":"#contact-us","title":"Contact us","text":"<p>There are multiple ways to get in touch with us:</p> <ul> <li>Join our Discord</li> <li>Send us an e-mail at hello@spectralcompute.co.uk</li> </ul>"},{"location":"licensing/","title":"SCALE Free License","text":"<p>SCALE by Spectral Compute: Free Software License \u00a9 2025 Spectral Compute Ltd</p> <ol> <li> <p>Introduction 1.1 This software licence (\u201cLicense\u201d) sets out the terms and conditions which form a legally binding agreement between you and Spectral Compute Ltd, a company incorporated in England and Wales with company number 11448807 (\u201cSpectral\u201d) that apply to your use of the Software. 1.2 Words which are defined in clause 2 below, will have the meaning given to them in clause 2 below. 1.3 By downloading, installing and using the Software (including any new releases of the Software or any complete or partial copies of the Software), you agree to all of the terms  and conditions below. 1.4 This License governs your use of the Software and the Documentation.</p> </li> <li> <p>Definitions In this License the following terms have the following meanings:</p> </li> <li>\u201cAdditional Material\u201d has the meaning given in clause 5 below.</li> <li>\u201cDocumentation\u201d means  any documentation, examples,  provided with the Software [including the materials at https://docs.scale-lang.com/stable/].</li> <li>\u201cInputs\u201d means any source code input by you into the Software during your use of the same.</li> <li>\u201cIntellectual Property Rights\u201d means (a) patents, inventions, designs, copyright and related rights, database rights, knowhow, trade marks (whether registered or unregistered) and related goodwill, trade names (whether registered or unregistered), and rights to apply for registration; (b) all other rights of a similar nature or having an equivalent effect anywhere in the world which currently exist or are recognised in the future; and (c) all applications, extensions and renewals in relation to any such rights.</li> <li>\u201cLicense\u201d is defined in clause 1 above.</li> <li>\u201cOutputs\u201d means any compiled code produced by you through your use of the Software.</li> <li>\u201cSoftware\u201d is SCALE by Spectral Compute together with any other software which Spectral has made available under these terms, including any portion of it, but excludes any Additional Software.</li> <li> <p>\u201cyou\u201d or \u201cyour\u201d refers to the individual or entity agreeing to these terms.</p> </li> <li> <p>Licenses 3.1 Spectral grants to you a non-exclusive, royalty-free, worldwide, non-sublicensable, revocable license to use, copy, distribute, and make available, the Software, in each case subject to the limitations below. 3.2 You grant to Spectral a royalty-free, worldwide, transferable, sub-licensable, irrevocable, perpetual license to use or incorporate into the Software and Documentation any enhancement requests, bug reports, or other feedback provided by you. For any feedback that you voluntarily provide, you hereby grant Spectral a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicence and distribute it. Spectral will not be obliged to implement any such enhancement request or feedback.</p> </li> <li> <p>Limitations 4.1 You may use the Software under this License only for non-commercial purposes. You may distribute the Software or any Outputs or provide it to others only if you do so free of charge for non-commercial purposes. For the purposes of this License, \u201cnon-commercial purposes\u201d means use of the Software where you do not receive any monetary payment or benefit from a third party either (a) in connection with the use of the Software; or (b) in connection with any Outputs generated by the Software. For the avoidance of doubt, \"non-commercial purposes\" does not include any use where you:</p> </li> <li>charge any fee for access to or use of the Software, or any services which you provide which involve your use of the Software;</li> <li>incorporate the Software into any paid-for product or service that you offer;</li> <li>use the Software to create Outputs that are sold, licensed, or otherwise provided in exchange for payment; or</li> <li>use the Software or Outputs in the course of any business or undertaking carried on for profit or monetary gain. 4.2 You may not:</li> <li>alter, remove, or obscure any licensing, patent, copyright, or other notices of Spectral in the Software or the Documentation;</li> <li>reverse engineer, modify, de-compile, attempt to derive source code (or underlying ideas, algorithms, structure or organisation) of or to the Software, as applicable; or</li> <li> <p>bypass or breach any security device or protection used for or contained in the Software.</p> </li> <li> <p>Intellectual Property Rights 5.1 Spectral retains ownership of all rights, title and interest, including Intellectual Property Rights, in and to the Software (and any modifications) and the Documentation. 5.2 The Software may be provided with or enable access to software, tools, libraries, data or other material which is either (a) provided by a third party; or (b) provided by Spectral on terms other than this License (\u201cAdditional Material\u201d), and such Additional Material is provided outside of this Licence and subject to the terms imposed by Spectral or the relevant third party. You agree that you will comply with any such terms, which will be included together with the Software, normally in the NOTICES File provided with it and/or in the Documentation. 5.3 You agree that, except as set out in this License, Spectral does not grant you any rights to, or in, the Software and/or the Documentation. 5.4 You retain ownership of all rights, title and interest, including Intellectual Property Rights, in and to your Inputs and (subject to clause 5.1 above) Outputs, including any source code developed by you independent of this License.</p> </li> <li> <p>Notices and Attribution 6.1 You must ensure that any person who receives a copy of the Software (or any part of it) from you also gets a copy of and complies with the terms of this License and the above copyright notice. 6.2 In relation to any Outputs, you must: only provide such Outputs for non-commercial use (as defined in clause 4 above).</p> </li> <li> <p>No Other Rights These terms do not imply any terms, conditions, warranties, licenses or provisions other than those expressly included in these terms.</p> </li> <li> <p>Disclaimers 8.1 To the extent the law allows, Spectral does not represent, warrant or make any commitment that: (i) the Software will meet your requirements; (ii) the Software will operate in combination with other hardware or software; or (iii) use of the Software will be uninterrupted or error free. 8.2 The Software is provided \u201cas is\u201d and the provisions of this License are in place of all warranties, conditions, terms, undertakings and obligations implied by statute, common law, custom, trade usage, course of dealing or otherwise, (including but not limited to implied undertakings of satisfactory quality, conformity with description and reasonable fitness for purpose) all of which are excluded to the maximum extent permitted by law. 8.3 Open-source software components may be distributed, embedded, or bundled with the Software. Such open-source software is separately licensed by its copyright holder. Use of the open-source software must be in accordance with its applicable license terms. Spectral makes no representation, warranty or other commitment of any kind regarding such open-source software. Spectral offers no support for such open-source software and shall, as far as the law allows, have no liability associated with its use.</p> </li> <li> <p>Liability 9.1 Nothing in this License shall exclude or limit the liability of either party to the other for:</p> </li> <li>death or personal injury arising out of negligence;</li> <li>fraud or fraudulent misrepresentation; or</li> <li> <p>any other liability that cannot be excluded or limited by law. 9.2 The Software and Documentation is provided by Spectral \u2018as is\u2019 and any express or implied warranties, including but not limited to, the implied warranties of merchantability and fitness for a particular purpose are disclaimed. As far as the law allows, in no event shall Spectral be liable for any direct, indirect, incidental, special, exemplary or consequential damages (including but not limited to, procurement of substitute goods or services; loss of use, data, or profits; or business interruption) however caused and on any theory of liability, whether in contract, strict liability, or tort (including negligence or otherwise) or misrepresentation (whether innocent or negligent), arising in any way out of the use of the Software, even if advised of the possibility of such damage.\u00a7</p> </li> <li> <p>Termination 10.1 If you use the Software or Documentation in breach of these terms, Spectral may terminate this License immediately on notice to you. 10.2 On termination of this License, you must immediately discontinue all use of the Software and Documentation and return to Spectral, or at Spectral\u2019s option destroy, all copies of same. 10.3 Termination of this License is without prejudice to any rights Spectral has against you, including but not limited to injunctive relief.</p> </li> <li> <p>Miscellaneous 11.1 Spectral may make changes to this License from time to time. Please check these terms regularly to ensure that you understand the terms that apply at the time that you access and use the Software. 11.2 Each of the clauses of this License operates separately. If any court or relevant authority decides that any of them are unlawful or unenforceable, the remaining clauses will remain in full force and effect. 11.3 If Spectral fails to enforce any rights against you, or delays in doing so, that will not mean that such rights have been waived and will not mean that you do not have to comply with your obligations under this License.</p> </li> <li> <p>Governing law and jurisdiction 12.1 This License is governed by the laws of England and Wales.  This means that your access to and use of the Software, and any dispute or claim arising out of or in connection therewith (including non-contractual disputes or claims), will be governed by English law. 12.2 The courts of England and Wales shall have exclusive jurisdiction to settle any dispute or claim (including non-contractual disputes or claims) arising out of or in connection with this License or its subject matter or formation.</p> </li> <li> <p>Survival. The termination of this License for any reason whatsoever shall not affect any survival or operation of the following clauses: 4 (Limitations), 5 (Intellectual Property Rights), 8 (Disclaimers), 9 (Liability), 11 (Miscellaneous) and 12 (Governing Law and Jurisdiction).</p> </li> <li> <p>If you have any questions regarding this license, please contact us at legal@spectralcompute.co.uk.</p> </li> </ol>"},{"location":"notices/","title":"Third party software","text":"<p>SCALE uses a number of third-party software tools, libraries and content.</p> <p>The file (gratefully) attributes the authors of those works, the licences under which they  are available, and indicates the terms of each license.</p> <p>============================================================================== Thrust is under the Apache Licence v2.0, with some specific exceptions listed below LLVM and libcu++ is under the Apache License v2.0 with LLVM Exceptions: ==============================================================================                                 Apache License                            Version 2.0, January 2004                         http://www.apache.org/licenses/</p> <pre><code>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n  \"License\" shall mean the terms and conditions for use, reproduction,\n  and distribution as defined by Sections 1 through 9 of this document.\n\n  \"Licensor\" shall mean the copyright owner or entity authorized by\n  the copyright owner that is granting the License.\n\n  \"Legal Entity\" shall mean the union of the acting entity and all\n  other entities that control, are controlled by, or are under common\n  control with that entity. For the purposes of this definition,\n  \"control\" means (i) the power, direct or indirect, to cause the\n  direction or management of such entity, whether by contract or\n  otherwise, or (ii) ownership of fifty percent (50%) or more of the\n  outstanding shares, or (iii) beneficial ownership of such entity.\n\n  \"You\" (or \"Your\") shall mean an individual or Legal Entity\n  exercising permissions granted by this License.\n\n  \"Source\" form shall mean the preferred form for making modifications,\n  including but not limited to software source code, documentation\n  source, and configuration files.\n\n  \"Object\" form shall mean any form resulting from mechanical\n  transformation or translation of a Source form, including but\n  not limited to compiled object code, generated documentation,\n  and conversions to other media types.\n\n  \"Work\" shall mean the work of authorship, whether in Source or\n  Object form, made available under the License, as indicated by a\n  copyright notice that is included in or attached to the work\n  (an example is provided in the Appendix below).\n\n  \"Derivative Works\" shall mean any work, whether in Source or Object\n  form, that is based on (or derived from) the Work and for which the\n  editorial revisions, annotations, elaborations, or other modifications\n  represent, as a whole, an original work of authorship. For the purposes\n  of this License, Derivative Works shall not include works that remain\n  separable from, or merely link (or bind by name) to the interfaces of,\n  the Work and Derivative Works thereof.\n\n  \"Contribution\" shall mean any work of authorship, including\n  the original version of the Work and any modifications or additions\n  to that Work or Derivative Works thereof, that is intentionally\n  submitted to Licensor for inclusion in the Work by the copyright owner\n  or by an individual or Legal Entity authorized to submit on behalf of\n  the copyright owner. For the purposes of this definition, \"submitted\"\n  means any form of electronic, verbal, or written communication sent\n  to the Licensor or its representatives, including but not limited to\n  communication on electronic mailing lists, source code control systems,\n  and issue tracking systems that are managed by, or on behalf of, the\n  Licensor for the purpose of discussing and improving the Work, but\n  excluding communication that is conspicuously marked or otherwise\n  designated in writing by the copyright owner as \"Not a Contribution.\"\n\n  \"Contributor\" shall mean Licensor and any individual or Legal Entity\n  on behalf of whom a Contribution has been received by Licensor and\n  subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of\n  this License, each Contributor hereby grants to You a perpetual,\n  worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n  copyright license to reproduce, prepare Derivative Works of,\n  publicly display, publicly perform, sublicense, and distribute the\n  Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of\n  this License, each Contributor hereby grants to You a perpetual,\n  worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n  (except as stated in this section) patent license to make, have made,\n  use, offer to sell, sell, import, and otherwise transfer the Work,\n  where such license applies only to those patent claims licensable\n  by such Contributor that are necessarily infringed by their\n  Contribution(s) alone or by combination of their Contribution(s)\n  with the Work to which such Contribution(s) was submitted. If You\n  institute patent litigation against any entity (including a\n  cross-claim or counterclaim in a lawsuit) alleging that the Work\n  or a Contribution incorporated within the Work constitutes direct\n  or contributory patent infringement, then any patent licenses\n  granted to You under this License for that Work shall terminate\n  as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the\n  Work or Derivative Works thereof in any medium, with or without\n  modifications, and in Source or Object form, provided that You\n  meet the following conditions:\n\n  (a) You must give any other recipients of the Work or\n      Derivative Works a copy of this License; and\n\n  (b) You must cause any modified files to carry prominent notices\n      stating that You changed the files; and\n\n  (c) You must retain, in the Source form of any Derivative Works\n      that You distribute, all copyright, patent, trademark, and\n      attribution notices from the Source form of the Work,\n      excluding those notices that do not pertain to any part of\n      the Derivative Works; and\n\n  (d) If the Work includes a \"NOTICE\" text file as part of its\n      distribution, then any Derivative Works that You distribute must\n      include a readable copy of the attribution notices contained\n      within such NOTICE file, excluding those notices that do not\n      pertain to any part of the Derivative Works, in at least one\n      of the following places: within a NOTICE text file distributed\n      as part of the Derivative Works; within the Source form or\n      documentation, if provided along with the Derivative Works; or,\n      within a display generated by the Derivative Works, if and\n      wherever such third-party notices normally appear. The contents\n      of the NOTICE file are for informational purposes only and\n      do not modify the License. You may add Your own attribution\n      notices within Derivative Works that You distribute, alongside\n      or as an addendum to the NOTICE text from the Work, provided\n      that such additional attribution notices cannot be construed\n      as modifying the License.\n\n  You may add Your own copyright statement to Your modifications and\n  may provide additional or different license terms and conditions\n  for use, reproduction, or distribution of Your modifications, or\n  for any such Derivative Works as a whole, provided Your use,\n  reproduction, and distribution of the Work otherwise complies with\n  the conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise,\n  any Contribution intentionally submitted for inclusion in the Work\n  by You to the Licensor shall be under the terms and conditions of\n  this License, without any additional terms or conditions.\n  Notwithstanding the above, nothing herein shall supersede or modify\n  the terms of any separate license agreement you may have executed\n  with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade\n  names, trademarks, service marks, or product names of the Licensor,\n  except as required for reasonable and customary use in describing the\n  origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or\n  agreed to in writing, Licensor provides the Work (and each\n  Contributor provides its Contributions) on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n  implied, including, without limitation, any warranties or conditions\n  of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n  PARTICULAR PURPOSE. You are solely responsible for determining the\n  appropriateness of using or redistributing the Work and assume any\n  risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory,\n  whether in tort (including negligence), contract, or otherwise,\n  unless required by applicable law (such as deliberate and grossly\n  negligent acts) or agreed to in writing, shall any Contributor be\n  liable to You for damages, including any direct, indirect, special,\n  incidental, or consequential damages of any character arising as a\n  result of this License or out of the use or inability to use the\n  Work (including but not limited to damages for loss of goodwill,\n  work stoppage, computer failure or malfunction, or any and all\n  other commercial damages or losses), even if such Contributor\n  has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing\n  the Work or Derivative Works thereof, You may choose to offer,\n  and charge a fee for, acceptance of support, warranty, indemnity,\n  or other liability obligations and/or rights consistent with this\n  License. However, in accepting such obligations, You may act only\n  on Your own behalf and on Your sole responsibility, not on behalf\n  of any other Contributor, and only if You agree to indemnify,\n  defend, and hold each Contributor harmless for any liability\n  incurred by, or claims asserted against, such Contributor by reason\n  of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\nAPPENDIX: How to apply the Apache License to your work.\n\n  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n\nCopyright [yyyy] [name of copyright owner]\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n</code></pre> <p>============================================================================== Legacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy): ============================================================================== ---- LLVM Exceptions to the Apache 2.0 License ----</p> <p>As an exception, if, as a result of your compiling your source code, portions of this Software are embedded into an Object form of such source code, you may redistribute such embedded portions in such Object form without complying with the conditions of Sections 4(a), 4(b) and 4(d) of the License.</p> <p>In addition, if you combine or link compiled forms of this Software with software that is licensed under the GPLv2 (\"Combined Software\") and if a court of competent jurisdiction determines that the patent provision (Section 3), the indemnity provision (Section 9) or other Section of the License conflicts with the conditions of the GPLv2, you may retroactively and prospectively choose to deem waived or otherwise exclude such Section(s) of the License, but only in their entirety and only with respect to the Combined Software.</p> <p>============================================================================== Software from third parties included in the LLVM Project: ============================================================================== The LLVM Project contains third party software which is under different license terms. All such code will be identified clearly using at least one of two mechanisms: 1) It will be in a separate directory tree with its own <code>LICENSE.txt</code> or    <code>LICENSE</code> file at the top containing the specific license and restrictions    which apply to that software, or 2) It will contain specific license and restriction terms at the top of every    file.</p> <p>============================================================================== Legacy LLVM License (https://llvm.org/docs/DeveloperPolicy.html#legacy): ==============================================================================</p> <p>The libc++ library is dual licensed under both the University of Illinois \"BSD-Like\" license and the MIT license.  As a user of this code you may choose to use it under either license.  As a contributor, you agree to allow your code to be used under both.</p> <p>Full text of the relevant licenses is included below.</p> <p>==============================================================================</p> <p>University of Illinois/NCSA Open Source License</p> <p>Copyright (c) 2009-2019 by the contributors listed in CREDITS.TXT</p> <p>All rights reserved.</p> <p>Developed by:</p> <pre><code>LLVM Team\n\nUniversity of Illinois at Urbana-Champaign\n\nhttp://llvm.org\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <pre><code>* Redistributions of source code must retain the above copyright notice,\n  this list of conditions and the following disclaimers.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimers in the\n  documentation and/or other materials provided with the distribution.\n\n* Neither the names of the LLVM Team, University of Illinois at\n  Urbana-Champaign, nor the names of its contributors may be used to\n  endorse or promote products derived from this Software without specific\n  prior written permission.\n</code></pre> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.</p> <p>==============================================================================</p> <p>Copyright (c) 2009-2014 by the contributors listed in CREDITS.TXT</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>============================================================================== Some libcudacxx components not shipped with this distribution are covered by the below license. Each source file indicates which license it is under.</p> <p>If you find a file in our distribution with the following license, please let us know at legal@spectralcompute.co.uk immediately. ==============================================================================</p> <p>NVIDIA SOFTWARE LICENSE</p> <p>This license is a legal agreement between you and NVIDIA Corporation (\"NVIDIA\") and governs your use of the NVIDIA/CUDA C++ Library software and materials provided hereunder (\u201cSOFTWARE\u201d).</p> <p>This license can be accepted only by an adult of legal age of majority in the country in which the SOFTWARE is used. If you are under the legal age of majority, you must ask your parent or legal guardian to consent to this license. By taking delivery of the SOFTWARE, you affirm that you have reached the legal age of majority, you accept the terms of this license, and you take legal and financial responsibility for the actions of your permitted users.</p> <p>You agree to use the SOFTWARE only for purposes that are permitted by (a) this license, and (b) any applicable law, regulation or generally accepted practices or guidelines in the relevant jurisdictions.</p> <ol> <li> <p>LICENSE. Subject to the terms of this license, NVIDIA grants you a non-exclusive limited license to: (a) install and use the SOFTWARE, and (b) distribute the SOFTWARE subject to the distribution requirements described in this license. NVIDIA reserves all rights, title and interest in and to the SOFTWARE not expressly granted to you under this license.</p> </li> <li> <p>DISTRIBUTION REQUIREMENTS. These are the distribution requirements for you to exercise the distribution grant: a.  The terms under which you distribute the SOFTWARE must be consistent with the terms of this license, including (without limitation) terms relating to the license grant and license restrictions and protection of NVIDIA\u2019s intellectual property rights. b.  You agree to notify NVIDIA in writing of any known or suspected distribution or use of the SOFTWARE not in compliance with the requirements of this license, and to enforce the terms of your agreements with respect to distributed SOFTWARE.</p> </li> <li> <p>LIMITATIONS. Your license to use the SOFTWARE is restricted as follows: a.  The SOFTWARE is licensed for you to develop applications only for use in systems with NVIDIA GPUs. b.  You may not reverse engineer, decompile or disassemble, or remove copyright or other proprietary notices from any portion of the SOFTWARE or copies of the SOFTWARE. c.  You may not modify or create derivative works of any portion of the SOFTWARE. d.  You may not bypass, disable, or circumvent any technical measure, encryption, security, digital rights management or authentication mechanism in the SOFTWARE. e.  You may not use the SOFTWARE in any manner that would cause it to become subject to an open source software license. As examples, licenses that require as a condition of use, modification, and/or distribution that the SOFTWARE be (i) disclosed or distributed in source code form; (ii) licensed for the purpose of making derivative works; or (iii) redistributable at no charge. f.  Unless you have an agreement with NVIDIA for this purpose, you may not use the SOFTWARE with any system or application where the use or failure of the system or application can reasonably be expected to threaten or result in personal injury, death, or catastrophic loss. Examples include use in avionics, navigation, military, medical, life support or other life critical applications. NVIDIA does not design, test or manufacture the SOFTWARE for these critical uses and NVIDIA shall not be liable to you or any third party, in whole or in part, for any claims or damages arising from such uses. g.  You agree to defend, indemnify and hold harmless NVIDIA and its affiliates, and their respective employees, contractors, agents, officers and directors, from and against any and all claims, damages, obligations, losses, liabilities, costs or debt, fines, restitutions and expenses (including but not limited to attorney\u2019s fees and costs incident to establishing the right of indemnification) arising out of or related to use of the SOFTWARE outside of the scope of this Agreement, or not in compliance with its terms.</p> </li> <li> <p>PRE-RELEASE. SOFTWARE versions identified as alpha, beta, preview, early access or otherwise as pre-release may not be fully functional, may contain errors or design flaws, and may have reduced or different security, privacy, availability, and reliability standards relative to commercial versions of NVIDIA software and materials. You may use a pre-release SOFTWARE version at your own risk, understanding that these versions are not intended for use in production or business-critical systems.</p> </li> <li> <p>OWNERSHIP. The SOFTWARE and the related intellectual property rights therein are and will remain the sole and exclusive property of NVIDIA or its licensors. The SOFTWARE is copyrighted and protected by the laws of the United States and other countries, and international treaty provisions. NVIDIA may make changes to the SOFTWARE, at any time without notice, but is not obligated to support or update the SOFTWARE.</p> </li> <li> <p>COMPONENTS UNDER OTHER LICENSES. The SOFTWARE may include NVIDIA or third-party components with separate legal notices or terms as may be described in proprietary notices accompanying the SOFTWARE. If and to the extent there is a conflict between the terms in this license and the license terms associated with a component, the license terms associated with the components control only to the extent necessary to resolve the conflict.</p> </li> <li> <p>FEEDBACK. You may, but don\u2019t have to, provide to NVIDIA any Feedback. \u201cFeedback\u201d means any suggestions, bug fixes, enhancements, modifications, feature requests or other feedback regarding the SOFTWARE. For any Feedback that you voluntarily provide, you hereby grant NVIDIA and its affiliates a perpetual, non-exclusive, worldwide, irrevocable license to use, reproduce, modify, license, sublicense (through multiple tiers of sublicensees), and distribute (through multiple tiers of distributors) the Feedback without the payment of any royalties or fees to you. NVIDIA will use Feedback at its choice.</p> </li> <li> <p>NO WARRANTIES. THE SOFTWARE IS PROVIDED \"AS IS\" WITHOUT ANY EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF MERCHANTABILITY, NONINFRINGEMENT, OR FITNESS FOR A PARTICULAR PURPOSE. NVIDIA DOES NOT WARRANT THAT THE SOFTWARE WILL MEET YOUR REQUIREMENTS OR THAT THE OPERATION THEREOF WILL BE UNINTERRUPTED OR ERROR-FREE, OR THAT ALL ERRORS WILL BE CORRECTED.</p> </li> <li> <p>LIMITATIONS OF LIABILITY. TO THE MAXIMUM EXTENT PERMITTED BY LAW, NVIDIA AND ITS AFFILIATES SHALL NOT BE LIABLE FOR ANY SPECIAL, INCIDENTAL, PUNITIVE OR CONSEQUENTIAL DAMAGES, OR ANY LOST PROFITS, PROJECT DELAYS, LOSS OF USE, LOSS OF DATA OR LOSS OF GOODWILL, OR THE COSTS OF PROCURING SUBSTITUTE PRODUCTS, ARISING OUT OF OR IN CONNECTION WITH THIS LICENSE OR THE USE OR PERFORMANCE OF THE SOFTWARE, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON BREACH OF CONTRACT, BREACH OF WARRANTY, TORT (INCLUDING NEGLIGENCE), PRODUCT LIABILITY OR ANY OTHER CAUSE OF ACTION OR THEORY OF LIABILITY, EVEN IF NVIDIA HAS PREVIOUSLY BEEN ADVISED OF, OR COULD REASONABLY HAVE FORESEEN, THE POSSIBILITY OF SUCH DAMAGES. IN NO EVENT WILL NVIDIA\u2019S AND ITS AFFILIATES TOTAL CUMULATIVE LIABILITY UNDER OR ARISING OUT OF THIS LICENSE EXCEED US$10.00. THE NATURE OF THE LIABILITY OR THE NUMBER OF CLAIMS OR SUITS SHALL NOT ENLARGE OR EXTEND THIS LIMIT.</p> </li> <li> <p>TERMINATION. Your rights under this license will terminate automatically without notice from NVIDIA if you fail to comply with any term and condition of this license or if you commence or participate in any legal proceeding against NVIDIA with respect to the SOFTWARE. NVIDIA may terminate this license with advance written notice to you if NVIDIA decides to no longer provide the SOFTWARE in a country or, in NVIDIA\u2019s sole discretion, the continued use of it is no longer commercially viable. Upon any termination of this license, you agree to promptly discontinue use of the SOFTWARE and destroy all copies in your possession or control. Your prior distributions in accordance with this license are not affected by the termination of this license. All provisions of this license will survive termination, except for the license granted to you.</p> </li> <li> <p>APPLICABLE LAW. This license will be governed in all respects by the laws of the United States and of the State of Delaware as those laws are applied to contracts entered into and performed entirely within Delaware by Delaware residents, without regard to the conflicts of laws principles. The United Nations Convention on Contracts for the International Sale of Goods is specifically disclaimed. You agree to all terms of this Agreement in the English language. The state or federal courts residing in Santa Clara County, California shall have exclusive jurisdiction over any dispute or claim arising out of this license. Notwithstanding this, you agree that NVIDIA shall still be allowed to apply for injunctive remedies or an equivalent type of urgent legal relief in any jurisdiction.</p> </li> <li> <p>NO ASSIGNMENT. This license and your rights and obligations thereunder may not be assigned by you by any means or operation of law without NVIDIA\u2019s permission. Any attempted assignment not approved by NVIDIA in writing shall be void and of no effect.</p> </li> <li> <p>EXPORT. The SOFTWARE is subject to United States export laws and regulations. You agree that you will not ship, transfer or export the SOFTWARE into any country, or use the SOFTWARE in any manner, prohibited by the United States Bureau of Industry and Security or economic sanctions regulations administered by the U.S. Department of Treasury\u2019s Office of Foreign Assets Control (OFAC), or any applicable export laws, restrictions or regulations. These laws include restrictions on destinations, end users and end use. By accepting this license, you confirm that you are not a resident or citizen of any country currently embargoed by the U.S. and that you are not otherwise prohibited from receiving the SOFTWARE.</p> </li> <li> <p>GOVERNMENT USE. The SOFTWARE has been developed entirely at private expense and is \u201ccommercial items\u201d consisting of \u201ccommercial computer software\u201d and \u201ccommercial computer software documentation\u201d provided with RESTRICTED RIGHTS. Use, duplication or disclosure by the U.S. Government or a U.S. Government subcontractor is subject to the restrictions in this license pursuant to DFARS 227.7202-3(a) or as set forth in subparagraphs (b)(1) and (2) of the Commercial Computer Software - Restricted Rights clause at FAR 52.227-19, as applicable. Contractor/manufacturer is NVIDIA, 2788 San Tomas Expressway, Santa Clara, CA 95051.</p> </li> <li> <p>ENTIRE AGREEMENT. This license is the final, complete and exclusive agreement between the parties relating to the subject matter of this license and supersedes all prior or contemporaneous understandings and agreements relating to this subject matter, whether oral or written. If any court of competent jurisdiction determines that any provision of this license is illegal, invalid or unenforceable, the remaining provisions will remain in full force and effect. This license may only be modified in a writing signed by an authorized representative of each party.</p> </li> </ol> <p>(v. August 20, 2021)</p> <p>================================================================================ Some portions of Thrust may be licensed under other compatible open-source licenses. Any divergence from the Apache 2 license will be noted in the source code where applicable. Portions under other terms include, but are not limited to: ================================================================================</p> <p>Various C++ utility classes in Thrust are based on the Boost Iterator, Tuple, System, and Random Number libraries, which are provided under the Boost Software License:</p> <pre><code>Boost Software License - Version 1.0 - August 17th, 2003\n\nPermission is hereby granted, free of charge, to any person or organization\nobtaining a copy of the software and accompanying documentation covered by\nthis license (the \"Software\") to use, reproduce, display, distribute,\nexecute, and transmit the Software, and to prepare derivative works of the\nSoftware, and to permit third-parties to whom the Software is furnished to\ndo so, all subject to the following:\n\nThe copyright notices in the Software and this entire statement, including\nthe above license grant, this restriction and the following disclaimer,\nmust be included in all copies of the Software, in whole or in part, and\nall derivative works of the Software, unless such copies or derivative\nworks are solely in the form of machine-executable object code generated by\na source language processor.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT\nSHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE\nFOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE.\n</code></pre> <p>================================================================================</p> <p>Portions of the thrust::complex implementation are derived from FreeBSD with the following terms:</p> <p>================================================================================</p> <pre><code>Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions\nare met:\n\n1. Redistributions of source code must retain the above copyright\n   notice[1] unmodified, this list of conditions, and the following\n   disclaimer.\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\nIMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\nOF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\nIN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\nINCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\nNOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\nTHIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre> <p>[1] Individual copyright notices from the original authors are included in     the relevant source files.</p> <p>============================================================================== CUB's source code is released under the BSD 3-Clause license: ============================================================================== Copyright (c) 2010-2011, Duane Merrill.  All rights reserved. Copyright (c) 2011-2023, NVIDIA CORPORATION.  All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:    *  Redistributions of source code must retain the above copyright       notice, this list of conditions and the following disclaimer.    *  Redistributions in binary form must reproduce the above copyright       notice, this list of conditions and the following disclaimer in the       documentation and/or other materials provided with the distribution.    *  Neither the name of the NVIDIA CORPORATION nor the       names of its contributors may be used to endorse or promote products       derived from this software without specific prior written permission.</p> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>============================================================================== The ROCm project is distributed under the following license ==============================================================================</p> <p>MIT License</p> <p>Copyright \u00a9 2023 - 2024 Advanced Micro Devices, Inc. All rights reserved.</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>============================================================================== The ROCm ROCT-Thunk-Interface is distributed under the following license ==============================================================================</p> <p>ROCT-Thunk Interface LICENSE</p> <p>Copyright (c) 2016 Advanced Micro Devices, Inc. All rights reserved.</p> <p>MIT LICENSE: Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>This product contains software provided by Nginx, Inc. and its contributors.</p> <p>Copyright (C) 2002-2018 Igor Sysoev Copyright (C) 2011-2018 Nginx, Inc. All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <p>Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p> <p>============================================================================== The ROCm ROCR-Runtime project is distributed under the following license ==============================================================================</p> <p>The University of Illinois/NCSA Open Source License (NCSA)</p> <p>Copyright (c) 2024, Advanced Micro Devices, Inc. All rights reserved.</p> <p>Developed by:</p> <pre><code>            AMD Research and AMD HSA Software Development\n\n            Advanced Micro Devices, Inc.\n\n            www.amd.com\n</code></pre> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <ul> <li>Redistributions of source code must retain the above copyright notice,    this list of conditions and the following disclaimers.</li> <li>Redistributions in binary form must reproduce the above copyright    notice, this list of conditions and the following disclaimers in    the documentation and/or other materials provided with the distribution.</li> <li>Neither the names of Advanced Micro Devices, Inc,    nor the names of its contributors may be used to endorse or promote    products derived from this Software without specific prior written    permission.</li> </ul> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.</p> <p>============================================================================== The erfinvf() function was implemented from scratch with inspiration from  the following algorithm:</p> <p>\u201cApproximating the erfinv function.\" - M. Giles https://people.maths.ox.ac.uk/gilesm/files/gems_erfinv.pdf ==============================================================================</p> <p>============================================================================== The erfcinvf() function's source code is released under the following license: ============================================================================== /*   Copyright 2023, Norbert Juffa</p> <p>Redistribution and use in source and binary forms, with or without   modification, are permitted provided that the following conditions   are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright      notice, this list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright      notice, this list of conditions and the following disclaimer in the      documentation and/or other materials provided with the distribution.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS   \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT   LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR   A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT   HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,   SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT   LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,   DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY   THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT   (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE   OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. */</p> <p>============================================================================== Blender Cycles is released under the Apache 2.0 license: ==============================================================================</p> <pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright [yyyy] [name of copyright owner]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p> <p>============================================================================== The docopt.cpp project is released under the following license: ==============================================================================</p> <p>Copyright (c) 2012 Vladimir Keleshev, vladimir@keleshev.com</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"use_of_trademarks/","title":"Use of Trademarks","text":"<p>All uses of trademarks on this website are purely for nominative and/or descriptive purposes. We do not claim any affiliations, partnerships, licensing agreements or any other kind of association with Nvidia Corporation, Advanced Micro Devices, Inc. or Intel Corporation.</p> <ul> <li>CUDA is a registered trademark of the Nvidia Corporation.</li> <li>AMD ROCm is a registered trademark of Advanced Micro Devices, Inc.</li> <li>SCALE is a registered trademark of Spectral Compute Ltd.</li> </ul> <p>Please contact us at legal@spectralcompute.co.uk for any inquiries or corrections regarding our use of trademarks.</p>"},{"location":"contact/join-our-discord/","title":"Join our Discord","text":"<p>The success of SCALE very much depends on the developer community supporting it. We invite anyone curious about our technology to join our Discord server and follow our journey.</p> <p>On our Discord, we aim to:</p> <ul> <li>Inform about upcoming SCALE releases</li> <li>Share latest our SCALE blog posts</li> <li>Deep dive on technical questions (nerd out)</li> <li>Discuss HPC industry trends</li> <li>Gather feedback for our amazing community!</li> </ul>"},{"location":"contact/report-a-bug/","title":"Report a Bug","text":"<p>SCALE is still in active development, so you may encounter bugs. If you run  into problems, contact us by:</p> <ul> <li>Joining our Discord</li> <li>Creating a ticket</li> <li>Sending us an e-mail at hello@spectralcompute.co.uk</li> </ul> <p>The remainder of this page provides information about how to make your  report as helpful as possible.</p>"},{"location":"contact/report-a-bug/#no-such-function-cudasomethingsomething","title":"\"No such function: cudaSomethingSomething()\"","text":"<p>If your project fails to compile due to a missing CUDA Runtime or Driver API function, get in touch: this helps us prioritise work by fixing the holes that have the most demand first.</p>"},{"location":"contact/report-a-bug/#no-such-function-cublascufftcusolversomethingsomething","title":"\"No such function: cuBlas/cuFFt/cuSolverSomethingSomething()\"","text":"<p>If your project needs a missing \"CUDA-X\" API (cuBLAS, cuFFT, cuSOLVER and friends), this is most likely something you can fix yourself by submitting a patch to the open-source library wrapper project. So long as an equivalent function is available in a ROCm library, the wrapper code is trivial.</p>"},{"location":"contact/report-a-bug/#compiler-crash","title":"Compiler crash","text":"<p>When the compiler crashes, it creates temporary files containing a reproducer for the compiler crash, like this:</p> <pre><code>********************\n\nPLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:\nPreprocessed source(s) and associated run script(s) are located at:\nclang++: note: diagnostic msg: /tmp/a-02f191.cpp\nclang++: note: diagnostic msg: /tmp/a-02f191.sh\nclang++: note: diagnostic msg:\n\n********************\n</code></pre> <p>These files will contain the preprocessed version of the source file that broke the compiler, among other things. If you are able to share this with us, it will significantly increase the usefulness of the bug report.</p> <p>If the source file contains sensitive/proprietary information, this could be destroyed by reducing the testcase using cvise. Alternatively, a bug report consisting of just the compiler output is still helpful - especially if it relates to PTX.</p>"},{"location":"contact/report-a-bug/#gpu-crash","title":"GPU Crash","text":"<p>If your GPU code crashes with SCALE but not with NVIDIA's compiler, more useful information can be harvested by enabling some environment variables that dump extra information. If you are able, sharing the output obtained from reproducing the crash with one or both of these enabled can be helpful:</p> <ul> <li><code>REDSCALE_CRASH_REPORT_DETAILED=1</code> will dump extra information from the   GPU trap handler. This includes register state and some symbol names, so   it is unlikely to contain any sensitive/proprietary information from your code.</li> <li><code>REDSCALE_CRASH_DUMP=somefilename</code> will write the crashing machine code to   a file. This makes it easier to investigate the problem, but it means that you're   sharing the compiled version of the crasing GPU kernel with us.</li> </ul>"},{"location":"contact/report-a-bug/#something-else","title":"Something else","text":"<p>It will be helpful if you provide the output of the following commands along with your report:</p> <pre><code>lspci | grep VGA\nscaleinfo\n</code></pre> <p>Running your program with the environment variable <code>SCALE_EXCEPTIONS=1</code> set might give a more detailed error that would be helpful to us too.</p>"},{"location":"examples/","title":"SCALE Example Programs","text":"<p>These example programs are simple CUDA programs demonstrating the  capabilities of SCALE.</p> <p>SCALE is capable of much more, but these small demonstrations serve as a  proof of concept of CUDA compatibility, as well as a starting point for  users wishing to get into GPGPU programming.</p>"},{"location":"examples/#list-of-examples","title":"List of examples","text":"<p>Here is the list of examples that are currently available:</p> Example What it is about Basic Usage in its simplest form PTX Using PTX Assembly BLAS Using BLAS maths wrapper"},{"location":"examples/#accessing-the-examples","title":"Accessing the examples","text":"<p>The examples are hosted in the public github repository with the rest of this manual.</p> <pre><code>git clone https://github.com/spectral-compute/scale-docs.git\ncd scale-docs/examples\n</code></pre>"},{"location":"examples/#using-the-examples","title":"Using the examples","text":"<p>To build an example: - Install SCALE - Decide on a GPU target - Build the example using cmake</p> <p>The example repository includes a helper script, <code>example.sh</code> that can fully  automate the process. Pass your SCALE target directory as the first argument, and the example you want to build/run as the second:</p> <pre><code># You should be in the `examples` directory of the `scale-docs` repository\n./example.sh /opt/scale gfx1030 basic\n</code></pre> <p>For the specified example, this will:</p> <ol> <li>Remove its build directory if it already exists.</li> <li>Configure CMake for that example in a freshly-created build directory.</li> <li>Build the example in that directory using Make.</li> <li>Set the <code>SCALE_EXCEPTIONS=1</code> environment variable for better     error reporting.</li> <li>Run the example.</li> </ol>"},{"location":"examples/basic/","title":"Basic example","text":"<p>This is simple vector-sum kernel using CUDA. </p> <p>The example:</p> <ul> <li>Generates test data on the host</li> <li>Sends data to the device</li> <li>Launches a kernel on the device</li> <li>Receives data back from the device</li> <li>Checks that the data is correct</li> </ul> <p>Build and run the example by following the general instructions.</p>"},{"location":"examples/basic/#example-source-code","title":"Example source code","text":"<pre><code>#include &lt;vector&gt;\n#include &lt;iostream&gt;\n\n\n// The kernel we are going to launch\n__global__ void basicSum(const int * a, const int * b, size_t n, int * out) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx &lt; n)\n    {\n        out[idx] = a[idx] + b[idx];\n    }\n}\n\n\n// A generic helper function to simplify error handling.\nvoid check(cudaError_t error, const char * file, size_t line) {\n    if (error != cudaSuccess)\n    {\n        std::cout &lt;&lt; \"cuda error: \" &lt;&lt; cudaGetErrorString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\n// A wrapper for the helper function above to include the filename and line number\n// where the error occurs into the output.\n#define CHECK(error) check(error, __FILE__, __LINE__)\n\n\nint main(int argc, char ** argv) {\n\n    const size_t N = 4096;\n    const size_t BYTES = N * sizeof(int);\n\n    std::vector&lt;int&gt; a(N);\n    std::vector&lt;int&gt; b(N);\n    std::vector&lt;int&gt; out(N);\n\n    // Generate input data\n    for (size_t i = 0; i &lt; N; i++) {\n        a[i] = i * 2;\n        b[i] = N - i;\n    }\n\n    int * devA;\n    int * devB;\n    int * devOut;\n\n    // Allocate memory for the inputs and the output\n    CHECK(cudaMalloc(&amp;devA, BYTES));\n    CHECK(cudaMalloc(&amp;devB, BYTES));\n    CHECK(cudaMalloc(&amp;devOut, BYTES));\n\n    // Copy the input data to the device\n    CHECK(cudaMemcpy(devA, a.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devB, b.data(), BYTES, cudaMemcpyHostToDevice));\n\n    // Launch the kernel\n    basicSum&lt;&lt;&lt;N / 256 + 1, 256&gt;&gt;&gt;(devA, devB, N, devOut);\n    CHECK(cudaDeviceSynchronize());\n    CHECK(cudaGetLastError());\n\n    // Copy the output data back to host\n    CHECK(cudaMemcpy(out.data(), devOut, BYTES, cudaMemcpyDeviceToHost));\n\n    // Free up the memory we allocated for the inputs and the output\n    CHECK(cudaFree(devA));\n    CHECK(cudaFree(devB));\n    CHECK(cudaFree(devOut));\n\n    // Test that the output matches our expectations\n    for (size_t i = 0; i &lt; N; i++) {\n        if (a[i] + b[i] != out[i]) {\n            std::cout &lt;&lt; \"Incorrect sum: \" &lt;&lt; a[i] &lt;&lt; \" + \" &lt;&lt; b[i] &lt;&lt; \" = \" &lt;&lt; out[i] &lt;&lt; \" ?\\n\";\n        }\n    }\n\n    std::cout &lt;&lt; \"Example finished\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"examples/basic/#cmakeliststxt-used","title":"<code>CMakeLists.txt</code> used","text":"<pre><code>cmake_minimum_required(VERSION 3.17 FATAL_ERROR)\nproject(example_basic LANGUAGES CUDA)\n\nadd_executable(example_basic basic.cu)\n</code></pre>"},{"location":"examples/blas/","title":"BLAS example","text":"<p>This example demonstrates SCALE's compatibility with cuBLAS APIs by using  cuBLAS to perform a double-precision dot-product on an AMD GPU.</p> <p>cuBLAS APIs are forwarded to use the relevant ROCm APIs. Note that the example links to <code>cublas</code> in its <code>CMakeLists.txt</code>.</p>"},{"location":"examples/blas/#example-source-code","title":"Example source code","text":"<pre><code>#include &lt;vector&gt;\n#include &lt;iostream&gt;\n\n#include &lt;cublas_v2.h&gt;\n\n\nvoid check(cudaError_t error, const char * file, size_t line) {\n    if (error != cudaSuccess)\n    {\n        std::cout &lt;&lt; \"cuda error: \" &lt;&lt; cudaGetErrorString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\nvoid checkCublas(cublasStatus_t error, const char * file, size_t line) {\n    if (error != CUBLAS_STATUS_SUCCESS) {\n        std::cout &lt;&lt; \"cublas error: \" &lt;&lt; cublasGetStatusString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\n#define CHECK(error) check(error, __FILE__, __LINE__)\n#define CHECK_CUBLAS(error) checkCublas(error, __FILE__, __LINE__)\n\n\nint main(int argc, char ** argv) {\n    cublasHandle_t handle;\n    CHECK_CUBLAS(cublasCreate(&amp;handle));\n\n    const size_t N = 10;\n    const size_t BYTES = N * sizeof(double);\n    const double E = 1e-5;\n\n    /* Prepare the data */\n\n    std::vector&lt;double&gt; A(N);\n    std::vector&lt;double&gt; B(N);\n\n    for (size_t i = 0; i &lt; N; i++) {\n        A[i] = i;\n        B[i] = i + N;\n    }\n\n    /* Send the data */\n\n    double * devA;\n    double * devB;\n\n    CHECK(cudaMalloc(&amp;devA, BYTES));\n    CHECK(cudaMalloc(&amp;devB, BYTES));\n\n    CHECK(cudaMemcpy(devA, A.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devB, B.data(), BYTES, cudaMemcpyHostToDevice));\n\n    /* Calculate */\n\n    const int strideA = 1;\n    const int strideB = 1;\n    double result = 0;\n\n    CHECK_CUBLAS(cublasDdot(handle, A.size(), devA, strideA, devB, strideB, &amp;result));\n\n    CHECK(cudaDeviceSynchronize());\n\n    double expected = 0;\n    for (size_t i = 0; i &lt; N; i++) {\n        expected += A[i] * B[i];\n    }\n\n    if (std::abs(result - expected) &gt; E) {\n        std::cout &lt;&lt; \"Result \" &lt;&lt; result &lt;&lt; \" is different from expected \" &lt;&lt; expected &lt;&lt; std::endl;\n    }\n\n    CHECK_CUBLAS(cublasDestroy(handle));\n\n    std::cout &lt;&lt; \"Example finished.\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"examples/blas/#cmakeliststxt-used","title":"<code>CMakeLists.txt</code> used","text":"<pre><code>cmake_minimum_required(VERSION 3.17 FATAL_ERROR)\nproject(example_blas LANGUAGES CUDA)\n\nadd_executable(example_blas blas.cu)\ntarget_link_libraries(example_blas PRIVATE cublas redscale)\n</code></pre>"},{"location":"examples/ptx/","title":"PTX example","text":"<p>This example demonstrates SCALE's support for inline PTX. A lot of real-world  CUDA code uses inline PTX asm blocks, which are inherently NVIDIA-only. There is no  need to rewrite those when using SCALE: the compiler just digests them and  outputs AMD machine code.</p> <p>This example uses C++ templates to access the functionality of the PTX  <code>lop3</code> instruction, used in various ways throughout the kernel.</p> <p>Build and run the example by following the general instructions.</p>"},{"location":"examples/ptx/#extra-info","title":"Extra info","text":"<ul> <li>Using inline PTX Assembly</li> <li>PTX ISA reference</li> </ul> <p>PTX instructions used:</p> <ul> <li><code>add</code></li> <li><code>lop3</code></li> </ul>"},{"location":"examples/ptx/#example-source-code","title":"Example source code","text":"<pre><code>#include &lt;bitset&gt;\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;cstdint&gt;\n\n\n__device__ inline uint32_t ptx_add(uint32_t x, uint32_t y) {\n    // Calculate a sum of `x` and `y`, put the result into `x`\n    asm(\n        \"add.u32 %0, %0, %1;\"\n        : \"+r\"(x)\n        : \"r\"(y)\n    );\n    return x;\n}\n\n\n__global__ void kernelAdd(const uint32_t * a, const uint32_t * b, size_t n, uint32_t * out) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx &lt; n)\n    {\n        out[idx] = ptx_add(a[idx], b[idx]);\n    }\n}\n\n\ntemplate&lt;uint8_t Op&gt;\n__device__ inline uint32_t ptx_lop3(uint32_t x, uint32_t y, uint32_t z) {\n    // Compute operator `Op` on `x`, `y`, `z`, put the result into `x`\n\n    asm(\n        \"lop3.b32 %0, %0, %1, %2, %3;\"\n        : \"+r\"(x)\n        : \"r\"(y), \"r\"(z), \"n\"(Op)\n    );\n    return x;\n}\n\n\ntemplate&lt;uint8_t Op&gt;\n__global__ void kernelLop3(const uint32_t * a, const uint32_t * b, const uint32_t * c, size_t n, uint32_t * out) {\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if(idx &lt; n)\n    {\n        out[idx] = ptx_lop3&lt;Op&gt;(a[idx], b[idx], c[idx]);\n    }\n}\n\n\nvoid check(cudaError_t error, const char * file, size_t line) {\n    if (error != cudaSuccess)\n    {\n        std::cout &lt;&lt; \"cuda error: \" &lt;&lt; cudaGetErrorString(error) &lt;&lt; \" at \" &lt;&lt; file &lt;&lt; \":\" &lt;&lt; line &lt;&lt; std::endl;\n        exit(1);\n    }\n}\n\n\n#define CHECK(error) check(error, __FILE__, __LINE__)\n\n\ntemplate&lt;typename T&gt;\nconstexpr T lop3op(T a, T b, T c) {\n    return a &amp; b ^ (~c);\n}\n\n\nint main(int argc, char ** argv) {\n\n    const size_t N = 4096;\n    const size_t BYTES = N * sizeof(uint32_t);\n\n    std::vector&lt;uint32_t&gt; a(N);\n    std::vector&lt;uint32_t&gt; b(N);\n    std::vector&lt;uint32_t&gt; c(N);\n    std::vector&lt;uint32_t&gt; out(N);\n\n    for (size_t i = 0; i &lt; N; i++) {\n        a[i] = i * 2;\n        b[i] = N - i;\n        c[i] = i * i;\n    }\n\n    uint32_t * devA;\n    uint32_t * devB;\n    uint32_t * devC;\n    uint32_t * devOut;\n\n    CHECK(cudaMalloc(&amp;devA, BYTES));\n    CHECK(cudaMalloc(&amp;devB, BYTES));\n    CHECK(cudaMalloc(&amp;devC, BYTES));\n    CHECK(cudaMalloc(&amp;devOut, BYTES));\n\n    CHECK(cudaMemcpy(devA, a.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devB, b.data(), BYTES, cudaMemcpyHostToDevice));\n    CHECK(cudaMemcpy(devC, c.data(), BYTES, cudaMemcpyHostToDevice));\n\n    // Test \"add\"\n\n    kernelAdd&lt;&lt;&lt;N / 256 + 1, 256&gt;&gt;&gt;(devA, devB, N, devOut);\n    CHECK(cudaDeviceSynchronize());\n    CHECK(cudaGetLastError());\n\n    CHECK(cudaMemcpy(out.data(), devOut, BYTES, cudaMemcpyDeviceToHost));\n\n    for (size_t i = 0; i &lt; N; i++) {\n        if (a[i] + b[i] != out[i]) {\n            std::cout &lt;&lt; \"Incorrect add: \" &lt;&lt; a[i] &lt;&lt; \" + \" &lt;&lt; b[i] &lt;&lt; \" = \" &lt;&lt; out[i] &lt;&lt; \" ?\\n\";\n        }\n    }\n\n    // Test \"lop3\"\n\n    constexpr uint8_t TA = 0xF0;\n    constexpr uint8_t TB = 0xCC;\n    constexpr uint8_t TC = 0xAA;\n    constexpr uint8_t Op = lop3op(TA, TB, TC);\n\n    kernelLop3&lt;Op&gt;&lt;&lt;&lt;N / 256 + 1, 256&gt;&gt;&gt;(devA, devB, devC, N, devOut);\n    CHECK(cudaDeviceSynchronize());\n    CHECK(cudaGetLastError());\n\n    CHECK(cudaMemcpy(out.data(), devOut, BYTES, cudaMemcpyDeviceToHost));\n\n    for (size_t i = 0; i &lt; N; i++) {\n        if (lop3op(a[i], b[i], c[i]) != out[i]) {\n            std::cout &lt;&lt; \"Incorrect lop3: \\n\"\n                &lt;&lt; \"    \" &lt;&lt; std::bitset&lt;32&gt;{a[i]} &lt;&lt; \"\\n\"\n                &lt;&lt; \" &amp;  \" &lt;&lt; std::bitset&lt;32&gt;{b[i]} &lt;&lt; \"\\n\"\n                &lt;&lt; \" ^ ~\" &lt;&lt; std::bitset&lt;32&gt;{c[i]} &lt;&lt; \"\\n\"\n                &lt;&lt; \" =  \" &lt;&lt; std::bitset&lt;32&gt;{out[i]} &lt;&lt; \" ?\\n\\n\";\n        }\n    }\n\n    CHECK(cudaFree(devA));\n    CHECK(cudaFree(devB));\n    CHECK(cudaFree(devC));\n    CHECK(cudaFree(devOut));\n\n    // Finish\n\n    std::cout &lt;&lt; \"Example finished\" &lt;&lt; std::endl;\n\n    return 0;\n}\n</code></pre>"},{"location":"examples/ptx/#cmakeliststxt-used","title":"<code>CMakeLists.txt</code> used","text":"<pre><code>cmake_minimum_required(VERSION 3.17 FATAL_ERROR)\nproject(example_ptx LANGUAGES CUDA)\n\nadd_executable(example_ptx ptx.cu)\n</code></pre>"},{"location":"manual/CHANGELOG/","title":"What's new?","text":""},{"location":"manual/CHANGELOG/#release-150-2025-11-15","title":"Release 1.5.0 (2025-11-15)","text":""},{"location":"manual/CHANGELOG/#platform","title":"Platform","text":"<ul> <li>Compiler is now based on llvm 20.1.8</li> <li>Using rocm 7.1 versios of rocBLAS etc.</li> </ul>"},{"location":"manual/CHANGELOG/#supported-architectures","title":"Supported architectures","text":"<ul> <li>All architectures are now enabled in the free version of SCALE. See new EULA for details.</li> <li>Newly-supported AMD GPU architectures:<ul> <li><code>gfx950</code> (MI350x/MI355x)</li> <li><code>gfx1151</code> (Strix-halo etc.)</li> <li><code>gfx1200</code> (RX 9060 XT etc.)</li> <li><code>gfx1201</code> (RX 9070 XT etc.)</li> <li><code>gfx908</code> (MI100)</li> <li><code>gfx906</code> (MI50/MI60)</li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#compiler-inline-ptx-support","title":"Compiler: Inline PTX support","text":"<ul> <li>Improve PTX diagnostics for unused instruction flags</li> <li>Support for <code>q</code> constraints (128-bit asm inputs)</li> <li>Diagnostics for implicit truncation via asm constraints.</li> <li>New PTX instructions:<ul> <li><code>movmatrix</code></li> <li><code>bar</code>/<code>barrier</code>: only cases that can be represented as <code>__synthreads_*</code>.</li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#compiler-nvcc-emulation","title":"Compiler: NVCC emulation","text":"<ul> <li>New NVCC flags:<ul> <li><code>-keep</code></li> <li><code>-keep-dir</code></li> <li><code>-link</code></li> <li><code>-preprocess</code> (alias of <code>-E</code>)</li> <li><code>-libdevice-directory</code>/<code>-ldir</code>: Meaningless</li> <li><code>-target-directory</code>/<code>target-dir</code>: Meaningless</li> <li><code>-cudadevrt</code>: no-op because our devrt implementation uses no smem, so no point.</li> <li><code>-opt-info</code>: alias of <code>-Rpass</code>, so it can do more than just <code>-inline</code></li> </ul> </li> <li>Fix resolution of relative paths for nvcc's option-files flag.</li> <li>Fix compiler crash when <code>fence.cluster</code> appears in inline PTX in dead code.</li> <li>Accept even more cases of invalid identifiers in unused code in nvcc mode.</li> <li>Improvements and fixes to the <code>__shfl*</code> optimiser. More DPP, less UB.</li> <li>Add a compiler warning to complain about mixing <code>__constant__</code> and <code>constexpr</code>.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler-misc","title":"Compiler (misc.)","text":"<ul> <li>Introduce builtins for conversion between arbitrary types (fp8 here we come)</li> <li>Further improvements to deferred diagnostics, especialy surrounding typo'd identifiers</li> <li>Add <code>[[clang::getter]]</code></li> <li>Fix kernel argument buffer alignment sometimes being wrong</li> <li>Microsoft extensions are no longer enabled in CUDA mode.</li> <li>Arithmetic involving <code>threadIdx</code> and friends now compiles.</li> <li>Various small optimiser enhancements.</li> </ul>"},{"location":"manual/CHANGELOG/#runtime","title":"Runtime","text":"<ul> <li>Fix a rare race condition resolving cudaEvents</li> <li>Slightly improve performance of every API by optimising error handling routines.</li> <li>Introduce nvtx3 support as a header-only library, like it should be.</li> <li>New API: <code>cudaEventRecordWithFlags</code></li> <li>Respect <code>CUDA_GRAPHS_USE_NODE_PRIORITY</code> environment variable.</li> <li>Implement UUID-query APIs, and make them consistent between the driver API and nvml</li> <li>Support the new <code>__nv_*</code> atomic functions</li> <li>Support (and ignore) the ancient <code>CU_CTX_MAP_HOST/cudaDeviceMapHost</code> flags. This feature is always enabled.</li> <li>Startup-time improvements.</li> <li>Fix crash when empty CUDA graphs are executed.</li> <li>Fix occasionally picking the wrong cache coherence domain for SDMA engines and breaking everything.</li> <li>fp16x2/bf16x2 are now trivially-copyable in C++11+: a very tiny extension.</li> <li>Fix intermittent crash in cuMemUnmap</li> <li>Add the new CUDA13 aligned vector types</li> <li>Workaround SDMA bug that gave incorrect results for cuMemsetD16 on some devices.</li> <li>Many random header compatibility/typedef fixes.</li> <li>Accuracy improvements to <code>tanh()</code> and <code>sin()</code>.</li> <li>Fix crash when millions of cudaStreams are destroyed all at once.</li> <li>Crash correctly when the GPU executes a trap instruction.</li> <li>Fix the GPU trap handler on GFX94x devices.</li> <li>Fix a few C89-compatibility issues in less-commonly-used headers.</li> <li>Make headers warning-free on GCC, since not everyone uses <code>-isystem</code> properly.</li> <li>Slightly improve performance of nvRTC.</li> <li>Raise an error if the user attempts to execute PTX as AMDGPU machine code, instead of actually trying it.</li> <li>Fix a few cases where the runtime library and RTC compiler would disagree about what architecture to build for.</li> </ul>"},{"location":"manual/CHANGELOG/#release-142-2025-09-19","title":"Release 1.4.2 (2025-09-19)","text":""},{"location":"manual/CHANGELOG/#platformpackaging","title":"Platform/Packaging","text":"<ul> <li>Simplified packaging/installation: removed dependency on rocm package repos. Removal of more rocm components from the package is in development.</li> <li>Using rocm 6.4.1 versions of rocBLAS etc.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler-diagnostics","title":"Compiler Diagnostics","text":"<ul> <li>Warn about PTX instruction flags that don't actually do anything.</li> <li>Warn about PTX variable declarations leaking into the enclosing function, since this may cause ptxas failures   when building for NVIDIA.</li> <li>Warn about passing a generic address space C++ pointer into an asm operand for a non-generic PTX memory instruction if the   corresponding addrspacecast is not a no-op.</li> <li>Detect when the user seems to have gotten <code>cvta.to</code> and <code>cvta</code> mixed up.</li> <li>Cleanly reject <code>.param</code> address space in PTX: using that in inline asm is undefined behaviour.</li> <li>Diagnose accidental address operands (eg. use of <code>[]</code> for <code>cvta</code> is a common screwup).</li> <li>Proper errors for integer width mismatch in PTX address space conversions. Implicitly truncating pointers is bad.</li> <li>Disallow overloads that differ only in return type and sideness in clang-dialect mode.</li> <li>Implement a fancier-than-nvcc edition of <code>-Wdefault-stream-launch</code>, warning about implicit use of the legacy default stream via any launch mechanism, with opt-in support for warning about any use of the default stream (ptds or not).</li> <li>PTX diagnositcs now correctly point at the offending operand, where applicable.</li> <li>Correctly report source locations for diagnostics in PTX blocks that use <code>C</code> constraints.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler-optimisation","title":"Compiler Optimisation","text":"<ul> <li>Use DPP to optimise applicable <code>__shfl*/__reduce*</code> calls, and loops that perform common reduction/cumsum idioms.</li> <li>Improved instruction selection for shared-memory reads of sub-word types.</li> <li>Vectorisation support for <code>__shfl*</code> (eg. turns multiple i8 shuffles into an i32 or i64 shuffle). Particularly useful for exploiting architectures that have support for i64 shuffles.</li> <li>Ability to move extending conversions across shuffles, to avoid shuffling useless bits.</li> <li>Don't generate redundant warp reductions prior to uniform atomic ops.</li> <li>Constant propagation for <code>__shfl*</code>.</li> <li>Improved program startup time in the presence of CUDA translation units with no device code.</li> <li>Improvements to <code>i1</code> vectorisation.</li> <li>Don't discard <code>__builtin_provable</code> too early when doing LTO or building bitcode libraries.</li> </ul>"},{"location":"manual/CHANGELOG/#nvcc-compatibility","title":"NVCC Compatibility","text":"<ul> <li>Support for <code>__shared__</code> and <code>__constant__</code> keywords an anonymous unions/structs.</li> <li>Allow <code>__device__</code> keyword to redundantly accompany <code>__shared__</code> or <code>__constant__</code>.</li> <li>Remove spurious warning about passing C++ pointers to memory ops.</li> <li>Commas in asm statement argument lists are, it turns out, optional.</li> <li>Support <code>forward-unknown-opts</code>.</li> <li>Correctly handle missing <code>template</code> keywords in more dependent name lookup scenarios.</li> <li>Tolerate wrong-sided access during constant evaluation.</li> <li>Cleaner diagnostics when handling redundant commas in the presence of multiple templates closing at once.</li> <li>Allow out-of-line redeclaration of namespaced template functions with conflicting signatures (this becomes an overload, when it should be a compile error).</li> </ul>"},{"location":"manual/CHANGELOG/#inline-ptx-support","title":"Inline PTX Support","text":"<ul> <li>Support for undocumented syntax quirks, like spaces in the middle of directives.</li> <li>Support for a wider variety of constant expressions.</li> <li>Fix miscompilation of <code>min.xorsign.abs</code>.</li> <li>Fix miscompilation of <code>testp.finite</code> in <code>-ffast-math</code> mode.</li> <li>Correct behaviour of <code>dp4a/dp2a</code> in the presence of overflow.</li> <li>Correctly parse identifiers including <code>%</code> on asm blocks with no inputs/outputs.</li> <li>Fix miscompile of shifts/bmsk with extremely large shift amounts.</li> <li>Implement insane implicit asm-input conversion rules.</li> <li>Fix miscompiles in video instructions using min/max as the secondary op, since real behaviour   turned out to differ from the manual.</li> <li>Avoid compiler crash when trying to constant-evaluate a PTX <code>n</code> input that has a type error.</li> <li><code>tf32</code> support, via upcasting to <code>fp32</code>.</li> <li>Fix some corner cases of mixing vector splats with <code>_</code> operands.</li> <li>Fix crash parsing video byte selectors.</li> <li>Correctly handle implicit vector-of-vector types, because it turns out those are at least semantically a thing.</li> <li>Newly-supported instructions:<ul> <li><code>cp.async.*</code></li> <li><code>min/max</code>, 3-input edition.</li> <li><code>add/sub/mul/fma/cvt</code> with non-default rounding modes.</li> <li><code>mma/wmma</code> for sub-byte integer types.</li> <li><code>mma/wmma</code> for all remaining shapes, in all datatypes except fp8/6/4.</li> <li><code>red/atom</code> for vector types.</li> <li><code>cvt.pack</code>, 4-input edition.</li> <li><code>nanosleep</code></li> </ul> </li> <li>Newly-supported special registers:<ul> <li><code>%globaltimer</code></li> <li><code>%globaltimer_hi</code></li> <li><code>%globaltimer_lo</code></li> <li><code>%warpid</code></li> <li><code>%nwarpid</code></li> <li><code>%smid</code></li> <li><code>%nsmid</code></li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#compiler-misc_1","title":"Compiler Misc.","text":"<ul> <li>Fixed compiler crash when doing <code>decltype(lambda)</code> in certain conditions.</li> <li><code>__syncthreads_and()</code> and friends no longer use any shared memory.</li> <li>Improvements to compile speed, espeically in the presence of inline PTX. Extremely so in the presence of <code>mma/wmma</code>.</li> </ul>"},{"location":"manual/CHANGELOG/#library-new-apis","title":"Library: New APIs","text":"<p>See the API diffs for precise information.</p> <ul> <li>Added most commonly-used NVML APIs.</li> <li>More <code>cuSOLVER/cuSPARSE</code>.</li> <li>Non-default-rounding-mode APIs (conversions and arithmetic).</li> <li>Added 1D CUarray copy APIs.</li> <li>Added device-side versions of device/context property query APIs (<code>cudaDeviceGetAttribute</code> etc).</li> <li><code>bmma_sync</code>.</li> <li><code>atomicAdd</code> for <code>float2</code> and <code>float4</code>.</li> <li>Added API for programmatically controlling SCALE's exception behaviour.</li> </ul>"},{"location":"manual/CHANGELOG/#library-fixes","title":"Library: Fixes","text":"<ul> <li>Allocator improvements prevent premature OOM due to address space fragmentation in long-running applications with a high level of memory churn.</li> <li>Fix const-correctness in coop-groups.</li> <li>Removed some macro leaks.</li> <li><code>__sad()</code> now behaves correctly in the presence of integer overload.</li> <li>Fix bugs in FFT plan creation.</li> <li>Fix some edgecases relating to delting the active CUcontext before popping it.</li> <li>Fixed use of <code>cuCtxSetCurrent</code> when stack is empty</li> <li>Don't crash when unloading a module at the same time as launching one of its kernels.</li> <li><code>scaleenv</code> now works with zsh, and does not pollute the shell environment.</li> <li>Endless tiny fixes, random macros, header compatibility tweaks, etc.</li> <li>Device PCI IDs now match the format of NVIDIA CUDA exactly.</li> <li>Fix some edgecases where denorms weren't being flushed, but should be.</li> <li>Stream creation is faster.</li> <li>Don't crash when the printf buffer is larger than 4GB.</li> <li>Fix rare hang when using the IPC APIs.</li> </ul>"},{"location":"manual/CHANGELOG/#release-131-2025-05-12","title":"Release 1.3.1 (2025-05-12)","text":""},{"location":"manual/CHANGELOG/#compiler","title":"Compiler","text":"<ul> <li>Fixed a bug in the handling of weak device-side symbols which broke the   device binaries for certain projects.</li> <li>Fixed various PTX miscompilations.</li> <li>Added support for approximate-math PTX instructions (<code>lg2.approx</code> and   friends).</li> </ul>"},{"location":"manual/CHANGELOG/#library","title":"Library","text":"<ul> <li>Fixed many small bugs in the device-side APIs.</li> <li>Per-thread-default-stream actually works now, rather than silently using   the legacy stream.</li> <li>Fixed a race condition in the fft library.</li> </ul>"},{"location":"manual/CHANGELOG/#thirdparty-project-demos","title":"Thirdparty Project demos","text":"<ul> <li>GROMACS now works. SCALE appears to support a wider selection of AMD   architectures than the HIP port, and seems to perform somewhat better (on   MI210, at least!).</li> </ul>"},{"location":"manual/CHANGELOG/#release-130-2025-04-23","title":"Release 1.3.0 (2025-04-23)","text":""},{"location":"manual/CHANGELOG/#platform_1","title":"Platform","text":"<ul> <li>Upgraded from llvm17 to llvm19.1.7.</li> <li>Support for rocm 6.3.1.</li> <li>Support for <code>gfx902</code> architecture.</li> <li>Enterprise edition: Support for new architectures:<ul> <li><code>gfx908</code></li> <li><code>gfx90a</code></li> <li><code>gfx942</code></li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#packaging","title":"Packaging","text":"<ul> <li>Packages for Rocky9 are now available.</li> <li>Package repos for Ubuntu and Rocky9 to simplify   installation/upgrades.</li> </ul>"},{"location":"manual/CHANGELOG/#new-features","title":"New Features","text":"<ul> <li>Added <code>scaleenv</code>, a new and much easier way to use SCALE.</li> <li>Support for simulating a warp size of 32 even on wave64 platforms, fixing   many projects on such platforms.</li> <li>Support for <code>bfloat16</code>.</li> <li>Compatibility improvements with non-cmake buildsystems.</li> <li>Added <code>SCALE_CUDA_VERSION</code> environment variable to tell SCALE to impersonate a   specific version of CUDA.</li> <li><code>SCALE_EXCEPTIONS</code> now supports a non-fatal mode.</li> </ul>"},{"location":"manual/CHANGELOG/#library-wrappers","title":"Library wrappers","text":"<ul> <li>Added most of cuFFT.</li> <li>Added lots more cuSolver and cuSPARSE.</li> <li>Filled in some missing NVTX APIs.</li> <li>Added homeopathic quantities of nvtx3.</li> </ul>"},{"location":"manual/CHANGELOG/#library_1","title":"Library","text":"<ul> <li>Lazy-initialisation of primary contexts now works properly, fixing some   subtle lifecycle issues.</li> <li>Added some missing undocumented headers like <code>texture_types.h</code>.</li> <li>Added the IPC memory/event APIs</li> <li>Added many multi-GPU APIs</li> <li>Added <code>cuMemcpyPeer</code>/<code>cuMemcpyPeerAsync</code>.</li> <li>Rewritten device allocator to work around HSA bugs and performance issues.</li> <li>Fix a crash when initialising SCALE with many GPUs with huge amounts of memory.</li> <li>Added CUDA IPC APIs. Among other things, this enables CUDA-MPI   applications to work, including AMGX's distributed mode.</li> <li>Fixed lots of multi-GPU brokenness.</li> <li>Implemented the <code>CU_CTX_SYNC_MEMOPS</code> context flag.</li> <li>Fixed accuracy issues in some of the CUDA Math APIs.</li> <li>fp16 headers no longer produce warnings for projects that include them without   <code>-isystem</code>.</li> <li>Improved performance and correctness of cudaMemcpy/memset.</li> <li>Fix subtle issues with pointer attribute APIs.</li> <li>Improvements to C89 compatibility of headers.</li> <li>Added more Cooperative Groups APIs.</li> <li>Support for <code>grid_sync()</code>.</li> <li>Fix some wave64 issues with <code>cooperative_groups.h</code>.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler_1","title":"Compiler","text":"<ul> <li><code>__launch_bounds__</code> now works correctly, significantly improving performance.</li> <li>Device atomics are now much more efficient.</li> <li>Denorm-flushing optimisations are no longer skipped when they aren't   supposed to be.</li> <li>Ability to use DPP to optimise warp shuffles in some cases. Currently,   this only works if the individual shfl is provably equivalent to a DPP op,   not when loop analysis would be required. <code>__shfl_xor</code> is your friend.</li> </ul>"},{"location":"manual/CHANGELOG/#nvcc-interface","title":"NVCC Interface","text":"<ul> <li>Corrected the behaviour of the nvcc <code>-odir</code> flag in during dependency generation.</li> <li>Added the nvcc <code>-prec-sqrt</code> and <code>-prec-div</code> flags.</li> <li><code>use_fast_math</code> now matches nivida's behaviour, instead of mapping to clang's   <code>-ffast-math</code>, which does too much.</li> <li><code>--device-c</code> no longer inappropriately triggers the linker.</li> <li>Newly-supported <code>nvcc</code> flags:<ul> <li><code>-arch=native</code></li> <li><code>-jump-table-density</code> (ignored)</li> <li><code>-compress-mode</code> (ignored)</li> <li><code>-split-compile-extended</code> (ignored)</li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#nvcc-semantics","title":"NVCC Semantics","text":"<ul> <li>Support broken template code in more situations in nvcc mode.</li> <li>Allow invalid const-correctness in unexpanded template code in nvcc mode.</li> <li>Allow trailing commas in template argument lists in nvcc mode.</li> <li>Fix a parser crash when explicitly calling <code>operator&lt;&lt;&lt;int&gt;()</code> in CUDA mode.</li> <li>Fix a crash when using <code>--compiler-options</code> to pass huge numbers of   options through to <code>-Wl</code>.</li> </ul>"},{"location":"manual/CHANGELOG/#diagnostics","title":"Diagnostics","text":"<ul> <li>Warning for unused PTX variables</li> <li>Error for attempts to return the carry bit (undefined behaviour on NVIDIA).</li> <li>Compiler diagnostic to catch some undefined behaviour patterns with CUDA   atomics.</li> </ul>"},{"location":"manual/CHANGELOG/#ptx","title":"PTX","text":"<ul> <li>New instructions supported<ul> <li><code>sm_100</code> variants of <code>redux</code>.</li> <li>Mixed-precision <code>add/sub/fma</code> FP instructions.</li> <li><code>membar</code></li> <li><code>bar.warp.sync</code></li> <li><code>fence</code> (partial)</li> <li><code>mma</code> (software emulated)</li> <li><code>wmma</code> (software emulated)</li> </ul> </li> <li>Fixed parsing of hex-float constants.</li> <li>Support for PTX <code>C</code> constraints (dynamic asm strings).</li> <li>f16/bf16 PTX instructions no longer depend on the corresponding C++ header.</li> <li>asm blocks can now refer to variables declared in other asm blocks, including   absolutely cursed patterns.</li> <li>Fixed an issue where template-dependent asm strings were mishandled.</li> <li>Fixed various parsing issues (undocumented syntax quirks etc.).</li> <li>Fixed a crash when trying to XOR floating point numbers together.</li> </ul>"},{"location":"manual/CHANGELOG/#thirdparty-project-demos_1","title":"Thirdparty Project demos","text":"<p>Things that now appear to work include:</p> <ul> <li>CUDA-aware MPI</li> <li>MAGMA</li> <li>whisper.cpp</li> <li>TCLB</li> </ul>"},{"location":"manual/CHANGELOG/#release-120-2024-11-27","title":"Release 1.2.0 (2024-11-27)","text":""},{"location":"manual/CHANGELOG/#library-enhancements","title":"Library Enhancements","text":"<ul> <li>Support for <code>gfx900</code> architecture.</li> <li>Support for <code>gfx1102</code> architecture.</li> </ul>"},{"location":"manual/CHANGELOG/#ptx_1","title":"PTX","text":"<ul> <li>Improved handling of wave64 in inline PTX.</li> <li>Various inline PTX compilation fixes.</li> </ul>"},{"location":"manual/CHANGELOG/#other","title":"Other","text":"<ul> <li>Support for Ubuntu 24.04.</li> <li>Upgraded to ROCm 6.2.2.</li> </ul>"},{"location":"manual/CHANGELOG/#release-110-2024-10-31","title":"Release 1.1.0 (2024-10-31)","text":""},{"location":"manual/CHANGELOG/#library-enhancements_1","title":"Library Enhancements","text":"<ul> <li>Added much of the CUDA graph API.</li> <li>Improvements to multi-GPU handling.</li> <li>Fixed rare shutdown-time segfaults.</li> <li>Added many random API functions. As usual, see The diff.</li> </ul>"},{"location":"manual/CHANGELOG/#ptx_2","title":"PTX","text":"<ul> <li><code>f16x2</code>, <code>u16x2</code> and <code>s16x2</code> types.</li> <li><code>fns</code> instruction</li> <li>Fixed miscompile of <code>sad</code> instruction.</li> </ul>"},{"location":"manual/CHANGELOG/#thirdparty-project-demos_2","title":"Thirdparty Project demos","text":"<p>The <code>scale-validation</code> repo now has working demos for the following additional projects:</p> <ul> <li>FLAMEGPU2</li> <li>GPUJPEG</li> <li>gpu_jpeg2k</li> </ul>"},{"location":"manual/CHANGELOG/#release-1020-2024-09-05","title":"Release 1.0.2.0 (2024-09-05)","text":"<p>Documented a record of the CUDA APIs already available in SCALE, and those still to come: Implemented APIs.</p>"},{"location":"manual/CHANGELOG/#library-enhancements_2","title":"Library Enhancements","text":"<ul> <li>Kernel arguments larger than 4kb no longer crash the library.</li> <li>Programs that ignore CUDA error codes can no longer get stuck in a state   where the library always returns the error code you ignored.</li> <li>Fixed synchronisation bugs when using synchronous <code>cuMemset*</code> APIs.</li> <li>Fixed implicit synchronisation behaviour of <code>cuMemcpy2D/cuMemcpy2DAsync()</code>.</li> <li>Fixed precision issues in fp16 <code>exp2()</code>, <code>rsqrt()</code>, and <code>h2log()</code>.</li> <li><code>cudaEventRecord</code> for an empty event no longer returns a time in the past.</li> <li>Fixed occupancy API behaviour in edgecases that are not multiples of warp   size.</li> <li>Fixed rare crashes during static de-initialisation when library wrappers   were in use.</li> <li>All flags supported by SCALE's nvcc are now also accepted by our nvrtc   implementation.</li> <li>Various small header fixes.</li> </ul>"},{"location":"manual/CHANGELOG/#compiler-enhancements","title":"Compiler Enhancements","text":"<ul> <li><code>decltype()</code> now works correctly for <code>__host__ __device__</code> functions.</li> <li><code>-Winvalid-constexpr</code> no longer defaults to <code>-Werror</code>, for consistency   with nvcc.</li> <li>PTX variable names including <code>%</code> are no longer rejected.</li> <li>Support for nvcc's nonstandard permissiveness surrounding missing   <code>typename</code> keywords in dependent types.</li> <li>Support for nvcc's wacky \"split declaration\" syntax for <code>__host__ __device</code>   functions (with a warning):   <pre><code>int foo();\n__device__ int foo();\n__host__ int foo() {\n    return 5;\n}\n// foo() is a __host__ __device__ function. :D\n</code></pre></li> <li>Newly-supported compiler flags (all of which are aliases for   standard flags, or combinations thereof):<ul> <li><code>-device-c</code></li> <li><code>-device-w</code></li> <li><code>-pre-include</code></li> <li><code>-library</code></li> <li><code>-output-file</code></li> <li><code>-define-macro</code></li> <li><code>-undefine-macro</code></li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#new-cuda-apis","title":"New CUDA APIs","text":""},{"location":"manual/CHANGELOG/#math-apis","title":"Math APIs","text":"<ul> <li><code>exp10(__half)</code></li> <li><code>exp2(__half)</code></li> <li><code>rcp(__half)</code></li> <li><code>rint(__half)</code></li> <li><code>h2exp10(__half2)</code></li> <li><code>h2exp2(__half2)</code></li> <li><code>h2rcp(__half2)</code></li> <li><code>h2rint(__half2)</code></li> </ul>"},{"location":"manual/CHANGELOG/#release-1010-2024-07-24","title":"Release 1.0.1.0 (2024-07-24)","text":"<p>This release primarily fixes issues that prevent people from successfully compiling their projects with SCALE. Many thanks to those users who submitted bug reports.</p>"},{"location":"manual/CHANGELOG/#cuda-apis","title":"CUDA APIs","text":"<ul> <li>The <code>extra</code> argument to <code>cuLaunchKernel</code> is now supported.</li> <li>Added support for some more undocumented NVIDIA headers.</li> <li>Fix various overload resolution issues with atomic APIs.</li> <li>Fix overload resolution issues with min/max.</li> <li>Added various undocumented macros to support projects that are explicitly   checking cuda include guard macros.</li> <li><code>lrint()</code> and <code>llrint()</code> no longer crash the compiler. :D</li> <li>Newly supported CUDA APIs:<ul> <li><code>nvrtcGetNumSupportedArchs</code></li> <li><code>nvrtcGetSupportedArchs</code></li> <li><code>cudaLaunchKernelEx</code>, <code>cuLaunchKernelEx</code>, <code>cudaLaunchKernelExC</code>: some  of the performance-hint launch options are no-ops.</li> <li><code>__vavgs2</code>, <code>__vavgs4</code></li> <li>All the <code>atomic*_block()</code> and <code>atomic*_system()</code> variants.</li> </ul> </li> </ul>"},{"location":"manual/CHANGELOG/#compiler_2","title":"Compiler","text":"<ul> <li>Improved parsing of nvcc arguments:<ul> <li>Allow undocumented option variants (<code>-foo bar</code>, <code>--foo bar</code>,    <code>--foo=bar</code>, and <code>-foo=bar</code> are always allowed, it seems).</li> <li>Implement \"interesting\" quoting/escaping rules in nvcc arguments, such as    embedded quotes and <code>\\,</code>. We now correctly handle cursed arguments like:    <code>'-Xcompiler=-Wl\\,-O1' '-Xcompiler=-Wl\\,-rpath\\,/usr/lib,-Wl\\,-rpath-link\\,/usr/lib'</code></li> </ul> </li> <li> <p>Support for more nvcc arguments:</p> <ul> <li>NVCC-style diagnostic flags: <code>-Werror</code>, <code>-disable-warnings</code>, etc.</li> <li><code>--run</code>, <code>--run-args</code></li> <li><code>-Xlinker</code>, <code>-linker-options</code></li> <li><code>-no-exceptions</code>, <code>-noeh</code></li> <li><code>-minimal</code>: no-op. Exact semantics are undocumented, and build times   are reasonably fast anyway.</li> <li><code>-gen-opt-lto</code>, <code>-dlink-time-opt</code>, <code>-dlto</code>. No-ops: device LTO not yet   supported.</li> <li><code>-t</code>, <code>--threads</code>, <code>-split-compile</code>: No-ops: they're flags for making   compilation faster and are specific to how nvcc is implemented.</li> <li><code>-device-int128</code>: no-op: we always enable int128.</li> <li><code>-extra-device-vectorization</code>: no-op: vectorisation optimisations are   controlled by the usual <code>-O*</code> flags.</li> <li><code>-entries</code>, <code>-source-in-ptx</code>, <code>-src-in-ptx</code>: no-ops: there is no PTX.</li> <li><code>-use-local-env</code>, <code>-idp</code>, <code>-ddp</code>, <code>-dp</code>, etc.: ignored since they are   meaningless except on Windows.</li> </ul> </li> <li> <p>Allow variadic device functions in non-evaluated functions.</p> </li> <li>Don't warn about implicit conversion from <code>cudaLaneMask_t</code> to <code>bool</code>.</li> <li><code>__builtin_provable</code> no longer causes compiler crashes in <code>-O0</code>/<code>-O1</code> builds.</li> <li>Fixed a bug causing PTX <code>asm</code> blocks inside non-template, non-dependent   member functions of template classes to sometimes not be compiled,   causing PTX to end up in the AMD binary unmodified.</li> <li>CUDA launch tokens with spaces (ie.: <code>myKernel&lt;&lt; &lt;1, 1&gt;&gt; &gt;()</code>) are now   supported.</li> <li>Building non-cuda C translation units with SCALE-nvcc now works.</li> </ul>"},{"location":"manual/CHANGELOG/#other_1","title":"Other","text":"<ul> <li>The <code>meson</code> build system no longer regards SCALE-nvcc as a \"broken\" compiler.</li> <li><code>hsakmtsysinfo</code> no longer explodes if it doesn't like your GPU.</li> <li>New documentation pages.</li> <li>Published more details about thirdparty testing, including the build scripts.</li> </ul>"},{"location":"manual/CHANGELOG/#release-1000-2024-07-15","title":"Release 1.0.0.0 (2024-07-15)","text":"<p>Initial release</p>"},{"location":"manual/api-driver/","title":"Driver API","text":""},{"location":"manual/api-driver/#61-data-types-used-by-cuda-driver","title":"6.1. Data types used by CUDA driver","text":"<pre><code> #define CUDA_ARRAY3D_2DARRAY\n #define CUDA_ARRAY3D_COLOR_ATTACHMENT\n #define CUDA_ARRAY3D_CUBEMAP\n #define CUDA_ARRAY3D_DEFERRED_MAPPING\n #define CUDA_ARRAY3D_DEPTH_TEXTURE\n #define CUDA_ARRAY3D_LAYERED\n #define CUDA_ARRAY3D_SPARSE\n #define CUDA_ARRAY3D_SURFACE_LDST\n #define CUDA_ARRAY3D_TEXTURE_GATHER\n #define CUDA_ARRAY3D_VIDEO_ENCODE_DECODE\n-#define CUDA_COOPERATIVE_LAUNCH_MULTI_DEVICE_NO_POST_LAUNCH_SYNC\n-#define CUDA_COOPERATIVE_LAUNCH_MULTI_DEVICE_NO_PRE_LAUNCH_SYNC\n-#define CUDA_EGL_INFINITE_TIMEOUT\n-#define CUDA_EXTERNAL_MEMORY_DEDICATED\n-#define CUDA_EXTERNAL_SEMAPHORE_SIGNAL_SKIP_NVSCIBUF_MEMSYNC\n-#define CUDA_EXTERNAL_SEMAPHORE_WAIT_SKIP_NVSCIBUF_MEMSYNC\n-#define CUDA_NVSCISYNC_ATTR_SIGNAL\n-#define CUDA_NVSCISYNC_ATTR_WAIT\n #define CUDA_VERSION\n #define CU_ARRAY_SPARSE_PROPERTIES_SINGLE_MIPTAIL\n #define CU_DEVICE_CPU\n #define CU_DEVICE_INVALID\n-#define CU_GRAPH_COND_ASSIGN_DEFAULT\n #define CU_GRAPH_KERNEL_NODE_PORT_DEFAULT\n #define CU_GRAPH_KERNEL_NODE_PORT_LAUNCH_ORDER\n #define CU_GRAPH_KERNEL_NODE_PORT_PROGRAMMATIC\n-#define CU_IPC_HANDLE_SIZE\n-#define CU_LAUNCH_KERNEL_REQUIRED_BLOCK_DIM\n-#define CU_LAUNCH_PARAM_BUFFER_POINTER\n-#define CU_LAUNCH_PARAM_BUFFER_POINTER_AS_INT\n-#define CU_LAUNCH_PARAM_BUFFER_SIZE\n-#define CU_LAUNCH_PARAM_BUFFER_SIZE_AS_INT\n-#define CU_LAUNCH_PARAM_END\n-#define CU_LAUNCH_PARAM_END_AS_INT\n #define CU_MEMHOSTALLOC_DEVICEMAP\n #define CU_MEMHOSTALLOC_PORTABLE\n #define CU_MEMHOSTALLOC_WRITECOMBINED\n #define CU_MEMHOSTREGISTER_DEVICEMAP\n #define CU_MEMHOSTREGISTER_IOMEMORY\n #define CU_MEMHOSTREGISTER_PORTABLE\n #define CU_MEMHOSTREGISTER_READ_ONLY\n-#define CU_MEM_CREATE_USAGE_HW_DECOMPRESS\n-#define CU_MEM_CREATE_USAGE_TILE_POOL\n-#define CU_MEM_POOL_CREATE_USAGE_HW_DECOMPRESS\n-#define CU_PARAM_TR_DEFAULT\n #define CU_STREAM_LEGACY\n #define CU_STREAM_PER_THREAD\n-#define CU_TENSOR_MAP_NUM_QWORDS\n-#define CU_TRSA_OVERRIDE_FORMAT\n #define CU_TRSF_DISABLE_TRILINEAR_OPTIMIZATION\n #define CU_TRSF_NORMALIZED_COORDINATES\n #define CU_TRSF_READ_AS_INTEGER\n #define CU_TRSF_SEAMLESS_CUBEMAP\n #define CU_TRSF_SRGB\n-#define MAX_PLANES\n-enum CUDA_POINTER_ATTRIBUTE_ACCESS_FLAGS\n-enum CUGPUDirectRDMAWritesOrdering\n-enum CUaccessProperty\n enum CUaddress_mode\n-enum CUarraySparseSubresourceType\n-enum CUarray_cubemap_face\n enum CUarray_format\n-enum CUasyncNotificationType\n-enum CUatomicOperation\n-enum CUatomicOperationCapability\n-enum CUclusterSchedulingPolicy\n enum CUcomputemode\n enum CUctx_flags\n-enum CUdeviceNumaConfig\n-enum CUdevice_P2PAttribute\n-enum CUdevice_attribute\n enum CUdriverProcAddressQueryResult\n enum CUdriverProcAddress_flags\n-enum CUeglColorFormat\n-enum CUeglFrameType\n-enum CUeglResourceLocationFlags\n-enum CUevent_flags\n-enum CUevent_record_flags\n-enum CUevent_sched_flags\n-enum CUevent_wait_flags\n enum CUexecAffinityType\n-enum CUexternalMemoryHandleType\n-enum CUexternalSemaphoreHandleType\n enum CUfilter_mode\n-enum CUflushGPUDirectRDMAWritesOptions\n-enum CUflushGPUDirectRDMAWritesScope\n-enum CUflushGPUDirectRDMAWritesTarget\n enum CUfunc_cache\n enum CUfunction_attribute\n-enum CUgraphChildGraphNodeOwnership\n-enum CUgraphConditionalNodeType\n-enum CUgraphDebugDot_flags\n-enum CUgraphDependencyType\n enum CUgraphExecUpdateResult\n-enum CUgraphInstantiateResult\n-enum CUgraphInstantiate_flags\n enum CUgraphNodeType\n-enum CUgraphicsMapResourceFlags\n-enum CUgraphicsRegisterFlags\n-enum CUipcMem_flags\n enum CUjitInputType\n-enum CUjit_cacheMode\n-enum CUjit_fallback\n-enum CUjit_option\n-enum CUjit_target\n-enum CUlaunchAttributeID\n-enum CUlaunchMemSyncDomain\n-enum CUlibraryOption\n enum CUlimit\n enum CUmemAccess_flags\n enum CUmemAllocationCompType\n enum CUmemAllocationGranularity_flags\n enum CUmemAllocationHandleType\n-enum CUmemAllocationType\n-enum CUmemAttach_flags\n-enum CUmemHandleType\n-enum CUmemLocationType\n-enum CUmemOperationType\n-enum CUmemPool_attribute\n-enum CUmemRangeFlags\n-enum CUmemRangeHandleType\n enum CUmem_advise\n-enum CUmemcpy3DOperandType\n-enum CUmemcpyFlags\n-enum CUmemcpySrcAccessOrder\n enum CUmemorytype\n-enum CUmulticastGranularity_flags\n-enum CUoccupancy_flags\n enum CUpointer_attribute\n-enum CUprocessState\n enum CUresourceViewFormat\n enum CUresourcetype\n-enum CUresult\n-enum CUshared_carveout\n enum CUsharedconfig\n-enum CUstreamBatchMemOpType\n-enum CUstreamCaptureMode\n-enum CUstreamCaptureStatus\n-enum CUstreamMemoryBarrier_flags\n-enum CUstreamUpdateCaptureDependencies_flags\n-enum CUstreamWaitValue_flags\n-enum CUstreamWriteValue_flags\n-enum CUstream_flags\n enum CUtensorMapDataType\n enum CUtensorMapFloatOOBfill\n enum CUtensorMapIm2ColWideMode\n enum CUtensorMapInterleave\n enum CUtensorMapL2promotion\n enum CUtensorMapSwizzle\n-enum CUuserObjectRetain_flags\n-enum CUuserObject_flags\n-enum cl_context_flags\n-enum cl_event_flags\n-struct CUDA_ARRAY3D_DESCRIPTOR_v2\n-struct CUDA_ARRAY_DESCRIPTOR_v2\n-struct CUDA_ARRAY_MEMORY_REQUIREMENTS_v1\n-struct CUDA_ARRAY_SPARSE_PROPERTIES_v1\n-struct CUDA_BATCH_MEM_OP_NODE_PARAMS_v1\n-struct CUDA_BATCH_MEM_OP_NODE_PARAMS_v2\n-struct CUDA_CHILD_GRAPH_NODE_PARAMS\n-struct CUDA_CONDITIONAL_NODE_PARAMS\n-struct CUDA_EVENT_RECORD_NODE_PARAMS\n-struct CUDA_EVENT_WAIT_NODE_PARAMS\n-struct CUDA_EXTERNAL_MEMORY_BUFFER_DESC_v1\n-struct CUDA_EXTERNAL_MEMORY_HANDLE_DESC_v1\n-struct CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC_v1\n-struct CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC_v1\n-struct CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS_v1\n-struct CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS_v1\n-struct CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v1\n-struct CUDA_EXT_SEM_SIGNAL_NODE_PARAMS_v2\n-struct CUDA_EXT_SEM_WAIT_NODE_PARAMS_v1\n-struct CUDA_EXT_SEM_WAIT_NODE_PARAMS_v2\n-struct CUDA_GRAPH_INSTANTIATE_PARAMS\n struct CUDA_HOST_NODE_PARAMS_v1\n struct CUDA_HOST_NODE_PARAMS_v2\n struct CUDA_KERNEL_NODE_PARAMS_v1\n struct CUDA_KERNEL_NODE_PARAMS_v2\n-struct CUDA_KERNEL_NODE_PARAMS_v3\n-struct CUDA_LAUNCH_PARAMS_v1\n struct CUDA_MEMCPY2D_v2\n-struct CUDA_MEMCPY3D_PEER_v1\n struct CUDA_MEMCPY3D_v2\n-struct CUDA_MEMCPY_NODE_PARAMS\n-struct CUDA_MEMSET_NODE_PARAMS_v1\n-struct CUDA_MEMSET_NODE_PARAMS_v2\n-struct CUDA_MEM_ALLOC_NODE_PARAMS_v1\n-struct CUDA_MEM_ALLOC_NODE_PARAMS_v2\n-struct CUDA_MEM_FREE_NODE_PARAMS\n-struct CUDA_POINTER_ATTRIBUTE_P2P_TOKENS_v1\n struct CUDA_RESOURCE_DESC_v1\n struct CUDA_RESOURCE_VIEW_DESC_v1\n struct CUDA_TEXTURE_DESC_v1\n struct CUaccessPolicyWindow_v1\n-struct CUarrayMapInfo_v1\n-struct CUasyncNotificationInfo\n-struct CUcheckpointCheckpointArgs\n-struct CUcheckpointGpuPair\n-struct CUcheckpointLockArgs\n-struct CUcheckpointRestoreArgs\n-struct CUcheckpointUnlockArgs\n struct CUctxCigParam\n struct CUctxCreateParams\n struct CUdevprop_v1\n-struct CUeglFrame_v1\n struct CUexecAffinityParam_v1\n struct CUexecAffinitySmCount_v1\n-struct CUextent3D_v1\n struct CUgraphEdgeData\n struct CUgraphExecUpdateResultInfo_v1\n-struct CUgraphNodeParams\n struct CUipcEventHandle_v1\n struct CUipcMemHandle_v1\n struct CUlaunchAttribute\n union CUlaunchAttributeValue\n struct CUlaunchConfig\n-struct CUlaunchMemSyncDomainMap\n struct CUmemAccessDesc_v1\n struct CUmemAllocationProp_v1\n-struct CUmemFabricHandle_v1\n struct CUmemLocation_v1\n-struct CUmemPoolProps_v1\n-struct CUmemPoolPtrExportData_v1\n-struct CUmemcpy3DOperand_v1\n-struct CUmemcpyAttributes_v1\n-struct CUmulticastObjectProp_v1\n-struct CUoffset3D_v1\n-union CUstreamBatchMemOpParams_v1\n struct CUtensorMap\n typedef CUarray_st * CUarray;\n-typedef void (*CUasyncCallback)(CUasyncNotificationInfo* info, void* userData, CUasyncCallbackHandle callback);\n-typedef CUasyncCallbackEntry_st * CUasyncCallbackHandle;\n typedef CUctx_st * CUcontext;\n typedef CUdevice_v1 CUdevice;\n typedef int CUdevice_v1;\n typedef CUdeviceptr_v2 CUdeviceptr;\n typedef unsigned long long CUdeviceptr_v2;\n-typedef CUeglStreamConnection_st * CUeglStreamConnection;\n typedef CUevent_st * CUevent;\n-typedef CUextMemory_st * CUexternalMemory;\n-typedef CUextSemaphore_st * CUexternalSemaphore;\n typedef CUfunc_st * CUfunction;\n typedef CUgraph_st * CUgraph;\n-typedef cuuint64_t CUgraphConditionalHandle;\n typedef CUgraphDeviceUpdatableNode_st * CUgraphDeviceNode;\n typedef CUgraphExec_st * CUgraphExec;\n typedef CUgraphNode_st * CUgraphNode;\n-typedef CUgraphicsResource_st * CUgraphicsResource;\n typedef CUgreenCtx_st * CUgreenCtx;\n typedef void(* CUhostFn)(void* userData);\n-typedef CUkern_st * CUkernel;\n-typedef CUlib_st * CUlibrary;\n typedef CUmemPoolHandle_st * CUmemoryPool;\n-typedef CUmipmappedArray_st * CUmipmappedArray;\n typedef CUmod_st * CUmodule;\n typedef size_t(* CUoccupancyB2DSize)(int blockSize);\n typedef CUstream_st * CUstream;\n-typedef void(* CUstreamCallback)(CUstream hStream, CUresult status, void* userData);\n-typedef CUsurfObject_v1 CUsurfObject;\n-typedef unsigned long long CUsurfObject_v1;\n-typedef CUsurfref_st * CUsurfref;\n typedef CUtexObject_v1 CUtexObject;\n typedef unsigned long long CUtexObject_v1;\n typedef CUtexref_st * CUtexref;\n-typedef CUuserObject_st * CUuserObject;\n</code></pre>"},{"location":"manual/api-driver/#62-error-handling","title":"6.2. Error Handling","text":"<pre><code> CUresult cuGetErrorName(CUresult error, const char** pStr);\n CUresult cuGetErrorString(CUresult error, const char** pStr);\n</code></pre>"},{"location":"manual/api-driver/#63-initialization","title":"6.3. Initialization","text":"<pre><code> CUresult cuInit(unsigned int Flags);\n</code></pre>"},{"location":"manual/api-driver/#64-version-management","title":"6.4. Version Management","text":"<pre><code> CUresult cuDriverGetVersion(int* driverVersion);\n</code></pre>"},{"location":"manual/api-driver/#65-device-management","title":"6.5. Device Management","text":"<pre><code> CUresult cuDeviceGet(CUdevice* device, int ordinal);\n CUresult cuDeviceGetAttribute(int* pi, CUdevice_attribute attrib, CUdevice dev);\n CUresult cuDeviceGetCount(int* count);\n-CUresult cuDeviceGetDefaultMemPool(CUmemoryPool* pool_out, CUdevice dev);\n CUresult cuDeviceGetExecAffinitySupport(int* pi, CUexecAffinityType type, CUdevice dev);\n-CUresult cuDeviceGetHostAtomicCapabilities(unsigned int* capabilities, const CUatomicOperation ** operations, unsigned int count, CUdevice dev);\n CUresult cuDeviceGetLuid(char* luid, unsigned int* deviceNodeMask, CUdevice dev);\n-CUresult cuDeviceGetMemPool(CUmemoryPool* pool, CUdevice dev);\n CUresult cuDeviceGetName(char* name, int len, CUdevice dev);\n-CUresult cuDeviceGetNvSciSyncAttributes(void* nvSciSyncAttrList, CUdevice dev, int flags);\n-CUresult cuDeviceGetTexture1DLinearMaxWidth(size_t* maxWidthInElements, CUarray_format format, unsigned numChannels, CUdevice dev);\n CUresult cuDeviceGetUuid(CUuuid* uuid, CUdevice dev);\n-CUresult cuDeviceSetMemPool(CUdevice dev, CUmemoryPool pool);\n CUresult cuDeviceTotalMem(size_t* bytes, CUdevice dev);\n-CUresult cuFlushGPUDirectRDMAWrites(CUflushGPUDirectRDMAWritesTarget target, CUflushGPUDirectRDMAWritesScope scope);\n</code></pre>"},{"location":"manual/api-driver/#66-device-management-deprecated","title":"6.6. Device Management [DEPRECATED]","text":"<pre><code> CUresult cuDeviceComputeCapability(int* major, int* minor, CUdevice dev);\n CUresult cuDeviceGetProperties(CUdevprop* prop, CUdevice dev);\n</code></pre>"},{"location":"manual/api-driver/#67-primary-context-management","title":"6.7. Primary Context Management","text":"<pre><code> CUresult cuDevicePrimaryCtxGetState(CUdevice dev, unsigned int* flags, int* active);\n CUresult cuDevicePrimaryCtxRelease(CUdevice dev);\n CUresult cuDevicePrimaryCtxReset(CUdevice dev);\n CUresult cuDevicePrimaryCtxRetain(CUcontext* pctx, CUdevice dev);\n CUresult cuDevicePrimaryCtxSetFlags(CUdevice dev, unsigned int flags);\n</code></pre>"},{"location":"manual/api-driver/#68-context-management","title":"6.8. Context Management","text":"<pre><code> CUresult cuCtxCreate(CUcontext* pctx, unsigned int flags, CUdevice dev);\n CUresult cuCtxCreate_v3(CUcontext* pctx, CUexecAffinityParam* paramsArray, int numParams, unsigned int flags, CUdevice dev);\n CUresult cuCtxCreate_v4(CUcontext* pctx, CUctxCreateParams* ctxCreateParams, unsigned int flags, CUdevice dev);\n CUresult cuCtxDestroy(CUcontext ctx);\n CUresult cuCtxGetApiVersion(CUcontext ctx, unsigned int* version);\n CUresult cuCtxGetCacheConfig(CUfunc_cache* pconfig);\n CUresult cuCtxGetCurrent(CUcontext* pctx);\n CUresult cuCtxGetDevice(CUdevice* device);\n-CUresult cuCtxGetDevice_v2(CUdevice* device, CUcontext ctx);\n CUresult cuCtxGetExecAffinity(CUexecAffinityParam* pExecAffinity, CUexecAffinityType type);\n CUresult cuCtxGetFlags(unsigned int* flags);\n CUresult cuCtxGetId(CUcontext ctx, unsigned long long* ctxId);\n CUresult cuCtxGetLimit(size_t* pvalue, CUlimit limit);\n CUresult cuCtxGetStreamPriorityRange(int* leastPriority, int* greatestPriority);\n CUresult cuCtxPopCurrent(CUcontext* pctx);\n CUresult cuCtxPushCurrent(CUcontext ctx);\n-CUresult cuCtxRecordEvent(CUcontext hCtx, CUevent hEvent);\n CUresult cuCtxResetPersistingL2Cache(void);\n CUresult cuCtxSetCacheConfig(CUfunc_cache config);\n CUresult cuCtxSetCurrent(CUcontext ctx);\n CUresult cuCtxSetFlags(unsigned int flags);\n CUresult cuCtxSetLimit(CUlimit limit, size_t value);\n CUresult cuCtxSynchronize(void);\n-CUresult cuCtxSynchronize_v2(CUcontext ctx);\n-CUresult cuCtxWaitEvent(CUcontext hCtx, CUevent hEvent);\n</code></pre>"},{"location":"manual/api-driver/#69-context-management-deprecated","title":"6.9. Context Management [DEPRECATED]","text":"<pre><code> CUresult cuCtxAttach(CUcontext* pctx, unsigned int flags);\n CUresult cuCtxDetach(CUcontext ctx);\n CUresult cuCtxGetSharedMemConfig(CUsharedconfig* pConfig);\n CUresult cuCtxSetSharedMemConfig(CUsharedconfig config);\n</code></pre>"},{"location":"manual/api-driver/#610-module-management","title":"6.10. Module Management","text":"<pre><code> enum CUmoduleLoadingMode\n CUresult cuLinkAddData(CUlinkState state, CUjitInputType type, void* data, size_t size, const char* name, unsigned int numOptions, CUjit_option* options, void** optionValues);\n CUresult cuLinkAddFile(CUlinkState state, CUjitInputType type, const char* path, unsigned int numOptions, CUjit_option* options, void** optionValues);\n CUresult cuLinkComplete(CUlinkState state, void** cubinOut, size_t* sizeOut);\n CUresult cuLinkCreate(unsigned int numOptions, CUjit_option* options, void** optionValues, CUlinkState* stateOut);\n CUresult cuLinkDestroy(CUlinkState state);\n-CUresult cuModuleEnumerateFunctions(CUfunction* functions, unsigned int numFunctions, CUmodule mod);\n CUresult cuModuleGetFunction(CUfunction* hfunc, CUmodule hmod, const char* name);\n-CUresult cuModuleGetFunctionCount(unsigned int* count, CUmodule mod);\n CUresult cuModuleGetGlobal(CUdeviceptr* dptr, size_t* bytes, CUmodule hmod, const char* name);\n CUresult cuModuleGetLoadingMode(CUmoduleLoadingMode* mode);\n CUresult cuModuleLoad(CUmodule* module, const char* fname);\n CUresult cuModuleLoadData(CUmodule* module, const void* image);\n CUresult cuModuleLoadDataEx(CUmodule* module, const void* image, unsigned int numOptions, CUjit_option* options, void** optionValues);\n CUresult cuModuleLoadFatBinary(CUmodule* module, const void* fatCubin);\n CUresult cuModuleUnload(CUmodule hmod);\n</code></pre>"},{"location":"manual/api-driver/#611-module-management-deprecated","title":"6.11. Module Management [DEPRECATED]","text":"<pre><code>-CUresult cuModuleGetSurfRef(CUsurfref* pSurfRef, CUmodule hmod, const char* name);\n-CUresult cuModuleGetTexRef(CUtexref* pTexRef, CUmodule hmod, const char* name);\n</code></pre>"},{"location":"manual/api-driver/#612-library-management","title":"6.12. Library Management","text":"<pre><code>-CUresult cuKernelGetAttribute(int* pi, CUfunction_attribute attrib, CUkernel kernel, CUdevice dev);\n-CUresult cuKernelGetFunction(CUfunction* pFunc, CUkernel kernel);\n-CUresult cuKernelGetLibrary(CUlibrary* pLib, CUkernel kernel);\n-CUresult cuKernelGetName(const char** name, CUkernel hfunc);\n-CUresult cuKernelGetParamInfo(CUkernel kernel, size_t paramIndex, size_t* paramOffset, size_t* paramSize);\n-CUresult cuKernelSetAttribute(CUfunction_attribute attrib, int val, CUkernel kernel, CUdevice dev);\n-CUresult cuKernelSetCacheConfig(CUkernel kernel, CUfunc_cache config, CUdevice dev);\n-CUresult cuLibraryEnumerateKernels(CUkernel* kernels, unsigned int numKernels, CUlibrary lib);\n-CUresult cuLibraryGetGlobal(CUdeviceptr* dptr, size_t* bytes, CUlibrary library, const char* name);\n-CUresult cuLibraryGetKernel(CUkernel* pKernel, CUlibrary library, const char* name);\n-CUresult cuLibraryGetKernelCount(unsigned int* count, CUlibrary lib);\n-CUresult cuLibraryGetManaged(CUdeviceptr* dptr, size_t* bytes, CUlibrary library, const char* name);\n-CUresult cuLibraryGetModule(CUmodule* pMod, CUlibrary library);\n-CUresult cuLibraryGetUnifiedFunction(void** fptr, CUlibrary library, const char* symbol);\n-CUresult cuLibraryLoadData(CUlibrary* library, const void* code, CUjit_option* jitOptions, void** jitOptionsValues, unsigned int numJitOptions, CUlibraryOption* libraryOptions, void** libraryOptionValues, unsigned int numLibraryOptions);\n-CUresult cuLibraryLoadFromFile(CUlibrary* library, const char* fileName, CUjit_option* jitOptions, void** jitOptionsValues, unsigned int numJitOptions, CUlibraryOption* libraryOptions, void** libraryOptionValues, unsigned int numLibraryOptions);\n-CUresult cuLibraryUnload(CUlibrary library);\n</code></pre>"},{"location":"manual/api-driver/#613-memory-management","title":"6.13. Memory Management","text":"<pre><code>-enum CUmemDecompressAlgorithm\n-struct CUmemDecompressParams\n-CUresult cuArray3DCreate(CUarray* pHandle, const CUDA_ARRAY3D_DESCRIPTOR* pAllocateArray);\n-CUresult cuArray3DGetDescriptor(CUDA_ARRAY3D_DESCRIPTOR* pArrayDescriptor, CUarray hArray);\n-CUresult cuArrayCreate(CUarray* pHandle, const CUDA_ARRAY_DESCRIPTOR* pAllocateArray);\n CUresult cuArrayDestroy(CUarray hArray);\n-CUresult cuArrayGetDescriptor(CUDA_ARRAY_DESCRIPTOR* pArrayDescriptor, CUarray hArray);\n-CUresult cuArrayGetMemoryRequirements(CUDA_ARRAY_MEMORY_REQUIREMENTS* memoryRequirements, CUarray array, CUdevice device);\n-CUresult cuArrayGetPlane(CUarray* pPlaneArray, CUarray hArray, unsigned int planeIdx);\n-CUresult cuArrayGetSparseProperties(CUDA_ARRAY_SPARSE_PROPERTIES* sparseProperties, CUarray array);\n CUresult cuDeviceGetByPCIBusId(CUdevice* dev, const char* pciBusId);\n CUresult cuDeviceGetPCIBusId(char* pciBusId, int len, CUdevice dev);\n-CUresult cuDeviceRegisterAsyncNotification(CUdevice device, CUasyncCallback callbackFunc, void* userData, CUasyncCallbackHandle* callback);\n-CUresult cuDeviceUnregisterAsyncNotification(CUdevice device, CUasyncCallbackHandle callback);\n CUresult cuIpcCloseMemHandle(CUdeviceptr dptr);\n CUresult cuIpcGetEventHandle(CUipcEventHandle* pHandle, CUevent event);\n CUresult cuIpcGetMemHandle(CUipcMemHandle* pHandle, CUdeviceptr dptr);\n CUresult cuIpcOpenEventHandle(CUevent* phEvent, CUipcEventHandle handle);\n CUresult cuIpcOpenMemHandle(CUdeviceptr* pdptr, CUipcMemHandle handle, unsigned int Flags);\n CUresult cuMemAlloc(CUdeviceptr* dptr, size_t bytesize);\n CUresult cuMemAllocHost(void** pp, size_t bytesize);\n CUresult cuMemAllocManaged(CUdeviceptr* dptr, size_t bytesize, unsigned int flags);\n CUresult cuMemAllocPitch(CUdeviceptr* dptr, size_t* pPitch, size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes);\n-CUresult cuMemBatchDecompressAsync(CUmemDecompressParams* paramsArray, size_t count, unsigned int flags, size_t* errorIndex, CUstream stream);\n CUresult cuMemFree(CUdeviceptr dptr);\n CUresult cuMemFreeHost(void* p);\n CUresult cuMemGetAddressRange(CUdeviceptr* pbase, size_t* psize, CUdeviceptr dptr);\n-CUresult cuMemGetHandleForAddressRange(void* handle, CUdeviceptr dptr, size_t size, CUmemRangeHandleType handleType, unsigned long long flags);\n CUresult cuMemGetInfo(size_t* free, size_t* total);\n CUresult cuMemHostAlloc(void** pp, size_t bytesize, unsigned int Flags);\n CUresult cuMemHostGetDevicePointer(CUdeviceptr* pdptr, void* p, unsigned int Flags);\n CUresult cuMemHostGetFlags(unsigned int* pFlags, void* p);\n CUresult cuMemHostRegister(void* p, size_t bytesize, unsigned int Flags);\n CUresult cuMemHostUnregister(void* p);\n CUresult cuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);\n CUresult cuMemcpy2D(const CUDA_MEMCPY2D* pCopy);\n CUresult cuMemcpy2DAsync(const CUDA_MEMCPY2D* pCopy, CUstream hStream);\n CUresult cuMemcpy2DUnaligned(const CUDA_MEMCPY2D* pCopy);\n-CUresult cuMemcpy3D(const CUDA_MEMCPY3D* pCopy);\n-CUresult cuMemcpy3DAsync(const CUDA_MEMCPY3D* pCopy, CUstream hStream);\n-CUresult cuMemcpy3DBatchAsync(size_t numOps, CUDA_MEMCPY3D_BATCH_OP* opList, unsigned long long flags, CUstream hStream);\n-CUresult cuMemcpy3DPeer(const CUDA_MEMCPY3D_PEER* pCopy);\n-CUresult cuMemcpy3DPeerAsync(const CUDA_MEMCPY3D_PEER* pCopy, CUstream hStream);\n CUresult cuMemcpyAsync(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount, CUstream hStream);\n CUresult cuMemcpyAtoA(CUarray dstArray, size_t dstOffset, CUarray srcArray, size_t srcOffset, size_t ByteCount);\n CUresult cuMemcpyAtoD(CUdeviceptr dstDevice, CUarray srcArray, size_t srcOffset, size_t ByteCount);\n CUresult cuMemcpyAtoH(void* dstHost, CUarray srcArray, size_t srcOffset, size_t ByteCount);\n CUresult cuMemcpyAtoHAsync(void* dstHost, CUarray srcArray, size_t srcOffset, size_t ByteCount, CUstream hStream);\n-CUresult cuMemcpyBatchAsync(CUdeviceptr* dsts, CUdeviceptr* srcs, size_t* sizes, size_t count, CUmemcpyAttributes* attrs, size_t* attrsIdxs, size_t numAttrs, CUstream hStream);\n CUresult cuMemcpyDtoA(CUarray dstArray, size_t dstOffset, CUdeviceptr srcDevice, size_t ByteCount);\n CUresult cuMemcpyDtoD(CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount);\n CUresult cuMemcpyDtoDAsync(CUdeviceptr dstDevice, CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream);\n CUresult cuMemcpyDtoH(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount);\n CUresult cuMemcpyDtoHAsync(void* dstHost, CUdeviceptr srcDevice, size_t ByteCount, CUstream hStream);\n CUresult cuMemcpyHtoA(CUarray dstArray, size_t dstOffset, const void* srcHost, size_t ByteCount);\n CUresult cuMemcpyHtoAAsync(CUarray dstArray, size_t dstOffset, const void* srcHost, size_t ByteCount, CUstream hStream);\n CUresult cuMemcpyHtoD(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount);\n CUresult cuMemcpyHtoDAsync(CUdeviceptr dstDevice, const void* srcHost, size_t ByteCount, CUstream hStream);\n CUresult cuMemcpyPeer(CUdeviceptr dstDevice, CUcontext dstContext, CUdeviceptr srcDevice, CUcontext srcContext, size_t ByteCount);\n CUresult cuMemcpyPeerAsync(CUdeviceptr dstDevice, CUcontext dstContext, CUdeviceptr srcDevice, CUcontext srcContext, size_t ByteCount, CUstream hStream);\n CUresult cuMemsetD16(CUdeviceptr dstDevice, unsigned short us, size_t N);\n CUresult cuMemsetD16Async(CUdeviceptr dstDevice, unsigned short us, size_t N, CUstream hStream);\n CUresult cuMemsetD2D16(CUdeviceptr dstDevice, size_t dstPitch, unsigned short us, size_t Width, size_t Height);\n CUresult cuMemsetD2D16Async(CUdeviceptr dstDevice, size_t dstPitch, unsigned short us, size_t Width, size_t Height, CUstream hStream);\n CUresult cuMemsetD2D32(CUdeviceptr dstDevice, size_t dstPitch, unsigned int ui, size_t Width, size_t Height);\n CUresult cuMemsetD2D32Async(CUdeviceptr dstDevice, size_t dstPitch, unsigned int ui, size_t Width, size_t Height, CUstream hStream);\n CUresult cuMemsetD2D8(CUdeviceptr dstDevice, size_t dstPitch, unsigned char uc, size_t Width, size_t Height);\n CUresult cuMemsetD2D8Async(CUdeviceptr dstDevice, size_t dstPitch, unsigned char uc, size_t Width, size_t Height, CUstream hStream);\n CUresult cuMemsetD32(CUdeviceptr dstDevice, unsigned int ui, size_t N);\n CUresult cuMemsetD32Async(CUdeviceptr dstDevice, unsigned int ui, size_t N, CUstream hStream);\n CUresult cuMemsetD8(CUdeviceptr dstDevice, unsigned char uc, size_t N);\n CUresult cuMemsetD8Async(CUdeviceptr dstDevice, unsigned char uc, size_t N, CUstream hStream);\n-CUresult cuMipmappedArrayCreate(CUmipmappedArray* pHandle, const CUDA_ARRAY3D_DESCRIPTOR* pMipmappedArrayDesc, unsigned int numMipmapLevels);\n-CUresult cuMipmappedArrayDestroy(CUmipmappedArray hMipmappedArray);\n-CUresult cuMipmappedArrayGetLevel(CUarray* pLevelArray, CUmipmappedArray hMipmappedArray, unsigned int level);\n-CUresult cuMipmappedArrayGetMemoryRequirements(CUDA_ARRAY_MEMORY_REQUIREMENTS* memoryRequirements, CUmipmappedArray mipmap, CUdevice device);\n-CUresult cuMipmappedArrayGetSparseProperties(CUDA_ARRAY_SPARSE_PROPERTIES* sparseProperties, CUmipmappedArray mipmap);\n</code></pre>"},{"location":"manual/api-driver/#614-virtual-memory-management","title":"6.14. Virtual Memory Management","text":"<pre><code> CUresult cuMemAddressFree(CUdeviceptr ptr, size_t size);\n CUresult cuMemAddressReserve(CUdeviceptr* ptr, size_t size, size_t alignment, CUdeviceptr addr, unsigned long long flags);\n CUresult cuMemCreate(CUmemGenericAllocationHandle* handle, size_t size, const CUmemAllocationProp* prop, unsigned long long flags);\n-CUresult cuMemExportToShareableHandle(void* shareableHandle, CUmemGenericAllocationHandle handle, CUmemAllocationHandleType handleType, unsigned long long flags);\n CUresult cuMemGetAccess(unsigned long long* flags, const CUmemLocation* location, CUdeviceptr ptr);\n CUresult cuMemGetAllocationGranularity(size_t* granularity, const CUmemAllocationProp* prop, CUmemAllocationGranularity_flags option);\n CUresult cuMemGetAllocationPropertiesFromHandle(CUmemAllocationProp* prop, CUmemGenericAllocationHandle handle);\n-CUresult cuMemImportFromShareableHandle(CUmemGenericAllocationHandle* handle, void* osHandle, CUmemAllocationHandleType shHandleType);\n CUresult cuMemMap(CUdeviceptr ptr, size_t size, size_t offset, CUmemGenericAllocationHandle handle, unsigned long long flags);\n-CUresult cuMemMapArrayAsync(CUarrayMapInfo* mapInfoList, unsigned int count, CUstream hStream);\n CUresult cuMemRelease(CUmemGenericAllocationHandle handle);\n CUresult cuMemRetainAllocationHandle(CUmemGenericAllocationHandle* handle, void* addr);\n CUresult cuMemSetAccess(CUdeviceptr ptr, size_t size, const CUmemAccessDesc* desc, size_t count);\n CUresult cuMemUnmap(CUdeviceptr ptr, size_t size);\n</code></pre>"},{"location":"manual/api-driver/#615-stream-ordered-memory-allocator","title":"6.15. Stream Ordered Memory Allocator","text":"<pre><code> CUresult cuMemAllocAsync(CUdeviceptr* dptr, size_t bytesize, CUstream hStream);\n-CUresult cuMemAllocFromPoolAsync(CUdeviceptr* dptr, size_t bytesize, CUmemoryPool pool, CUstream hStream);\n CUresult cuMemFreeAsync(CUdeviceptr dptr, CUstream hStream);\n-CUresult cuMemGetDefaultMemPool(CUmemoryPool* pool_out, CUmemLocation* location, CUmemAllocationType type);\n-CUresult cuMemGetMemPool(CUmemoryPool* pool, CUmemLocation* location, CUmemAllocationType type);\n-CUresult cuMemPoolCreate(CUmemoryPool* pool, const CUmemPoolProps* poolProps);\n-CUresult cuMemPoolDestroy(CUmemoryPool pool);\n-CUresult cuMemPoolExportPointer(CUmemPoolPtrExportData* shareData_out, CUdeviceptr ptr);\n-CUresult cuMemPoolExportToShareableHandle(void* handle_out, CUmemoryPool pool, CUmemAllocationHandleType handleType, unsigned long long flags);\n CUresult cuMemPoolGetAccess(CUmemAccess_flags* flags, CUmemoryPool memPool, CUmemLocation* location);\n-CUresult cuMemPoolGetAttribute(CUmemoryPool pool, CUmemPool_attribute attr, void* value);\n-CUresult cuMemPoolImportFromShareableHandle(CUmemoryPool* pool_out, void* handle, CUmemAllocationHandleType handleType, unsigned long long flags);\n-CUresult cuMemPoolImportPointer(CUdeviceptr* ptr_out, CUmemoryPool pool, CUmemPoolPtrExportData* shareData);\n-CUresult cuMemPoolSetAccess(CUmemoryPool pool, const CUmemAccessDesc* map, size_t count);\n-CUresult cuMemPoolSetAttribute(CUmemoryPool pool, CUmemPool_attribute attr, void* value);\n-CUresult cuMemPoolTrimTo(CUmemoryPool pool, size_t minBytesToKeep);\n-CUresult cuMemSetMemPool(CUmemLocation* location, CUmemAllocationType type, CUmemoryPool pool);\n</code></pre>"},{"location":"manual/api-driver/#616-multicast-object-management","title":"6.16. Multicast Object Management","text":"<pre><code>-CUresult cuMulticastAddDevice(CUmemGenericAllocationHandle mcHandle, CUdevice dev);\n-CUresult cuMulticastBindAddr(CUmemGenericAllocationHandle mcHandle, size_t mcOffset, CUdeviceptr memptr, size_t size, unsigned long long flags);\n-CUresult cuMulticastBindMem(CUmemGenericAllocationHandle mcHandle, size_t mcOffset, CUmemGenericAllocationHandle memHandle, size_t memOffset, size_t size, unsigned long long flags);\n-CUresult cuMulticastCreate(CUmemGenericAllocationHandle* mcHandle, const CUmulticastObjectProp* prop);\n-CUresult cuMulticastGetGranularity(size_t* granularity, const CUmulticastObjectProp* prop, CUmulticastGranularity_flags option);\n-CUresult cuMulticastUnbind(CUmemGenericAllocationHandle mcHandle, CUdevice dev, size_t mcOffset, size_t size);\n</code></pre>"},{"location":"manual/api-driver/#617-unified-addressing","title":"6.17. Unified Addressing","text":"<pre><code> CUresult cuMemAdvise(CUdeviceptr devPtr, size_t count, CUmem_advise advice, CUdevice device);\n CUresult cuMemAdvise_v2(CUdeviceptr devPtr, size_t count, CUmem_advise advice, CUmemLocation location);\n CUresult cuMemPrefetchAsync(CUdeviceptr devPtr, size_t count, CUdevice dstDevice, CUstream hStream);\n CUresult cuMemPrefetchAsync_v2(CUdeviceptr devPtr, size_t count, CUmemLocation location, unsigned int flags, CUstream hStream);\n-CUresult cuMemDiscardAndPrefetchBatchAsync(CUdeviceptr* dptrs, size_t* sizes, size_t count, CUmemLocation* prefetchLocs, size_t* prefetchLocIdxs, size_t numPrefetchLocs, unsigned long long flags, CUstream hStream);\n-CUresult cuMemDiscardBatchAsync(CUdeviceptr* dptrs, size_t* sizes, size_t count, unsigned long long flags, CUstream hStream);\n-CUresult cuMemPrefetchBatchAsync(CUdeviceptr* dptrs, size_t* sizes, size_t count, CUmemLocation* prefetchLocs, size_t* prefetchLocIdxs, size_t numPrefetchLocs, unsigned long long flags, CUstream hStream);\n-CUresult cuMemRangeGetAttribute(void* data, size_t dataSize, CUmem_range_attribute attribute, CUdeviceptr devPtr, size_t count);\n-CUresult cuMemRangeGetAttributes(void** data, size_t* dataSizes, CUmem_range_attribute* attributes, size_t numAttributes, CUdeviceptr devPtr, size_t count);\n CUresult cuPointerGetAttribute(void* data, CUpointer_attribute attribute, CUdeviceptr ptr);\n CUresult cuPointerGetAttributes(unsigned int numAttributes, CUpointer_attribute* attributes, void** data, CUdeviceptr ptr);\n CUresult cuPointerSetAttribute(const void* value, CUpointer_attribute attribute, CUdeviceptr ptr);\n</code></pre>"},{"location":"manual/api-driver/#618-stream-management","title":"6.18. Stream Management","text":"<pre><code>-CUresult cuStreamAddCallback(CUstream hStream, CUstreamCallback callback, void* userData, unsigned int flags);\n-CUresult cuStreamAttachMemAsync(CUstream hStream, CUdeviceptr dptr, size_t length, unsigned int flags);\n-CUresult cuStreamBeginCapture(CUstream hStream, CUstreamCaptureMode mode);\n-CUresult cuStreamBeginCaptureToGraph(CUstream hStream, CUgraph hGraph, const CUgraphNode* dependencies, const CUgraphEdgeData* dependencyData, size_t numDependencies, CUstreamCaptureMode mode);\n-CUresult cuStreamCopyAttributes(CUstream dst, CUstream src);\n CUresult cuStreamCreate(CUstream* phStream, unsigned int Flags);\n CUresult cuStreamCreateWithPriority(CUstream* phStream, unsigned int flags, int priority);\n CUresult cuStreamDestroy(CUstream hStream);\n-CUresult cuStreamEndCapture(CUstream hStream, CUgraph* phGraph);\n-CUresult cuStreamGetAttribute(CUstream hStream, CUstreamAttrID attr, CUstreamAttrValue* value_out);\n-CUresult cuStreamGetCaptureInfo(CUstream hStream, CUstreamCaptureStatus* captureStatus_out, cuuint64_t* id_out, CUgraph* graph_out, const CUgraphNode** dependencies_out, const CUgraphEdgeData** edgeData_out, size_t* numDependencies_out);\n CUresult cuStreamGetCtx(CUstream hStream, CUcontext* pctx);\n CUresult cuStreamGetCtx_v2(CUstream hStream, CUcontext* pCtx, CUgreenCtx* pGreenCtx);\n CUresult cuStreamGetDevice(CUstream hStream, CUdevice* device);\n CUresult cuStreamGetFlags(CUstream hStream, unsigned int* flags);\n CUresult cuStreamGetId(CUstream hStream, unsigned long long* streamId);\n CUresult cuStreamGetPriority(CUstream hStream, int* priority);\n-CUresult cuStreamIsCapturing(CUstream hStream, CUstreamCaptureStatus* captureStatus);\n CUresult cuStreamQuery(CUstream hStream);\n-CUresult cuStreamSetAttribute(CUstream hStream, CUstreamAttrID attr, const CUstreamAttrValue* value);\n CUresult cuStreamSynchronize(CUstream hStream);\n-CUresult cuStreamUpdateCaptureDependencies(CUstream hStream, CUgraphNode* dependencies, const CUgraphEdgeData* dependencyData, size_t numDependencies, unsigned int flags);\n CUresult cuStreamWaitEvent(CUstream hStream, CUevent hEvent, unsigned int Flags);\n-CUresult cuThreadExchangeStreamCaptureMode(CUstreamCaptureMode* mode);\n</code></pre>"},{"location":"manual/api-driver/#619-event-management","title":"6.19. Event Management","text":"<pre><code> CUresult cuEventCreate(CUevent* phEvent, unsigned int Flags);\n CUresult cuEventDestroy(CUevent hEvent);\n CUresult cuEventElapsedTime(float* pMilliseconds, CUevent hStart, CUevent hEnd);\n-CUresult cuEventElapsedTime_v2(float* pMilliseconds, CUevent hStart, CUevent hEnd);\n CUresult cuEventQuery(CUevent hEvent);\n CUresult cuEventRecord(CUevent hEvent, CUstream hStream);\n CUresult cuEventRecordWithFlags(CUevent hEvent, CUstream hStream, unsigned int flags);\n CUresult cuEventSynchronize(CUevent hEvent);\n</code></pre>"},{"location":"manual/api-driver/#620-external-resource-interoperability","title":"6.20. External Resource Interoperability","text":"<pre><code>-CUresult cuDestroyExternalMemory(CUexternalMemory extMem);\n-CUresult cuDestroyExternalSemaphore(CUexternalSemaphore extSem);\n-CUresult cuExternalMemoryGetMappedBuffer(CUdeviceptr* devPtr, CUexternalMemory extMem, const CUDA_EXTERNAL_MEMORY_BUFFER_DESC* bufferDesc);\n-CUresult cuExternalMemoryGetMappedMipmappedArray(CUmipmappedArray* mipmap, CUexternalMemory extMem, const CUDA_EXTERNAL_MEMORY_MIPMAPPED_ARRAY_DESC* mipmapDesc);\n-CUresult cuImportExternalMemory(CUexternalMemory* extMem_out, const CUDA_EXTERNAL_MEMORY_HANDLE_DESC* memHandleDesc);\n-CUresult cuImportExternalSemaphore(CUexternalSemaphore* extSem_out, const CUDA_EXTERNAL_SEMAPHORE_HANDLE_DESC* semHandleDesc);\n-CUresult cuSignalExternalSemaphoresAsync(const CUexternalSemaphore* extSemArray, const CUDA_EXTERNAL_SEMAPHORE_SIGNAL_PARAMS* paramsArray, unsigned int numExtSems, CUstream stream);\n-CUresult cuWaitExternalSemaphoresAsync(const CUexternalSemaphore* extSemArray, const CUDA_EXTERNAL_SEMAPHORE_WAIT_PARAMS* paramsArray, unsigned int numExtSems, CUstream stream);\n</code></pre>"},{"location":"manual/api-driver/#621-stream-memory-operations","title":"6.21. Stream Memory Operations","text":"<pre><code>-CUresult cuStreamBatchMemOp(CUstream stream, unsigned int count, CUstreamBatchMemOpParams* paramArray, unsigned int flags);\n-CUresult cuStreamWaitValue32(CUstream stream, CUdeviceptr addr, cuuint32_t value, unsigned int flags);\n-CUresult cuStreamWaitValue64(CUstream stream, CUdeviceptr addr, cuuint64_t value, unsigned int flags);\n-CUresult cuStreamWriteValue32(CUstream stream, CUdeviceptr addr, cuuint32_t value, unsigned int flags);\n-CUresult cuStreamWriteValue64(CUstream stream, CUdeviceptr addr, cuuint64_t value, unsigned int flags);\n</code></pre>"},{"location":"manual/api-driver/#622-execution-control","title":"6.22. Execution Control","text":"<pre><code> CUresult cuFuncGetAttribute(int* pi, CUfunction_attribute attrib, CUfunction hfunc);\n-CUresult cuFuncGetModule(CUmodule* hmod, CUfunction hfunc);\n-CUresult cuFuncGetName(const char** name, CUfunction hfunc);\n-CUresult cuFuncGetParamInfo(CUfunction func, size_t paramIndex, size_t* paramOffset, size_t* paramSize);\n CUresult cuFuncIsLoaded(CUfunctionLoadingState* state, CUfunction function);\n CUresult cuFuncLoad(CUfunction function);\n CUresult cuFuncSetAttribute(CUfunction hfunc, CUfunction_attribute attrib, int value);\n CUresult cuFuncSetCacheConfig(CUfunction hfunc, CUfunc_cache config);\n CUresult cuLaunchCooperativeKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void** kernelParams);\n-CUresult cuLaunchCooperativeKernelMultiDevice(CUDA_LAUNCH_PARAMS* launchParamsList, unsigned int numDevices, unsigned int flags);\n CUresult cuLaunchHostFunc(CUstream hStream, CUhostFn fn, void* userData);\n CUresult cuLaunchKernel(CUfunction f, unsigned int gridDimX, unsigned int gridDimY, unsigned int gridDimZ, unsigned int blockDimX, unsigned int blockDimY, unsigned int blockDimZ, unsigned int sharedMemBytes, CUstream hStream, void** kernelParams, void** extra);\n CUresult cuLaunchKernelEx(const CUlaunchConfig* config, CUfunction f, void** kernelParams, void** extra);\n</code></pre>"},{"location":"manual/api-driver/#623-execution-control-deprecated","title":"6.23. Execution Control [DEPRECATED]","text":"<pre><code>-CUresult cuFuncSetBlockShape(CUfunction hfunc, int x, int y, int z);\n CUresult cuFuncSetSharedMemConfig(CUfunction hfunc, CUsharedconfig config);\n-CUresult cuFuncSetSharedSize(CUfunction hfunc, unsigned int bytes);\n-CUresult cuLaunch(CUfunction f);\n-CUresult cuLaunchGrid(CUfunction f, int grid_width, int grid_height);\n-CUresult cuLaunchGridAsync(CUfunction f, int grid_width, int grid_height, CUstream hStream);\n-CUresult cuParamSetSize(CUfunction hfunc, unsigned int numbytes);\n-CUresult cuParamSetTexRef(CUfunction hfunc, int texunit, CUtexref hTexRef);\n-CUresult cuParamSetf(CUfunction hfunc, int offset, float value);\n-CUresult cuParamSeti(CUfunction hfunc, int offset, unsigned int value);\n-CUresult cuParamSetv(CUfunction hfunc, int offset, void* ptr, unsigned int numbytes);\n</code></pre>"},{"location":"manual/api-driver/#624-graph-management","title":"6.24. Graph Management","text":"<pre><code>-CUresult cuDeviceGetGraphMemAttribute(CUdevice device, CUgraphMem_attribute attr, void* value);\n-CUresult cuDeviceGraphMemTrim(CUdevice device);\n-CUresult cuDeviceSetGraphMemAttribute(CUdevice device, CUgraphMem_attribute attr, void* value);\n-CUresult cuGraphAddBatchMemOpNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams);\n-CUresult cuGraphAddChildGraphNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUgraph childGraph);\n-CUresult cuGraphAddDependencies(CUgraph hGraph, const CUgraphNode* from, const CUgraphNode* to, const CUgraphEdgeData* edgeData, size_t numDependencies);\n-CUresult cuGraphAddEmptyNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies);\n-CUresult cuGraphAddEventRecordNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUevent event);\n-CUresult cuGraphAddEventWaitNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUevent event);\n-CUresult cuGraphAddExternalSemaphoresSignalNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* nodeParams);\n-CUresult cuGraphAddExternalSemaphoresWaitNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_EXT_SEM_WAIT_NODE_PARAMS* nodeParams);\n CUresult cuGraphAddHostNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_HOST_NODE_PARAMS* nodeParams);\n-CUresult cuGraphAddKernelNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_KERNEL_NODE_PARAMS* nodeParams);\n-CUresult cuGraphAddMemAllocNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUDA_MEM_ALLOC_NODE_PARAMS* nodeParams);\n-CUresult cuGraphAddMemFreeNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, CUdeviceptr dptr);\n-CUresult cuGraphAddMemcpyNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_MEMCPY3D* copyParams, CUcontext ctx);\n-CUresult cuGraphAddMemsetNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, size_t numDependencies, const CUDA_MEMSET_NODE_PARAMS* memsetParams, CUcontext ctx);\n-CUresult cuGraphAddNode(CUgraphNode* phGraphNode, CUgraph hGraph, const CUgraphNode* dependencies, const CUgraphEdgeData* dependencyData, size_t numDependencies, CUgraphNodeParams* nodeParams);\n-CUresult cuGraphBatchMemOpNodeGetParams(CUgraphNode hNode, CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams_out);\n-CUresult cuGraphBatchMemOpNodeSetParams(CUgraphNode hNode, const CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams);\n-CUresult cuGraphChildGraphNodeGetGraph(CUgraphNode hNode, CUgraph* phGraph);\n CUresult cuGraphClone(CUgraph* phGraphClone, CUgraph originalGraph);\n-CUresult cuGraphConditionalHandleCreate(CUgraphConditionalHandle* pHandle_out, CUgraph hGraph, CUcontext ctx, unsigned int defaultLaunchValue, unsigned int flags);\n CUresult cuGraphCreate(CUgraph* phGraph, unsigned int flags);\n-CUresult cuGraphDebugDotPrint(CUgraph hGraph, const char* path, unsigned int flags);\n CUresult cuGraphDestroy(CUgraph hGraph);\n CUresult cuGraphDestroyNode(CUgraphNode hNode);\n-CUresult cuGraphEventRecordNodeGetEvent(CUgraphNode hNode, CUevent* event_out);\n-CUresult cuGraphEventRecordNodeSetEvent(CUgraphNode hNode, CUevent event);\n-CUresult cuGraphEventWaitNodeGetEvent(CUgraphNode hNode, CUevent* event_out);\n-CUresult cuGraphEventWaitNodeSetEvent(CUgraphNode hNode, CUevent event);\n-CUresult cuGraphExecBatchMemOpNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_BATCH_MEM_OP_NODE_PARAMS* nodeParams);\n-CUresult cuGraphExecChildGraphNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, CUgraph childGraph);\n CUresult cuGraphExecDestroy(CUgraphExec hGraphExec);\n-CUresult cuGraphExecEventRecordNodeSetEvent(CUgraphExec hGraphExec, CUgraphNode hNode, CUevent event);\n-CUresult cuGraphExecEventWaitNodeSetEvent(CUgraphExec hGraphExec, CUgraphNode hNode, CUevent event);\n-CUresult cuGraphExecExternalSemaphoresSignalNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* nodeParams);\n-CUresult cuGraphExecExternalSemaphoresWaitNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_EXT_SEM_WAIT_NODE_PARAMS* nodeParams);\n-CUresult cuGraphExecGetFlags(CUgraphExec hGraphExec, cuuint64_t* flags);\n-CUresult cuGraphExecHostNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_HOST_NODE_PARAMS* nodeParams);\n-CUresult cuGraphExecKernelNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_KERNEL_NODE_PARAMS* nodeParams);\n-CUresult cuGraphExecMemcpyNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_MEMCPY3D* copyParams, CUcontext ctx);\n-CUresult cuGraphExecMemsetNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, const CUDA_MEMSET_NODE_PARAMS* memsetParams, CUcontext ctx);\n-CUresult cuGraphExecNodeSetParams(CUgraphExec hGraphExec, CUgraphNode hNode, CUgraphNodeParams* nodeParams);\n CUresult cuGraphExecUpdate(CUgraphExec hGraphExec, CUgraph hGraph, CUgraphExecUpdateResultInfo* resultInfo);\n-CUresult cuGraphExternalSemaphoresSignalNodeGetParams(CUgraphNode hNode, CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* params_out);\n-CUresult cuGraphExternalSemaphoresSignalNodeSetParams(CUgraphNode hNode, const CUDA_EXT_SEM_SIGNAL_NODE_PARAMS* nodeParams);\n-CUresult cuGraphExternalSemaphoresWaitNodeGetParams(CUgraphNode hNode, CUDA_EXT_SEM_WAIT_NODE_PARAMS* params_out);\n-CUresult cuGraphExternalSemaphoresWaitNodeSetParams(CUgraphNode hNode, const CUDA_EXT_SEM_WAIT_NODE_PARAMS* nodeParams);\n CUresult cuGraphGetEdges(CUgraph hGraph, CUgraphNode* from, CUgraphNode* to, size_t* numEdges);\n-CUresult cuGraphGetEdges_v2(CUgraph hGraph, CUgraphNode* from, CUgraphNode* to, CUgraphEdgeData* edgeData, size_t* numEdges);\n CUresult cuGraphGetNodes(CUgraph hGraph, CUgraphNode* nodes, size_t* numNodes);\n-CUresult cuGraphGetRootNodes(CUgraph hGraph, CUgraphNode* rootNodes, size_t* numRootNodes);\n CUresult cuGraphHostNodeGetParams(CUgraphNode hNode, CUDA_HOST_NODE_PARAMS* nodeParams);\n CUresult cuGraphHostNodeSetParams(CUgraphNode hNode, const CUDA_HOST_NODE_PARAMS* nodeParams);\n CUresult cuGraphInstantiate(CUgraphExec* phGraphExec, CUgraph hGraph, unsigned long long flags);\n-CUresult cuGraphInstantiateWithParams(CUgraphExec* phGraphExec, CUgraph hGraph, CUDA_GRAPH_INSTANTIATE_PARAMS* instantiateParams);\n-CUresult cuGraphKernelNodeCopyAttributes(CUgraphNode dst, CUgraphNode src);\n-CUresult cuGraphKernelNodeGetAttribute(CUgraphNode hNode, CUkernelNodeAttrID attr, CUkernelNodeAttrValue* value_out);\n-CUresult cuGraphKernelNodeGetParams(CUgraphNode hNode, CUDA_KERNEL_NODE_PARAMS* nodeParams);\n-CUresult cuGraphKernelNodeSetAttribute(CUgraphNode hNode, CUkernelNodeAttrID attr, const CUkernelNodeAttrValue* value);\n-CUresult cuGraphKernelNodeSetParams(CUgraphNode hNode, const CUDA_KERNEL_NODE_PARAMS* nodeParams);\n CUresult cuGraphLaunch(CUgraphExec hGraphExec, CUstream hStream);\n-CUresult cuGraphMemAllocNodeGetParams(CUgraphNode hNode, CUDA_MEM_ALLOC_NODE_PARAMS* params_out);\n-CUresult cuGraphMemFreeNodeGetParams(CUgraphNode hNode, CUdeviceptr* dptr_out);\n-CUresult cuGraphMemcpyNodeGetParams(CUgraphNode hNode, CUDA_MEMCPY3D* nodeParams);\n-CUresult cuGraphMemcpyNodeSetParams(CUgraphNode hNode, const CUDA_MEMCPY3D* nodeParams);\n-CUresult cuGraphMemsetNodeGetParams(CUgraphNode hNode, CUDA_MEMSET_NODE_PARAMS* nodeParams);\n-CUresult cuGraphMemsetNodeSetParams(CUgraphNode hNode, const CUDA_MEMSET_NODE_PARAMS* nodeParams);\n-CUresult cuGraphNodeFindInClone(CUgraphNode* phNode, CUgraphNode hOriginalNode, CUgraph hClonedGraph);\n-CUresult cuGraphNodeGetDependencies(CUgraphNode hNode, CUgraphNode* dependencies, CUgraphEdgeData* edgeData, size_t* numDependencies);\n-CUresult cuGraphNodeGetDependentNodes(CUgraphNode hNode, CUgraphNode* dependentNodes, CUgraphEdgeData* edgeData, size_t* numDependentNodes);\n-CUresult cuGraphNodeGetEnabled(CUgraphExec hGraphExec, CUgraphNode hNode, unsigned int* isEnabled);\n CUresult cuGraphNodeGetType(CUgraphNode hNode, CUgraphNodeType* type);\n-CUresult cuGraphNodeSetEnabled(CUgraphExec hGraphExec, CUgraphNode hNode, unsigned int isEnabled);\n-CUresult cuGraphNodeSetParams(CUgraphNode hNode, CUgraphNodeParams* nodeParams);\n-CUresult cuGraphReleaseUserObject(CUgraph graph, CUuserObject object, unsigned int count);\n-CUresult cuGraphRemoveDependencies(CUgraph hGraph, const CUgraphNode* from, const CUgraphNode* to, const CUgraphEdgeData* edgeData, size_t numDependencies);\n-CUresult cuGraphRetainUserObject(CUgraph graph, CUuserObject object, unsigned int count, unsigned int flags);\n-CUresult cuGraphUpload(CUgraphExec hGraphExec, CUstream hStream);\n-CUresult cuUserObjectCreate(CUuserObject* object_out, void* ptr, CUhostFn destroy, unsigned int initialRefcount, unsigned int flags);\n-CUresult cuUserObjectRelease(CUuserObject object, unsigned int count);\n-CUresult cuUserObjectRetain(CUuserObject object, unsigned int count);\n</code></pre>"},{"location":"manual/api-driver/#625-occupancy","title":"6.25. Occupancy","text":"<pre><code> CUresult cuOccupancyAvailableDynamicSMemPerBlock(size_t* dynamicSmemSize, CUfunction func, int numBlocks, int blockSize);\n CUresult cuOccupancyMaxActiveBlocksPerMultiprocessor(int* numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize);\n CUresult cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int* numBlocks, CUfunction func, int blockSize, size_t dynamicSMemSize, unsigned int flags);\n-CUresult cuOccupancyMaxActiveClusters(int* numClusters, CUfunction func, const CUlaunchConfig* config);\n CUresult cuOccupancyMaxPotentialBlockSize(int* minGridSize, int* blockSize, CUfunction func, CUoccupancyB2DSize blockSizeToDynamicSMemSize, size_t dynamicSMemSize, int blockSizeLimit);\n CUresult cuOccupancyMaxPotentialBlockSizeWithFlags(int* minGridSize, int* blockSize, CUfunction func, CUoccupancyB2DSize blockSizeToDynamicSMemSize, size_t dynamicSMemSize, int blockSizeLimit, unsigned int flags);\n-CUresult cuOccupancyMaxPotentialClusterSize(int* clusterSize, CUfunction func, const CUlaunchConfig* config);\n</code></pre>"},{"location":"manual/api-driver/#626-texture-reference-management-deprecated","title":"6.26. Texture Reference Management [DEPRECATED]","text":"<pre><code> CUresult cuTexRefCreate(CUtexref* pTexRef);\n CUresult cuTexRefDestroy(CUtexref hTexRef);\n CUresult cuTexRefGetAddress(CUdeviceptr* pdptr, CUtexref hTexRef);\n CUresult cuTexRefGetAddressMode(CUaddress_mode* pam, CUtexref hTexRef, int dim);\n CUresult cuTexRefGetArray(CUarray* phArray, CUtexref hTexRef);\n-CUresult cuTexRefGetBorderColor(float* pBorderColor, CUtexref hTexRef);\n CUresult cuTexRefGetFilterMode(CUfilter_mode* pfm, CUtexref hTexRef);\n-CUresult cuTexRefGetFlags(unsigned int* pFlags, CUtexref hTexRef);\n-CUresult cuTexRefGetFormat(CUarray_format* pFormat, int* pNumChannels, CUtexref hTexRef);\n-CUresult cuTexRefGetMaxAnisotropy(int* pmaxAniso, CUtexref hTexRef);\n-CUresult cuTexRefGetMipmapFilterMode(CUfilter_mode* pfm, CUtexref hTexRef);\n-CUresult cuTexRefGetMipmapLevelBias(float* pbias, CUtexref hTexRef);\n-CUresult cuTexRefGetMipmapLevelClamp(float* pminMipmapLevelClamp, float* pmaxMipmapLevelClamp, CUtexref hTexRef);\n-CUresult cuTexRefGetMipmappedArray(CUmipmappedArray* phMipmappedArray, CUtexref hTexRef);\n-CUresult cuTexRefSetAddress(size_t* ByteOffset, CUtexref hTexRef, CUdeviceptr dptr, size_t bytes);\n-CUresult cuTexRefSetAddress2D(CUtexref hTexRef, const CUDA_ARRAY_DESCRIPTOR* desc, CUdeviceptr dptr, size_t Pitch);\n-CUresult cuTexRefSetAddressMode(CUtexref hTexRef, int dim, CUaddress_mode am);\n-CUresult cuTexRefSetArray(CUtexref hTexRef, CUarray hArray, unsigned int Flags);\n-CUresult cuTexRefSetBorderColor(CUtexref hTexRef, float* pBorderColor);\n-CUresult cuTexRefSetFilterMode(CUtexref hTexRef, CUfilter_mode fm);\n-CUresult cuTexRefSetFlags(CUtexref hTexRef, unsigned int Flags);\n-CUresult cuTexRefSetFormat(CUtexref hTexRef, CUarray_format fmt, int NumPackedComponents);\n-CUresult cuTexRefSetMaxAnisotropy(CUtexref hTexRef, unsigned int maxAniso);\n-CUresult cuTexRefSetMipmapFilterMode(CUtexref hTexRef, CUfilter_mode fm);\n-CUresult cuTexRefSetMipmapLevelBias(CUtexref hTexRef, float bias);\n-CUresult cuTexRefSetMipmapLevelClamp(CUtexref hTexRef, float minMipmapLevelClamp, float maxMipmapLevelClamp);\n-CUresult cuTexRefSetMipmappedArray(CUtexref hTexRef, CUmipmappedArray hMipmappedArray, unsigned int Flags);\n</code></pre>"},{"location":"manual/api-driver/#627-surface-reference-management-deprecated","title":"6.27. Surface Reference Management [DEPRECATED]","text":"<pre><code>-CUresult cuSurfRefGetArray(CUarray* phArray, CUsurfref hSurfRef);\n-CUresult cuSurfRefSetArray(CUsurfref hSurfRef, CUarray hArray, unsigned int Flags);\n</code></pre>"},{"location":"manual/api-driver/#628-texture-object-management","title":"6.28. Texture Object Management","text":"<pre><code> CUresult cuTexObjectCreate(CUtexObject* pTexObject, const CUDA_RESOURCE_DESC* pResDesc, const CUDA_TEXTURE_DESC* pTexDesc, const CUDA_RESOURCE_VIEW_DESC* pResViewDesc);\n CUresult cuTexObjectDestroy(CUtexObject texObject);\n CUresult cuTexObjectGetResourceDesc(CUDA_RESOURCE_DESC* pResDesc, CUtexObject texObject);\n CUresult cuTexObjectGetResourceViewDesc(CUDA_RESOURCE_VIEW_DESC* pResViewDesc, CUtexObject texObject);\n CUresult cuTexObjectGetTextureDesc(CUDA_TEXTURE_DESC* pTexDesc, CUtexObject texObject);\n</code></pre>"},{"location":"manual/api-driver/#629-surface-object-management","title":"6.29. Surface Object Management","text":"<pre><code>-CUresult cuSurfObjectCreate(CUsurfObject* pSurfObject, const CUDA_RESOURCE_DESC* pResDesc);\n-CUresult cuSurfObjectDestroy(CUsurfObject surfObject);\n-CUresult cuSurfObjectGetResourceDesc(CUDA_RESOURCE_DESC* pResDesc, CUsurfObject surfObject);\n</code></pre>"},{"location":"manual/api-driver/#630-tensor-map-object-managment","title":"6.30. Tensor Map Object Managment","text":"<pre><code> CUresult cuTensorMapEncodeIm2col(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType, cuuint32_t tensorRank, void* globalAddress, const cuuint64_t* globalDim, const cuuint64_t* globalStrides, const int* pixelBoxLowerCorner, const int* pixelBoxUpperCorner, cuuint32_t channelsPerPixel, cuuint32_t pixelsPerColumn, const cuuint32_t* elementStrides, CUtensorMapInterleave interleave, CUtensorMapSwizzle swizzle, CUtensorMapL2promotion l2Promotion, CUtensorMapFloatOOBfill oobFill);\n CUresult cuTensorMapEncodeIm2colWide(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType, cuuint32_t tensorRank, void* globalAddress, const cuuint64_t* globalDim, const cuuint64_t* globalStrides, int pixelBoxLowerCornerWidth, int pixelBoxUpperCornerWidth, cuuint32_t channelsPerPixel, cuuint32_t pixelsPerColumn, const cuuint32_t* elementStrides, CUtensorMapInterleave interleave, CUtensorMapIm2ColWideMode mode, CUtensorMapSwizzle swizzle, CUtensorMapL2promotion l2Promotion, CUtensorMapFloatOOBfill oobFill);\n CUresult cuTensorMapEncodeTiled(CUtensorMap* tensorMap, CUtensorMapDataType tensorDataType, cuuint32_t tensorRank, void* globalAddress, const cuuint64_t* globalDim, const cuuint64_t* globalStrides, const cuuint32_t* boxDim, const cuuint32_t* elementStrides, CUtensorMapInterleave interleave, CUtensorMapSwizzle swizzle, CUtensorMapL2promotion l2Promotion, CUtensorMapFloatOOBfill oobFill);\n CUresult cuTensorMapReplaceAddress(CUtensorMap* tensorMap, void* globalAddress);\n</code></pre>"},{"location":"manual/api-driver/#631-peer-context-memory-access","title":"6.31. Peer Context Memory Access","text":"<pre><code>-CUresult cuCtxDisablePeerAccess(CUcontext peerContext);\n-CUresult cuCtxEnablePeerAccess(CUcontext peerContext, unsigned int Flags);\n CUresult cuDeviceCanAccessPeer(int* canAccessPeer, CUdevice dev, CUdevice peerDev);\n-CUresult cuDeviceGetP2PAtomicCapabilities(unsigned int* capabilities, const CUatomicOperation ** operations, unsigned int count, CUdevice srcDevice, CUdevice dstDevice);\n CUresult cuDeviceGetP2PAttribute(int* value, CUdevice_P2PAttribute attrib, CUdevice srcDevice, CUdevice dstDevice);\n</code></pre>"},{"location":"manual/api-driver/#632-graphics-interoperability","title":"6.32. Graphics Interoperability","text":"<pre><code>-CUresult cuGraphicsMapResources(unsigned int count, CUgraphicsResource* resources, CUstream hStream);\n-CUresult cuGraphicsResourceGetMappedMipmappedArray(CUmipmappedArray* pMipmappedArray, CUgraphicsResource resource);\n-CUresult cuGraphicsResourceGetMappedPointer(CUdeviceptr* pDevPtr, size_t* pSize, CUgraphicsResource resource);\n-CUresult cuGraphicsResourceSetMapFlags(CUgraphicsResource resource, unsigned int flags);\n-CUresult cuGraphicsSubResourceGetMappedArray(CUarray* pArray, CUgraphicsResource resource, unsigned int arrayIndex, unsigned int mipLevel);\n-CUresult cuGraphicsUnmapResources(unsigned int count, CUgraphicsResource* resources, CUstream hStream);\n-CUresult cuGraphicsUnregisterResource(CUgraphicsResource resource);\n</code></pre>"},{"location":"manual/api-driver/#633-driver-entry-point-access","title":"6.33. Driver Entry Point Access","text":"<pre><code> CUresult cuGetProcAddress(const char* symbol, void** pfn, int cudaVersion, cuuint64_t flags, CUdriverProcAddressQueryResult* symbolStatus);\n</code></pre>"},{"location":"manual/api-driver/#634-coredump-attributes-control-api","title":"6.34. Coredump Attributes Control API","text":"<pre><code>-enum CUCoredumpGenerationFlags\n-enum CUcoredumpSettings\n-CUresult cuCoredumpGetAttribute(CUcoredumpSettings attrib, void* value, size_t* size);\n-CUresult cuCoredumpGetAttributeGlobal(CUcoredumpSettings attrib, void* value, size_t* size);\n-CUresult cuCoredumpSetAttribute(CUcoredumpSettings attrib, void* value, size_t* size);\n-CUresult cuCoredumpSetAttributeGlobal(CUcoredumpSettings attrib, void* value, size_t* size);\n</code></pre>"},{"location":"manual/api-driver/#635-green-contexts","title":"6.35. Green Contexts","text":"<pre><code>-enum CUdevResourceType\n-struct CUdevResource\n-struct CUdevSmResource\n-typedef CUdevResourceDesc_st * CUdevResourceDesc;\n-CUresult cuCtxFromGreenCtx(CUcontext* pContext, CUgreenCtx hCtx);\n-CUresult cuCtxGetDevResource(CUcontext hCtx, CUdevResource* resource, CUdevResourceType type);\n-CUresult cuDevResourceGenerateDesc(CUdevResourceDesc* phDesc, CUdevResource* resources, unsigned int nbResources);\n-CUresult cuDevSmResourceSplitByCount(CUdevResource* result, unsigned int* nbGroups, const CUdevResource* input, CUdevResource* remaining, unsigned int useFlags, unsigned int minCount);\n-CUresult cuDeviceGetDevResource(CUdevice device, CUdevResource* resource, CUdevResourceType type);\n-CUresult cuGreenCtxCreate(CUgreenCtx* phCtx, CUdevResourceDesc desc, CUdevice dev, unsigned int flags);\n-CUresult cuGreenCtxDestroy(CUgreenCtx hCtx);\n-CUresult cuGreenCtxGetDevResource(CUgreenCtx hCtx, CUdevResource* resource, CUdevResourceType type);\n-CUresult cuGreenCtxGetId(CUgreenCtx greenCtx, unsigned long long* greenCtxId);\n-CUresult cuGreenCtxRecordEvent(CUgreenCtx hCtx, CUevent hEvent);\n-CUresult cuGreenCtxStreamCreate(CUstream* phStream, CUgreenCtx greenCtx, unsigned int flags, int priority);\n-CUresult cuGreenCtxWaitEvent(CUgreenCtx hCtx, CUevent hEvent);\n-CUresult cuStreamGetGreenCtx(CUstream hStream, CUgreenCtx* phCtx);\n</code></pre>"},{"location":"manual/api-driver/#636-error-log-management-functions","title":"6.36. Error Log Management Functions","text":"<pre><code>-CUresult cuLogsCurrent(CUlogIterator* iterator_out, unsigned int flags);\n-CUresult cuLogsDumpToFile(CUlogIterator* iterator, const char* pathToFile, unsigned int flags);\n-CUresult cuLogsDumpToMemory(CUlogIterator* iterator, char* buffer, size_t* size, unsigned int flags);\n-CUresult cuLogsRegisterCallback(CUlogsCallback callbackFunc, void* userData, CUlogsCallbackHandle* callback_out);\n-CUresult cuLogsUnregisterCallback(CUlogsCallbackHandle callback);\n</code></pre>"},{"location":"manual/api-driver/#637-cuda-checkpointing","title":"6.37. CUDA Checkpointing","text":"<pre><code>-CUresult cuCheckpointProcessCheckpoint(int pid, CUcheckpointCheckpointArgs* args);\n-CUresult cuCheckpointProcessGetRestoreThreadId(int pid, int* tid);\n-CUresult cuCheckpointProcessGetState(int pid, CUprocessState* state);\n-CUresult cuCheckpointProcessLock(int pid, CUcheckpointLockArgs* args);\n-CUresult cuCheckpointProcessRestore(int pid, CUcheckpointRestoreArgs* args);\n-CUresult cuCheckpointProcessUnlock(int pid, CUcheckpointUnlockArgs* args);\n</code></pre>"},{"location":"manual/api-driver/#638-profiler-control-deprecated","title":"6.38. Profiler Control [DEPRECATED]","text":"<pre><code> CUresult cuProfilerInitialize(const char* configFile, const char* outputFile, CUoutput_mode outputMode);\n</code></pre>"},{"location":"manual/api-driver/#639-profiler-control","title":"6.39. Profiler Control","text":"<pre><code> CUresult cuProfilerStart(void);\n CUresult cuProfilerStop(void);\n</code></pre>"},{"location":"manual/api-driver/#640-opengl-interoperability","title":"6.40. OpenGL Interoperability","text":"<pre><code>-enum CUGLDeviceList\n-CUresult cuGLGetDevices(unsigned int* pCudaDeviceCount, CUdevice* pCudaDevices, unsigned int cudaDeviceCount, CUGLDeviceList deviceList);\n-CUresult cuGraphicsGLRegisterBuffer(CUgraphicsResource* pCudaResource, GLuint buffer, unsigned int Flags);\n-CUresult cuGraphicsGLRegisterImage(CUgraphicsResource* pCudaResource, GLuint image, GLenum target, unsigned int Flags);\n-CUresult cuWGLGetDevice(CUdevice* pDevice, HGPUNV hGpu);\n</code></pre>"},{"location":"manual/api-driver/#6401-opengl-interoperability-deprecated","title":"6.40.1. OpenGL Interoperability [DEPRECATED]","text":"<pre><code>-enum CUGLmap_flags\n-CUresult cuGLCtxCreate(CUcontext* pCtx, unsigned int Flags, CUdevice device);\n-CUresult cuGLInit(void);\n-CUresult cuGLMapBufferObject(CUdeviceptr* dptr, size_t* size, GLuint buffer);\n-CUresult cuGLMapBufferObjectAsync(CUdeviceptr* dptr, size_t* size, GLuint buffer, CUstream hStream);\n-CUresult cuGLRegisterBufferObject(GLuint buffer);\n-CUresult cuGLSetBufferObjectMapFlags(GLuint buffer, unsigned int Flags);\n-CUresult cuGLUnmapBufferObject(GLuint buffer);\n-CUresult cuGLUnmapBufferObjectAsync(GLuint buffer, CUstream hStream);\n-CUresult cuGLUnregisterBufferObject(GLuint buffer);\n</code></pre>"},{"location":"manual/api-driver/#641-direct3d-9-interoperability","title":"6.41. Direct3D 9 Interoperability","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-driver/#6411-direct3d-9-interoperability-deprecated","title":"6.41.1. Direct3D 9 Interoperability [DEPRECATED]","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-driver/#642-direct3d-10-interoperability","title":"6.42. Direct3D 10 Interoperability","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-driver/#6421-direct3d-10-interoperability-deprecated","title":"6.42.1. Direct3D 10 Interoperability [DEPRECATED]","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-driver/#643-direct3d-11-interoperability","title":"6.43. Direct3D 11 Interoperability","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-driver/#6431-direct3d-11-interoperability-deprecated","title":"6.43.1. Direct3D 11 Interoperability [DEPRECATED]","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-driver/#644-vdpau-interoperability","title":"6.44. VDPAU Interoperability","text":"<pre><code>-VDPAU is currently unsupported\n</code></pre>"},{"location":"manual/api-driver/#645-egl-interoperability","title":"6.45. EGL Interoperability","text":"<pre><code>-EGL is currently unsupported\n</code></pre>"},{"location":"manual/api-math/","title":"Math API","text":""},{"location":"manual/api-math/#1-fp4-intrinsics","title":"1. FP4 Intrinsics","text":"<pre><code>-4 bit floating point is not supported\n</code></pre>"},{"location":"manual/api-math/#2-fp6-intrinsics","title":"2. FP6 Intrinsics","text":"<pre><code>-6 bit floating point is not supported\n</code></pre>"},{"location":"manual/api-math/#3-fp8-intrinsics","title":"3. FP8 Intrinsics","text":"<pre><code>-8 bit floating point is not supported\n</code></pre>"},{"location":"manual/api-math/#4-half-precision-intrinsics","title":"4. Half Precision Intrinsics","text":"<pre><code> typedef __half __nv_half;\n typedef __half2 __nv_half2;\n typedef __half2_raw __nv_half2_raw;\n typedef __half_raw __nv_half_raw;\n typedef __half half;\n typedef __half2 half2;\n typedef __half nv_half;\n typedef __half2 nv_half2;\n #define CUDART_INF_FP16\n #define CUDART_MAX_NORMAL_FP16\n #define CUDART_MIN_DENORM_FP16\n #define CUDART_NAN_FP16\n #define CUDART_NEG_ZERO_FP16\n #define CUDART_ONE_FP16\n #define CUDART_ZERO_FP16\n __half __habs(const __half a);\n __device__ __half __habs(const __half a);\n __half __hadd(const __half a, const __half b);\n __device__ __half __hadd(const __half a, const __half b);\n __half __hadd_rn(const __half a, const __half b);\n __device__ __half __hadd_rn(const __half a, const __half b);\n __half __hadd_sat(const __half a, const __half b);\n __device__ __half __hadd_sat(const __half a, const __half b);\n __half __hdiv(const __half a, const __half b);\n __device__ __half __hdiv(const __half a, const __half b);\n __device__ __half __hfma(const __half a, const __half b, const __half c);\n __device__ __half __hfma_relu(const __half a, const __half b, const __half c);\n __device__ __half __hfma_sat(const __half a, const __half b, const __half c);\n __half __hmul(const __half a, const __half b);\n __device__ __half __hmul(const __half a, const __half b);\n __half __hmul_rn(const __half a, const __half b);\n __device__ __half __hmul_rn(const __half a, const __half b);\n __half __hmul_sat(const __half a, const __half b);\n __device__ __half __hmul_sat(const __half a, const __half b);\n __half __hneg(const __half a);\n __device__ __half __hneg(const __half a);\n __half __hsub(const __half a, const __half b);\n __device__ __half __hsub(const __half a, const __half b);\n __half __hsub_rn(const __half a, const __half b);\n __device__ __half __hsub_rn(const __half a, const __half b);\n __half __hsub_sat(const __half a, const __half b);\n __device__ __half __hsub_sat(const __half a, const __half b);\n __device__ __half atomicAdd(__half *const address, const __half val);\n bool __heq(const __half a, const __half b);\n __device__ bool __heq(const __half a, const __half b);\n bool __hequ(const __half a, const __half b);\n __device__ bool __hequ(const __half a, const __half b);\n bool __hge(const __half a, const __half b);\n __device__ bool __hge(const __half a, const __half b);\n bool __hgeu(const __half a, const __half b);\n __device__ bool __hgeu(const __half a, const __half b);\n bool __hgt(const __half a, const __half b);\n __device__ bool __hgt(const __half a, const __half b);\n bool __hgtu(const __half a, const __half b);\n __device__ bool __hgtu(const __half a, const __half b);\n int __hisinf(const __half a);\n __device__ int __hisinf(const __half a);\n bool __hisnan(const __half a);\n __device__ bool __hisnan(const __half a);\n bool __hle(const __half a, const __half b);\n __device__ bool __hle(const __half a, const __half b);\n bool __hleu(const __half a, const __half b);\n __device__ bool __hleu(const __half a, const __half b);\n bool __hlt(const __half a, const __half b);\n __device__ bool __hlt(const __half a, const __half b);\n bool __hltu(const __half a, const __half b);\n __device__ bool __hltu(const __half a, const __half b);\n __half __hmax(const __half a, const __half b);\n __device__ __half __hmax(const __half a, const __half b);\n __half __hmax_nan(const __half a, const __half b);\n __device__ __half __hmax_nan(const __half a, const __half b);\n __half __hmin(const __half a, const __half b);\n __device__ __half __hmin(const __half a, const __half b);\n __half __hmin_nan(const __half a, const __half b);\n __device__ __half __hmin_nan(const __half a, const __half b);\n bool __hne(const __half a, const __half b);\n __device__ bool __hne(const __half a, const __half b);\n bool __hneu(const __half a, const __half b);\n __device__ bool __hneu(const __half a, const __half b);\n __device__ __half hceil(const __half h);\n __device__ __half hcos(const __half a);\n __device__ __half hexp(const __half a);\n __device__ __half hexp10(const __half a);\n __device__ __half hexp2(const __half a);\n __device__ __half hfloor(const __half h);\n __device__ __half hlog(const __half a);\n __device__ __half hlog10(const __half a);\n __device__ __half hlog2(const __half a);\n __device__ __half hrcp(const __half a);\n __device__ __half hrint(const __half h);\n __device__ __half hrsqrt(const __half a);\n __device__ __half hsin(const __half a);\n __device__ __half hsqrt(const __half a);\n __device__ __half htanh(const __half a);\n __device__ __half htanh_approx(const __half a);\n __device__ __half htrunc(const __half h);\n __half __double2half(const double a);\n __device__ __half __double2half(const double a);\n __half2 __float22half2_rn(const float2 a);\n __device__ __half2 __float22half2_rn(const float2 a);\n __half __float2half(const float a);\n __device__ __half __float2half(const float a);\n __half2 __float2half2_rn(const float a);\n __device__ __half2 __float2half2_rn(const float a);\n __half __float2half_rd(const float a);\n __device__ __half __float2half_rd(const float a);\n __half __float2half_rn(const float a);\n __device__ __half __float2half_rn(const float a);\n __half __float2half_ru(const float a);\n __device__ __half __float2half_ru(const float a);\n __half __float2half_rz(const float a);\n __device__ __half __float2half_rz(const float a);\n __half2 __floats2half2_rn(const float a, const float b);\n __device__ __half2 __floats2half2_rn(const float a, const float b);\n float2 __half22float2(const __half2 a);\n __device__ float2 __half22float2(const __half2 a);\n-signed char __half2char_rz(const __half h);\n-__device__ signed char __half2char_rz(const __half h);\n float __half2float(const __half a);\n __device__ float __half2float(const __half a);\n __half2 __half2half2(const __half a);\n __device__ __half2 __half2half2(const __half a);\n __device__ int __half2int_rd(const __half h);\n __device__ int __half2int_rn(const __half h);\n __device__ int __half2int_ru(const __half h);\n int __half2int_rz(const __half h);\n __device__ int __half2int_rz(const __half h);\n __device__ long long int __half2ll_rd(const __half h);\n __device__ long long int __half2ll_rn(const __half h);\n __device__ long long int __half2ll_ru(const __half h);\n long long int __half2ll_rz(const __half h);\n __device__ long long int __half2ll_rz(const __half h);\n __device__ short int __half2short_rd(const __half h);\n __device__ short int __half2short_rn(const __half h);\n __device__ short int __half2short_ru(const __half h);\n short int __half2short_rz(const __half h);\n __device__ short int __half2short_rz(const __half h);\n unsigned char __half2uchar_rz(const __half h);\n __device__ unsigned char __half2uchar_rz(const __half h);\n __device__ unsigned int __half2uint_rd(const __half h);\n __device__ unsigned int __half2uint_rn(const __half h);\n __device__ unsigned int __half2uint_ru(const __half h);\n unsigned int __half2uint_rz(const __half h);\n __device__ unsigned int __half2uint_rz(const __half h);\n __device__ unsigned long long int __half2ull_rd(const __half h);\n __device__ unsigned long long int __half2ull_rn(const __half h);\n __device__ unsigned long long int __half2ull_ru(const __half h);\n unsigned long long int __half2ull_rz(const __half h);\n __device__ unsigned long long int __half2ull_rz(const __half h);\n __device__ unsigned short int __half2ushort_rd(const __half h);\n __device__ unsigned short int __half2ushort_rn(const __half h);\n __device__ unsigned short int __half2ushort_ru(const __half h);\n unsigned short int __half2ushort_rz(const __half h);\n __device__ unsigned short int __half2ushort_rz(const __half h);\n short int __half_as_short(const __half h);\n __device__ short int __half_as_short(const __half h);\n unsigned short int __half_as_ushort(const __half h);\n __device__ unsigned short int __half_as_ushort(const __half h);\n __half2 __halves2half2(const __half a, const __half b);\n __device__ __half2 __halves2half2(const __half a, const __half b);\n float __high2float(const __half2 a);\n __device__ float __high2float(const __half2 a);\n __half __high2half(const __half2 a);\n __device__ __half __high2half(const __half2 a);\n __half2 __high2half2(const __half2 a);\n __device__ __half2 __high2half2(const __half2 a);\n __half2 __highs2half2(const __half2 a, const __half2 b);\n __device__ __half2 __highs2half2(const __half2 a, const __half2 b);\n __half __int2half_rd(const int i);\n __device__ __half __int2half_rd(const int i);\n __half __int2half_rn(const int i);\n __device__ __half __int2half_rn(const int i);\n __half __int2half_ru(const int i);\n __device__ __half __int2half_ru(const int i);\n __half __int2half_rz(const int i);\n __device__ __half __int2half_rz(const int i);\n __device__ __half2 __ldca(const __half2 *const ptr);\n __device__ __half __ldca(const __half *const ptr);\n __device__ __half __ldcg(const __half *const ptr);\n __device__ __half2 __ldcg(const __half2 *const ptr);\n __device__ __half __ldcs(const __half *const ptr);\n __device__ __half2 __ldcs(const __half2 *const ptr);\n __device__ __half2 __ldcv(const __half2 *const ptr);\n __device__ __half __ldcv(const __half *const ptr);\n __device__ __half2 __ldg(const __half2 *const ptr);\n __device__ __half __ldg(const __half *const ptr);\n __device__ __half __ldlu(const __half *const ptr);\n __device__ __half2 __ldlu(const __half2 *const ptr);\n __half __ll2half_rd(const long long int i);\n __device__ __half __ll2half_rd(const long long int i);\n __half __ll2half_rn(const long long int i);\n __device__ __half __ll2half_rn(const long long int i);\n __half __ll2half_ru(const long long int i);\n __device__ __half __ll2half_ru(const long long int i);\n __half __ll2half_rz(const long long int i);\n __device__ __half __ll2half_rz(const long long int i);\n float __low2float(const __half2 a);\n __device__ float __low2float(const __half2 a);\n __half __low2half(const __half2 a);\n __device__ __half __low2half(const __half2 a);\n __half2 __low2half2(const __half2 a);\n __device__ __half2 __low2half2(const __half2 a);\n __half2 __lowhigh2highlow(const __half2 a);\n __device__ __half2 __lowhigh2highlow(const __half2 a);\n __half2 __lows2half2(const __half2 a, const __half2 b);\n __device__ __half2 __lows2half2(const __half2 a, const __half2 b);\n-__device__ __half __shfl_down_sync(const unsigned int mask, const __half var, const unsigned int delta, const int width=warpSize);\n-__device__ __half2 __shfl_down_sync(const unsigned int mask, const __half2 var, const unsigned int delta, const int width=warpSize);\n-__device__ __half2 __shfl_sync(const unsigned int mask, const __half2 var, const int srcLane, const int width=warpSize);\n-__device__ __half __shfl_sync(const unsigned int mask, const __half var, const int srcLane, const int width=warpSize);\n-__device__ __half2 __shfl_up_sync(const unsigned int mask, const __half2 var, const unsigned int delta, const int width=warpSize);\n-__device__ __half __shfl_up_sync(const unsigned int mask, const __half var, const unsigned int delta, const int width=warpSize);\n-__device__ __half2 __shfl_xor_sync(const unsigned int mask, const __half2 var, const int laneMask, const int width=warpSize);\n-__device__ __half __shfl_xor_sync(const unsigned int mask, const __half var, const int laneMask, const int width=warpSize);\n __half __short2half_rd(const short int i);\n __device__ __half __short2half_rd(const short int i);\n __half __short2half_rn(const short int i);\n __device__ __half __short2half_rn(const short int i);\n __half __short2half_ru(const short int i);\n __device__ __half __short2half_ru(const short int i);\n __half __short2half_rz(const short int i);\n __device__ __half __short2half_rz(const short int i);\n __half __short_as_half(const short int i);\n __device__ __half __short_as_half(const short int i);\n __device__ void __stcg(__half2 *const ptr, const __half2 value);\n __device__ void __stcg(__half *const ptr, const __half value);\n __device__ void __stcs(__half2 *const ptr, const __half2 value);\n __device__ void __stcs(__half *const ptr, const __half value);\n __device__ void __stwb(__half2 *const ptr, const __half2 value);\n __device__ void __stwb(__half *const ptr, const __half value);\n __device__ void __stwt(__half *const ptr, const __half value);\n __device__ void __stwt(__half2 *const ptr, const __half2 value);\n __half __uint2half_rd(const unsigned int i);\n __device__ __half __uint2half_rd(const unsigned int i);\n __half __uint2half_rn(const unsigned int i);\n __device__ __half __uint2half_rn(const unsigned int i);\n __half __uint2half_ru(const unsigned int i);\n __device__ __half __uint2half_ru(const unsigned int i);\n __half __uint2half_rz(const unsigned int i);\n __device__ __half __uint2half_rz(const unsigned int i);\n __half __ull2half_rd(const unsigned long long int i);\n __device__ __half __ull2half_rd(const unsigned long long int i);\n __half __ull2half_rn(const unsigned long long int i);\n __device__ __half __ull2half_rn(const unsigned long long int i);\n __half __ull2half_ru(const unsigned long long int i);\n __device__ __half __ull2half_ru(const unsigned long long int i);\n __half __ull2half_rz(const unsigned long long int i);\n __device__ __half __ull2half_rz(const unsigned long long int i);\n __half __ushort2half_rd(const unsigned short int i);\n __device__ __half __ushort2half_rd(const unsigned short int i);\n __half __ushort2half_rn(const unsigned short int i);\n __device__ __half __ushort2half_rn(const unsigned short int i);\n __half __ushort2half_ru(const unsigned short int i);\n __device__ __half __ushort2half_ru(const unsigned short int i);\n __half __ushort2half_rz(const unsigned short int i);\n __device__ __half __ushort2half_rz(const unsigned short int i);\n __half __ushort_as_half(const unsigned short int i);\n __device__ __half __ushort_as_half(const unsigned short int i);\n __half2 make_half2(const __half x, const __half y);\n __device__ __half2 make_half2(const __half x, const __half y);\n __half2 __h2div(const __half2 a, const __half2 b);\n __device__ __half2 __h2div(const __half2 a, const __half2 b);\n __half2 __habs2(const __half2 a);\n __device__ __half2 __habs2(const __half2 a);\n __half2 __hadd2(const __half2 a, const __half2 b);\n __device__ __half2 __hadd2(const __half2 a, const __half2 b);\n __half2 __hadd2_rn(const __half2 a, const __half2 b);\n __device__ __half2 __hadd2_rn(const __half2 a, const __half2 b);\n __half2 __hadd2_sat(const __half2 a, const __half2 b);\n __device__ __half2 __hadd2_sat(const __half2 a, const __half2 b);\n __device__ __half2 __hcmadd(const __half2 a, const __half2 b, const __half2 c);\n __device__ __half2 __hfma2(const __half2 a, const __half2 b, const __half2 c);\n __device__ __half2 __hfma2_relu(const __half2 a, const __half2 b, const __half2 c);\n __device__ __half2 __hfma2_sat(const __half2 a, const __half2 b, const __half2 c);\n __half2 __hmul2(const __half2 a, const __half2 b);\n __device__ __half2 __hmul2(const __half2 a, const __half2 b);\n __half2 __hmul2_rn(const __half2 a, const __half2 b);\n __device__ __half2 __hmul2_rn(const __half2 a, const __half2 b);\n __half2 __hmul2_sat(const __half2 a, const __half2 b);\n __device__ __half2 __hmul2_sat(const __half2 a, const __half2 b);\n __half2 __hneg2(const __half2 a);\n __device__ __half2 __hneg2(const __half2 a);\n __half2 __hsub2(const __half2 a, const __half2 b);\n __device__ __half2 __hsub2(const __half2 a, const __half2 b);\n __half2 __hsub2_rn(const __half2 a, const __half2 b);\n __device__ __half2 __hsub2_rn(const __half2 a, const __half2 b);\n __half2 __hsub2_sat(const __half2 a, const __half2 b);\n __device__ __half2 __hsub2_sat(const __half2 a, const __half2 b);\n __device__ __half2 atomicAdd(__half2 *const address, const __half2 val);\n bool __hbeq2(const __half2 a, const __half2 b);\n __device__ bool __hbeq2(const __half2 a, const __half2 b);\n bool __hbequ2(const __half2 a, const __half2 b);\n __device__ bool __hbequ2(const __half2 a, const __half2 b);\n bool __hbge2(const __half2 a, const __half2 b);\n __device__ bool __hbge2(const __half2 a, const __half2 b);\n bool __hbgeu2(const __half2 a, const __half2 b);\n __device__ bool __hbgeu2(const __half2 a, const __half2 b);\n bool __hbgt2(const __half2 a, const __half2 b);\n __device__ bool __hbgt2(const __half2 a, const __half2 b);\n bool __hbgtu2(const __half2 a, const __half2 b);\n __device__ bool __hbgtu2(const __half2 a, const __half2 b);\n bool __hble2(const __half2 a, const __half2 b);\n __device__ bool __hble2(const __half2 a, const __half2 b);\n bool __hbleu2(const __half2 a, const __half2 b);\n __device__ bool __hbleu2(const __half2 a, const __half2 b);\n bool __hblt2(const __half2 a, const __half2 b);\n __device__ bool __hblt2(const __half2 a, const __half2 b);\n bool __hbltu2(const __half2 a, const __half2 b);\n __device__ bool __hbltu2(const __half2 a, const __half2 b);\n bool __hbne2(const __half2 a, const __half2 b);\n __device__ bool __hbne2(const __half2 a, const __half2 b);\n bool __hbneu2(const __half2 a, const __half2 b);\n __device__ bool __hbneu2(const __half2 a, const __half2 b);\n __half2 __heq2(const __half2 a, const __half2 b);\n __device__ __half2 __heq2(const __half2 a, const __half2 b);\n unsigned int __heq2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __heq2_mask(const __half2 a, const __half2 b);\n __half2 __hequ2(const __half2 a, const __half2 b);\n __device__ __half2 __hequ2(const __half2 a, const __half2 b);\n unsigned int __hequ2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hequ2_mask(const __half2 a, const __half2 b);\n __half2 __hge2(const __half2 a, const __half2 b);\n __device__ __half2 __hge2(const __half2 a, const __half2 b);\n unsigned int __hge2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hge2_mask(const __half2 a, const __half2 b);\n __half2 __hgeu2(const __half2 a, const __half2 b);\n __device__ __half2 __hgeu2(const __half2 a, const __half2 b);\n unsigned int __hgeu2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hgeu2_mask(const __half2 a, const __half2 b);\n __half2 __hgt2(const __half2 a, const __half2 b);\n __device__ __half2 __hgt2(const __half2 a, const __half2 b);\n unsigned int __hgt2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hgt2_mask(const __half2 a, const __half2 b);\n __half2 __hgtu2(const __half2 a, const __half2 b);\n __device__ __half2 __hgtu2(const __half2 a, const __half2 b);\n unsigned int __hgtu2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hgtu2_mask(const __half2 a, const __half2 b);\n __half2 __hisnan2(const __half2 a);\n __device__ __half2 __hisnan2(const __half2 a);\n __half2 __hle2(const __half2 a, const __half2 b);\n __device__ __half2 __hle2(const __half2 a, const __half2 b);\n unsigned int __hle2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hle2_mask(const __half2 a, const __half2 b);\n __half2 __hleu2(const __half2 a, const __half2 b);\n __device__ __half2 __hleu2(const __half2 a, const __half2 b);\n unsigned int __hleu2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hleu2_mask(const __half2 a, const __half2 b);\n __half2 __hlt2(const __half2 a, const __half2 b);\n __device__ __half2 __hlt2(const __half2 a, const __half2 b);\n unsigned int __hlt2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hlt2_mask(const __half2 a, const __half2 b);\n __half2 __hltu2(const __half2 a, const __half2 b);\n __device__ __half2 __hltu2(const __half2 a, const __half2 b);\n unsigned int __hltu2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hltu2_mask(const __half2 a, const __half2 b);\n __half2 __hmax2(const __half2 a, const __half2 b);\n __device__ __half2 __hmax2(const __half2 a, const __half2 b);\n __half2 __hmax2_nan(const __half2 a, const __half2 b);\n __device__ __half2 __hmax2_nan(const __half2 a, const __half2 b);\n __half2 __hmin2(const __half2 a, const __half2 b);\n __device__ __half2 __hmin2(const __half2 a, const __half2 b);\n __half2 __hmin2_nan(const __half2 a, const __half2 b);\n __device__ __half2 __hmin2_nan(const __half2 a, const __half2 b);\n __half2 __hne2(const __half2 a, const __half2 b);\n __device__ __half2 __hne2(const __half2 a, const __half2 b);\n unsigned int __hne2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hne2_mask(const __half2 a, const __half2 b);\n __half2 __hneu2(const __half2 a, const __half2 b);\n __device__ __half2 __hneu2(const __half2 a, const __half2 b);\n unsigned int __hneu2_mask(const __half2 a, const __half2 b);\n __device__ unsigned int __hneu2_mask(const __half2 a, const __half2 b);\n __device__ __half2 h2ceil(const __half2 h);\n __device__ __half2 h2cos(const __half2 a);\n __device__ __half2 h2exp(const __half2 a);\n __device__ __half2 h2exp10(const __half2 a);\n __device__ __half2 h2exp2(const __half2 a);\n __device__ __half2 h2floor(const __half2 h);\n __device__ __half2 h2log(const __half2 a);\n __device__ __half2 h2log10(const __half2 a);\n __device__ __half2 h2log2(const __half2 a);\n __device__ __half2 h2rcp(const __half2 a);\n __device__ __half2 h2rint(const __half2 h);\n __device__ __half2 h2rsqrt(const __half2 a);\n __device__ __half2 h2sin(const __half2 a);\n __device__ __half2 h2sqrt(const __half2 a);\n __device__ __half2 h2tanh(const __half2 a);\n __device__ __half2 h2tanh_approx(const __half2 a);\n __device__ __half2 h2trunc(const __half2 h);\n</code></pre>"},{"location":"manual/api-math/#5-bfloat16-precision-intrinsics","title":"5. Bfloat16 Precision Intrinsics","text":"<pre><code> typedef __nv_bfloat16 nv_bfloat16;\n typedef __nv_bfloat162 nv_bfloat162;\n #define CUDART_INF_BF16\n #define CUDART_MAX_NORMAL_BF16\n #define CUDART_MIN_DENORM_BF16\n #define CUDART_NAN_BF16\n #define CUDART_NEG_ZERO_BF16\n #define CUDART_ONE_BF16\n #define CUDART_ZERO_BF16\n __nv_bfloat16 __habs(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 __habs(const __nv_bfloat16 a);\n __nv_bfloat16 __hadd(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hadd(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hadd_rn(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hadd_rn(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hadd_sat(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hadd_sat(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hdiv(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hdiv(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hfma(const __nv_bfloat16 a, const __nv_bfloat16 b, const __nv_bfloat16 c);\n __device__ __nv_bfloat16 __hfma_relu(const __nv_bfloat16 a, const __nv_bfloat16 b, const __nv_bfloat16 c);\n __device__ __nv_bfloat16 __hfma_sat(const __nv_bfloat16 a, const __nv_bfloat16 b, const __nv_bfloat16 c);\n __nv_bfloat16 __hmul(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hmul(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hmul_rn(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hmul_rn(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hmul_sat(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hmul_sat(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hneg(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 __hneg(const __nv_bfloat16 a);\n __nv_bfloat16 __hsub(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hsub(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hsub_rn(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hsub_rn(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hsub_sat(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hsub_sat(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 atomicAdd(__nv_bfloat16 *const address, const __nv_bfloat16 val);\n bool __heq(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __heq(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hequ(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hequ(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hge(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hge(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hgeu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hgeu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hgt(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hgt(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hgtu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hgtu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n int __hisinf(const __nv_bfloat16 a);\n __device__ int __hisinf(const __nv_bfloat16 a);\n bool __hisnan(const __nv_bfloat16 a);\n __device__ bool __hisnan(const __nv_bfloat16 a);\n bool __hle(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hle(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hleu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hleu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hlt(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hlt(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hltu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hltu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hmax(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hmax(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hmax_nan(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hmax_nan(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hmin(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hmin(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __hmin_nan(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 __hmin_nan(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hne(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hne(const __nv_bfloat16 a, const __nv_bfloat16 b);\n bool __hneu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ bool __hneu(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat16 hceil(const __nv_bfloat16 h);\n __device__ __nv_bfloat16 hcos(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hexp(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hexp10(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hexp2(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hfloor(const __nv_bfloat16 h);\n __device__ __nv_bfloat16 hlog(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hlog10(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hlog2(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hrcp(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hrint(const __nv_bfloat16 h);\n __device__ __nv_bfloat16 hrsqrt(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hsin(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 hsqrt(const __nv_bfloat16 a);\n-__device__ __nv_bfloat16 htanh(const __nv_bfloat16 a);\n-__device__ __nv_bfloat16 htanh_approx(const __nv_bfloat16 a);\n __device__ __nv_bfloat16 htrunc(const __nv_bfloat16 h);\n float2 __bfloat1622float2(const __nv_bfloat162 a);\n __device__ float2 __bfloat1622float2(const __nv_bfloat162 a);\n __nv_bfloat162 __bfloat162bfloat162(const __nv_bfloat16 a);\n __device__ __nv_bfloat162 __bfloat162bfloat162(const __nv_bfloat16 a);\n-signed char __bfloat162char_rz(const __nv_bfloat16 h);\n-__device__ signed char __bfloat162char_rz(const __nv_bfloat16 h);\n float __bfloat162float(const __nv_bfloat16 a);\n __device__ float __bfloat162float(const __nv_bfloat16 a);\n __device__ int __bfloat162int_rd(const __nv_bfloat16 h);\n __device__ int __bfloat162int_rn(const __nv_bfloat16 h);\n __device__ int __bfloat162int_ru(const __nv_bfloat16 h);\n int __bfloat162int_rz(const __nv_bfloat16 h);\n __device__ int __bfloat162int_rz(const __nv_bfloat16 h);\n __device__ long long int __bfloat162ll_rd(const __nv_bfloat16 h);\n __device__ long long int __bfloat162ll_rn(const __nv_bfloat16 h);\n __device__ long long int __bfloat162ll_ru(const __nv_bfloat16 h);\n long long int __bfloat162ll_rz(const __nv_bfloat16 h);\n __device__ long long int __bfloat162ll_rz(const __nv_bfloat16 h);\n __device__ short int __bfloat162short_rd(const __nv_bfloat16 h);\n __device__ short int __bfloat162short_rn(const __nv_bfloat16 h);\n __device__ short int __bfloat162short_ru(const __nv_bfloat16 h);\n short int __bfloat162short_rz(const __nv_bfloat16 h);\n __device__ short int __bfloat162short_rz(const __nv_bfloat16 h);\n unsigned char __bfloat162uchar_rz(const __nv_bfloat16 h);\n __device__ unsigned char __bfloat162uchar_rz(const __nv_bfloat16 h);\n __device__ unsigned int __bfloat162uint_rd(const __nv_bfloat16 h);\n __device__ unsigned int __bfloat162uint_rn(const __nv_bfloat16 h);\n __device__ unsigned int __bfloat162uint_ru(const __nv_bfloat16 h);\n unsigned int __bfloat162uint_rz(const __nv_bfloat16 h);\n __device__ unsigned int __bfloat162uint_rz(const __nv_bfloat16 h);\n __device__ unsigned long long int __bfloat162ull_rd(const __nv_bfloat16 h);\n __device__ unsigned long long int __bfloat162ull_rn(const __nv_bfloat16 h);\n __device__ unsigned long long int __bfloat162ull_ru(const __nv_bfloat16 h);\n unsigned long long int __bfloat162ull_rz(const __nv_bfloat16 h);\n __device__ unsigned long long int __bfloat162ull_rz(const __nv_bfloat16 h);\n __device__ unsigned short int __bfloat162ushort_rd(const __nv_bfloat16 h);\n __device__ unsigned short int __bfloat162ushort_rn(const __nv_bfloat16 h);\n __device__ unsigned short int __bfloat162ushort_ru(const __nv_bfloat16 h);\n unsigned short int __bfloat162ushort_rz(const __nv_bfloat16 h);\n __device__ unsigned short int __bfloat162ushort_rz(const __nv_bfloat16 h);\n short int __bfloat16_as_short(const __nv_bfloat16 h);\n __device__ short int __bfloat16_as_short(const __nv_bfloat16 h);\n unsigned short int __bfloat16_as_ushort(const __nv_bfloat16 h);\n __device__ unsigned short int __bfloat16_as_ushort(const __nv_bfloat16 h);\n __nv_bfloat16 __double2bfloat16(const double a);\n __device__ __nv_bfloat16 __double2bfloat16(const double a);\n __nv_bfloat162 __float22bfloat162_rn(const float2 a);\n __device__ __nv_bfloat162 __float22bfloat162_rn(const float2 a);\n __nv_bfloat16 __float2bfloat16(const float a);\n __device__ __nv_bfloat16 __float2bfloat16(const float a);\n __nv_bfloat162 __float2bfloat162_rn(const float a);\n __device__ __nv_bfloat162 __float2bfloat162_rn(const float a);\n __nv_bfloat16 __float2bfloat16_rd(const float a);\n __device__ __nv_bfloat16 __float2bfloat16_rd(const float a);\n __nv_bfloat16 __float2bfloat16_rn(const float a);\n __device__ __nv_bfloat16 __float2bfloat16_rn(const float a);\n __nv_bfloat16 __float2bfloat16_ru(const float a);\n __device__ __nv_bfloat16 __float2bfloat16_ru(const float a);\n __nv_bfloat16 __float2bfloat16_rz(const float a);\n __device__ __nv_bfloat16 __float2bfloat16_rz(const float a);\n __nv_bfloat162 __floats2bfloat162_rn(const float a, const float b);\n __device__ __nv_bfloat162 __floats2bfloat162_rn(const float a, const float b);\n __nv_bfloat162 __halves2bfloat162(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __device__ __nv_bfloat162 __halves2bfloat162(const __nv_bfloat16 a, const __nv_bfloat16 b);\n __nv_bfloat16 __high2bfloat16(const __nv_bfloat162 a);\n __device__ __nv_bfloat16 __high2bfloat16(const __nv_bfloat162 a);\n __nv_bfloat162 __high2bfloat162(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 __high2bfloat162(const __nv_bfloat162 a);\n float __high2float(const __nv_bfloat162 a);\n __device__ float __high2float(const __nv_bfloat162 a);\n __nv_bfloat162 __highs2bfloat162(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __highs2bfloat162(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat16 __int2bfloat16_rd(const int i);\n __nv_bfloat16 __int2bfloat16_rn(const int i);\n __device__ __nv_bfloat16 __int2bfloat16_rn(const int i);\n __device__ __nv_bfloat16 __int2bfloat16_ru(const int i);\n __device__ __nv_bfloat16 __int2bfloat16_rz(const int i);\n __device__ __nv_bfloat162 __ldca(const __nv_bfloat162 *const ptr);\n __device__ __nv_bfloat16 __ldca(const __nv_bfloat16 *const ptr);\n __device__ __nv_bfloat16 __ldcg(const __nv_bfloat16 *const ptr);\n __device__ __nv_bfloat162 __ldcg(const __nv_bfloat162 *const ptr);\n __device__ __nv_bfloat162 __ldcs(const __nv_bfloat162 *const ptr);\n __device__ __nv_bfloat16 __ldcs(const __nv_bfloat16 *const ptr);\n __device__ __nv_bfloat16 __ldcv(const __nv_bfloat16 *const ptr);\n __device__ __nv_bfloat162 __ldcv(const __nv_bfloat162 *const ptr);\n __device__ __nv_bfloat162 __ldg(const __nv_bfloat162 *const ptr);\n __device__ __nv_bfloat16 __ldg(const __nv_bfloat16 *const ptr);\n __device__ __nv_bfloat162 __ldlu(const __nv_bfloat162 *const ptr);\n __device__ __nv_bfloat16 __ldlu(const __nv_bfloat16 *const ptr);\n __device__ __nv_bfloat16 __ll2bfloat16_rd(const long long int i);\n __nv_bfloat16 __ll2bfloat16_rn(const long long int i);\n __device__ __nv_bfloat16 __ll2bfloat16_rn(const long long int i);\n __device__ __nv_bfloat16 __ll2bfloat16_ru(const long long int i);\n __device__ __nv_bfloat16 __ll2bfloat16_rz(const long long int i);\n __nv_bfloat16 __low2bfloat16(const __nv_bfloat162 a);\n __device__ __nv_bfloat16 __low2bfloat16(const __nv_bfloat162 a);\n __nv_bfloat162 __low2bfloat162(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 __low2bfloat162(const __nv_bfloat162 a);\n float __low2float(const __nv_bfloat162 a);\n __device__ float __low2float(const __nv_bfloat162 a);\n __nv_bfloat162 __lowhigh2highlow(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 __lowhigh2highlow(const __nv_bfloat162 a);\n __nv_bfloat162 __lows2bfloat162(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __lows2bfloat162(const __nv_bfloat162 a, const __nv_bfloat162 b);\n-__device__ __nv_bfloat162 __shfl_down_sync(const unsigned int mask, const __nv_bfloat162 var, const unsigned int delta, const int width=warpSize);\n-__device__ __nv_bfloat16 __shfl_down_sync(const unsigned int mask, const __nv_bfloat16 var, const unsigned int delta, const int width=warpSize);\n-__device__ __nv_bfloat162 __shfl_sync(const unsigned int mask, const __nv_bfloat162 var, const int srcLane, const int width=warpSize);\n-__device__ __nv_bfloat16 __shfl_sync(const unsigned int mask, const __nv_bfloat16 var, const int srcLane, const int width=warpSize);\n-__device__ __nv_bfloat16 __shfl_up_sync(const unsigned int mask, const __nv_bfloat16 var, const unsigned int delta, const int width=warpSize);\n-__device__ __nv_bfloat162 __shfl_up_sync(const unsigned int mask, const __nv_bfloat162 var, const unsigned int delta, const int width=warpSize);\n-__device__ __nv_bfloat16 __shfl_xor_sync(const unsigned int mask, const __nv_bfloat16 var, const int laneMask, const int width=warpSize);\n-__device__ __nv_bfloat162 __shfl_xor_sync(const unsigned int mask, const __nv_bfloat162 var, const int laneMask, const int width=warpSize);\n __device__ __nv_bfloat16 __short2bfloat16_rd(const short int i);\n __nv_bfloat16 __short2bfloat16_rn(const short int i);\n __device__ __nv_bfloat16 __short2bfloat16_rn(const short int i);\n __device__ __nv_bfloat16 __short2bfloat16_ru(const short int i);\n __device__ __nv_bfloat16 __short2bfloat16_rz(const short int i);\n __nv_bfloat16 __short_as_bfloat16(const short int i);\n __device__ __nv_bfloat16 __short_as_bfloat16(const short int i);\n __device__ void __stcg(__nv_bfloat16 *const ptr, const __nv_bfloat16 value);\n __device__ void __stcg(__nv_bfloat162 *const ptr, const __nv_bfloat162 value);\n __device__ void __stcs(__nv_bfloat16 *const ptr, const __nv_bfloat16 value);\n __device__ void __stcs(__nv_bfloat162 *const ptr, const __nv_bfloat162 value);\n __device__ void __stwb(__nv_bfloat16 *const ptr, const __nv_bfloat16 value);\n __device__ void __stwb(__nv_bfloat162 *const ptr, const __nv_bfloat162 value);\n __device__ void __stwt(__nv_bfloat162 *const ptr, const __nv_bfloat162 value);\n __device__ void __stwt(__nv_bfloat16 *const ptr, const __nv_bfloat16 value);\n __device__ __nv_bfloat16 __uint2bfloat16_rd(const unsigned int i);\n __nv_bfloat16 __uint2bfloat16_rn(const unsigned int i);\n __device__ __nv_bfloat16 __uint2bfloat16_rn(const unsigned int i);\n __device__ __nv_bfloat16 __uint2bfloat16_ru(const unsigned int i);\n __device__ __nv_bfloat16 __uint2bfloat16_rz(const unsigned int i);\n __device__ __nv_bfloat16 __ull2bfloat16_rd(const unsigned long long int i);\n __nv_bfloat16 __ull2bfloat16_rn(const unsigned long long int i);\n __device__ __nv_bfloat16 __ull2bfloat16_rn(const unsigned long long int i);\n __device__ __nv_bfloat16 __ull2bfloat16_ru(const unsigned long long int i);\n __device__ __nv_bfloat16 __ull2bfloat16_rz(const unsigned long long int i);\n __device__ __nv_bfloat16 __ushort2bfloat16_rd(const unsigned short int i);\n __nv_bfloat16 __ushort2bfloat16_rn(const unsigned short int i);\n __device__ __nv_bfloat16 __ushort2bfloat16_rn(const unsigned short int i);\n __device__ __nv_bfloat16 __ushort2bfloat16_ru(const unsigned short int i);\n __device__ __nv_bfloat16 __ushort2bfloat16_rz(const unsigned short int i);\n __nv_bfloat16 __ushort_as_bfloat16(const unsigned short int i);\n __device__ __nv_bfloat16 __ushort_as_bfloat16(const unsigned short int i);\n __nv_bfloat162 make_bfloat162(const __nv_bfloat16 x, const __nv_bfloat16 y);\n __device__ __nv_bfloat162 make_bfloat162(const __nv_bfloat16 x, const __nv_bfloat16 y);\n __nv_bfloat162 __h2div(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __h2div(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __habs2(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 __habs2(const __nv_bfloat162 a);\n __nv_bfloat162 __hadd2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hadd2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hadd2_rn(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hadd2_rn(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hadd2_sat(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hadd2_sat(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hcmadd(const __nv_bfloat162 a, const __nv_bfloat162 b, const __nv_bfloat162 c);\n __device__ __nv_bfloat162 __hfma2(const __nv_bfloat162 a, const __nv_bfloat162 b, const __nv_bfloat162 c);\n __device__ __nv_bfloat162 __hfma2_relu(const __nv_bfloat162 a, const __nv_bfloat162 b, const __nv_bfloat162 c);\n __device__ __nv_bfloat162 __hfma2_sat(const __nv_bfloat162 a, const __nv_bfloat162 b, const __nv_bfloat162 c);\n __nv_bfloat162 __hmul2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hmul2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hmul2_rn(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hmul2_rn(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hmul2_sat(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hmul2_sat(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hneg2(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 __hneg2(const __nv_bfloat162 a);\n __nv_bfloat162 __hsub2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hsub2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hsub2_rn(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hsub2_rn(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hsub2_sat(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hsub2_sat(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 atomicAdd(__nv_bfloat162 *const address, const __nv_bfloat162 val);\n bool __hbeq2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbeq2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbequ2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbequ2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbge2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbge2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbgeu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbgeu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbgt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbgt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbgtu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbgtu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hble2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hble2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbleu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbleu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hblt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hblt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbltu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbltu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbne2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbne2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n bool __hbneu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ bool __hbneu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __heq2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __heq2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __heq2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __heq2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hequ2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hequ2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hequ2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hequ2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hge2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hge2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hge2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hge2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hgeu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hgeu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hgeu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hgeu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hgt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hgt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hgt2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hgt2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hgtu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hgtu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hgtu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hgtu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hisnan2(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 __hisnan2(const __nv_bfloat162 a);\n __nv_bfloat162 __hle2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hle2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hle2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hle2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hleu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hleu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hleu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hleu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hlt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hlt2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hlt2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hlt2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hltu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hltu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hltu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hltu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hmax2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hmax2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hmax2_nan(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hmax2_nan(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hmin2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hmin2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hmin2_nan(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hmin2_nan(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hne2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hne2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hne2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hne2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __nv_bfloat162 __hneu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 __hneu2(const __nv_bfloat162 a, const __nv_bfloat162 b);\n unsigned int __hneu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ unsigned int __hneu2_mask(const __nv_bfloat162 a, const __nv_bfloat162 b);\n __device__ __nv_bfloat162 h2ceil(const __nv_bfloat162 h);\n __device__ __nv_bfloat162 h2cos(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2exp(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2exp10(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2exp2(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2floor(const __nv_bfloat162 h);\n __device__ __nv_bfloat162 h2log(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2log10(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2log2(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2rcp(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2rint(const __nv_bfloat162 h);\n __device__ __nv_bfloat162 h2rsqrt(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2sin(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2sqrt(const __nv_bfloat162 a);\n-__device__ __nv_bfloat162 h2tanh(const __nv_bfloat162 a);\n-__device__ __nv_bfloat162 h2tanh_approx(const __nv_bfloat162 a);\n __device__ __nv_bfloat162 h2trunc(const __nv_bfloat162 h);\n</code></pre>"},{"location":"manual/api-math/#6-single-precision-mathematical-functions","title":"6. Single Precision Mathematical Functions","text":"<pre><code> __device__ float acosf(float x);\n __device__ float acoshf(float x);\n __device__ float asinf(float x);\n __device__ float asinhf(float x);\n __device__ float atan2f(float y, float x);\n __device__ float atanf(float x);\n __device__ float atanhf(float x);\n __device__ float cbrtf(float x);\n __device__ float ceilf(float x);\n __device__ float copysignf(float x, float y);\n __device__ float cosf(float x);\n __device__ float coshf(float x);\n __device__ float cospif(float x);\n-__device__ float cyl_bessel_i0f(float x);\n-__device__ float cyl_bessel_i1f(float x);\n __device__ float erfcf(float x);\n __device__ float erfcinvf(float x);\n-__device__ float erfcxf(float x);\n __device__ float erff(float x);\n __device__ float erfinvf(float x);\n __device__ float exp10f(float x);\n __device__ float exp2f(float x);\n __device__ float expf(float x);\n __device__ float expm1f(float x);\n __device__ float fabsf(float x);\n __device__ float fdimf(float x, float y);\n __device__ float fdividef(float x, float y);\n __device__ float floorf(float x);\n __device__ float fmaf(float x, float y, float z);\n __device__ float fmaxf(float x, float y);\n __device__ float fminf(float x, float y);\n __device__ float fmodf(float x, float y);\n __device__ float frexpf(float x, int *nptr);\n __device__ float hypotf(float x, float y);\n __device__ int ilogbf(float x);\n __device__ bool isfinite(float a);\n __device__ bool isinf(float a);\n __device__ bool isnan(float a);\n-__device__ float j0f(float x);\n-__device__ float j1f(float x);\n-__device__ float jnf(int n, float x);\n __device__ float ldexpf(float x, int exp);\n __device__ float lgammaf(float x);\n __device__ long long int llrintf(float x);\n __device__ long long int llroundf(float x);\n __device__ float log10f(float x);\n __device__ float log1pf(float x);\n __device__ float log2f(float x);\n __device__ float logbf(float x);\n __device__ float logf(float x);\n __device__ long int lrintf(float x);\n __device__ long int lroundf(float x);\n __device__ float max(const float a, const float b);\n __device__ float min(const float a, const float b);\n __device__ float modff(float x, float *iptr);\n __device__ float nanf(const char *tagp);\n __device__ float nearbyintf(float x);\n __device__ float nextafterf(float x, float y);\n __device__ float norm3df(float a, float b, float c);\n __device__ float norm4df(float a, float b, float c, float d);\n __device__ float normcdff(float x);\n-__device__ float normcdfinvf(float x);\n __device__ float normf(int dim, float const *p);\n __device__ float powf(float x, float y);\n __device__ float rcbrtf(float x);\n __device__ float remainderf(float x, float y);\n __device__ float remquof(float x, float y, int *quo);\n __device__ float rhypotf(float x, float y);\n __device__ float rintf(float x);\n __device__ float rnorm3df(float a, float b, float c);\n __device__ float rnorm4df(float a, float b, float c, float d);\n __device__ float rnormf(int dim, float const *p);\n __device__ float roundf(float x);\n __device__ float rsqrtf(float x);\n __device__ float scalblnf(float x, long int n);\n __device__ float scalbnf(float x, int n);\n __device__ bool signbit(float a);\n __device__ void sincosf(float x, float *sptr, float *cptr);\n __device__ void sincospif(float x, float *sptr, float *cptr);\n __device__ float sinf(float x);\n __device__ float sinhf(float x);\n __device__ float sinpif(float x);\n __device__ float sqrtf(float x);\n __device__ float tanf(float x);\n __device__ float tanhf(float x);\n __device__ float tgammaf(float x);\n __device__ float truncf(float x);\n-__device__ float y0f(float x);\n-__device__ float y1f(float x);\n-__device__ float ynf(int n, float x);\n</code></pre>"},{"location":"manual/api-math/#7-single-precision-intrinsics","title":"7. Single Precision Intrinsics","text":"<pre><code> __device__ float __cosf(float x);\n __device__ float __exp10f(float x);\n __device__ float __expf(float x);\n __device__ float2 __fadd2_rd(float2 x, float2 y);\n __device__ float2 __fadd2_rn(float2 x, float2 y);\n __device__ float2 __fadd2_ru(float2 x, float2 y);\n __device__ float2 __fadd2_rz(float2 x, float2 y);\n __device__ float __fadd_rd(float x, float y);\n __device__ float __fadd_rn(float x, float y);\n __device__ float __fadd_ru(float x, float y);\n __device__ float __fadd_rz(float x, float y);\n __device__ float __fdiv_rd(float x, float y);\n __device__ float __fdiv_rn(float x, float y);\n __device__ float __fdiv_ru(float x, float y);\n __device__ float __fdiv_rz(float x, float y);\n __device__ float __fdividef(float x, float y);\n __device__ float2 __ffma2_rd(float2 x, float2 y, float2 z);\n __device__ float2 __ffma2_rn(float2 x, float2 y, float2 z);\n __device__ float2 __ffma2_ru(float2 x, float2 y, float2 z);\n __device__ float2 __ffma2_rz(float2 x, float2 y, float2 z);\n-__device__ float __fmaf_ieee_rd(float x, float y, float z);\n __device__ float __fmaf_ieee_rn(float x, float y, float z);\n-__device__ float __fmaf_ieee_ru(float x, float y, float z);\n-__device__ float __fmaf_ieee_rz(float x, float y, float z);\n __device__ float __fmaf_rd(float x, float y, float z);\n __device__ float __fmaf_rn(float x, float y, float z);\n __device__ float __fmaf_ru(float x, float y, float z);\n __device__ float __fmaf_rz(float x, float y, float z);\n __device__ float2 __fmul2_rd(float2 x, float2 y);\n __device__ float2 __fmul2_rn(float2 x, float2 y);\n __device__ float2 __fmul2_ru(float2 x, float2 y);\n __device__ float2 __fmul2_rz(float2 x, float2 y);\n __device__ float __fmul_rd(float x, float y);\n __device__ float __fmul_rn(float x, float y);\n __device__ float __fmul_ru(float x, float y);\n __device__ float __fmul_rz(float x, float y);\n __device__ float __frcp_rd(float x);\n __device__ float __frcp_rn(float x);\n __device__ float __frcp_ru(float x);\n __device__ float __frcp_rz(float x);\n __device__ float __frsqrt_rn(float x);\n __device__ float __fsqrt_rd(float x);\n __device__ float __fsqrt_rn(float x);\n __device__ float __fsqrt_ru(float x);\n __device__ float __fsqrt_rz(float x);\n __device__ float __fsub_rd(float x, float y);\n __device__ float __fsub_rn(float x, float y);\n __device__ float __fsub_ru(float x, float y);\n __device__ float __fsub_rz(float x, float y);\n __device__ float __log10f(float x);\n __device__ float __log2f(float x);\n __device__ float __logf(float x);\n __device__ float __powf(float x, float y);\n __device__ float __saturatef(float x);\n __device__ void __sincosf(float x, float *sptr, float *cptr);\n __device__ float __sinf(float x);\n __device__ float __tanf(float x);\n __device__ float __tanhf(float x);\n</code></pre>"},{"location":"manual/api-math/#8-double-precision-mathematical-functions","title":"8. Double Precision Mathematical Functions","text":"<pre><code> __device__ double acos(double x);\n __device__ double acosh(double x);\n __device__ double asin(double x);\n __device__ double asinh(double x);\n __device__ double atan(double x);\n __device__ double atan2(double y, double x);\n __device__ double atanh(double x);\n __device__ double cbrt(double x);\n __device__ double ceil(double x);\n __device__ double copysign(double x, double y);\n __device__ double cos(double x);\n __device__ double cosh(double x);\n __device__ double cospi(double x);\n-__device__ double cyl_bessel_i0(double x);\n-__device__ double cyl_bessel_i1(double x);\n __device__ double erf(double x);\n __device__ double erfc(double x);\n __device__ double erfcinv(double x);\n-__device__ double erfcx(double x);\n __device__ double erfinv(double x);\n __device__ double exp(double x);\n __device__ double exp10(double x);\n __device__ double exp2(double x);\n __device__ double expm1(double x);\n __device__ double fabs(double x);\n __device__ double fdim(double x, double y);\n __device__ double floor(double x);\n __device__ double fma(double x, double y, double z);\n __device__ double fmax(double, double);\n __device__ double fmin(double x, double y);\n __device__ double fmod(double x, double y);\n __device__ double frexp(double x, int *nptr);\n __device__ double hypot(double x, double y);\n __device__ int ilogb(double x);\n __device__ bool isfinite(double a);\n __device__ bool isinf(double a);\n __device__ bool isnan(double a);\n-__device__ double j0(double x);\n-__device__ double j1(double x);\n-__device__ double jn(int n, double x);\n __device__ double ldexp(double x, int exp);\n __device__ double lgamma(double x);\n __device__ long long int llrint(double x);\n __device__ long long int llround(double x);\n __device__ double log(double x);\n __device__ double log10(double x);\n __device__ double log1p(double x);\n __device__ double log2(double x);\n __device__ double logb(double x);\n __device__ long int lrint(double x);\n __device__ long int lround(double x);\n __device__ double max(const float a, const double b);\n __device__ double max(const double a, const float b);\n __device__ double max(const double a, const double b);\n __device__ double min(const float a, const double b);\n __device__ double min(const double a, const double b);\n __device__ double min(const double a, const float b);\n __device__ double modf(double x, double *iptr);\n __device__ double nan(const char *tagp);\n __device__ double nearbyint(double x);\n __device__ double nextafter(double x, double y);\n __device__ double norm(int dim, double const *p);\n __device__ double norm3d(double a, double b, double c);\n __device__ double norm4d(double a, double b, double c, double d);\n __device__ double normcdf(double x);\n-__device__ double normcdfinv(double x);\n __device__ double pow(double x, double y);\n __device__ double rcbrt(double x);\n __device__ double remainder(double x, double y);\n __device__ double remquo(double x, double y, int *quo);\n __device__ double rhypot(double x, double y);\n __device__ double rint(double x);\n __device__ double rnorm(int dim, double const *p);\n __device__ double rnorm3d(double a, double b, double c);\n __device__ double rnorm4d(double a, double b, double c, double d);\n __device__ double round(double x);\n __device__ double rsqrt(double x);\n __device__ double scalbln(double x, long int n);\n __device__ double scalbn(double x, int n);\n __device__ bool signbit(double a);\n __device__ double sin(double x);\n __device__ void sincos(double x, double *sptr, double *cptr);\n __device__ void sincospi(double x, double *sptr, double *cptr);\n __device__ double sinh(double x);\n __device__ double sinpi(double x);\n __device__ double sqrt(double x);\n __device__ double tan(double x);\n __device__ double tanh(double x);\n __device__ double tgamma(double x);\n __device__ double trunc(double x);\n-__device__ double y0(double x);\n-__device__ double y1(double x);\n-__device__ double yn(int n, double x);\n</code></pre>"},{"location":"manual/api-math/#9-double-precision-intrinsics","title":"9. Double Precision Intrinsics","text":"<pre><code> __device__ double __dadd_rd(double x, double y);\n __device__ double __dadd_rn(double x, double y);\n __device__ double __dadd_ru(double x, double y);\n __device__ double __dadd_rz(double x, double y);\n __device__ double __ddiv_rd(double x, double y);\n __device__ double __ddiv_rn(double x, double y);\n __device__ double __ddiv_ru(double x, double y);\n __device__ double __ddiv_rz(double x, double y);\n __device__ double __dmul_rd(double x, double y);\n __device__ double __dmul_rn(double x, double y);\n __device__ double __dmul_ru(double x, double y);\n __device__ double __dmul_rz(double x, double y);\n __device__ double __drcp_rd(double x);\n __device__ double __drcp_rn(double x);\n __device__ double __drcp_ru(double x);\n __device__ double __drcp_rz(double x);\n __device__ double __dsqrt_rd(double x);\n __device__ double __dsqrt_rn(double x);\n __device__ double __dsqrt_ru(double x);\n __device__ double __dsqrt_rz(double x);\n __device__ double __dsub_rd(double x, double y);\n __device__ double __dsub_rn(double x, double y);\n __device__ double __dsub_ru(double x, double y);\n __device__ double __dsub_rz(double x, double y);\n __device__ double __fma_rd(double x, double y, double z);\n __device__ double __fma_rn(double x, double y, double z);\n __device__ double __fma_ru(double x, double y, double z);\n __device__ double __fma_rz(double x, double y, double z);\n</code></pre>"},{"location":"manual/api-math/#10-fp128-quad-precision-mathematical-functions","title":"10. FP128 Quad Precision Mathematical Functions","text":"<pre><code>-128 bit floating point is not supported\n</code></pre>"},{"location":"manual/api-math/#11-type-casting-intrinsics","title":"11. Type Casting Intrinsics","text":"<pre><code> __device__ float __double2float_rd(double x);\n __device__ float __double2float_rn(double x);\n __device__ float __double2float_ru(double x);\n __device__ float __double2float_rz(double x);\n __device__ int __double2hiint(double x);\n __device__ int __double2int_rd(double x);\n __device__ int __double2int_rn(double x);\n __device__ int __double2int_ru(double x);\n __device__ int __double2int_rz(double x);\n __device__ long long int __double2ll_rd(double x);\n __device__ long long int __double2ll_rn(double x);\n __device__ long long int __double2ll_ru(double x);\n __device__ long long int __double2ll_rz(double x);\n __device__ int __double2loint(double x);\n __device__ unsigned int __double2uint_rd(double x);\n __device__ unsigned int __double2uint_rn(double x);\n __device__ unsigned int __double2uint_ru(double x);\n __device__ unsigned int __double2uint_rz(double x);\n __device__ unsigned long long int __double2ull_rd(double x);\n __device__ unsigned long long int __double2ull_rn(double x);\n __device__ unsigned long long int __double2ull_ru(double x);\n __device__ unsigned long long int __double2ull_rz(double x);\n __device__ long long int __double_as_longlong(double x);\n __device__ int __float2int_rd(float x);\n __device__ int __float2int_rn(float x);\n __device__ int __float2int_ru(float);\n __device__ int __float2int_rz(float x);\n __device__ long long int __float2ll_rd(float x);\n __device__ long long int __float2ll_rn(float x);\n __device__ long long int __float2ll_ru(float x);\n __device__ long long int __float2ll_rz(float x);\n __device__ unsigned int __float2uint_rd(float x);\n __device__ unsigned int __float2uint_rn(float x);\n __device__ unsigned int __float2uint_ru(float x);\n __device__ unsigned int __float2uint_rz(float x);\n __device__ unsigned long long int __float2ull_rd(float x);\n __device__ unsigned long long int __float2ull_rn(float x);\n __device__ unsigned long long int __float2ull_ru(float x);\n __device__ unsigned long long int __float2ull_rz(float x);\n __device__ int __float_as_int(float x);\n __device__ unsigned int __float_as_uint(float x);\n __device__ double __hiloint2double(int hi, int lo);\n __device__ double __int2double_rn(int x);\n __device__ float __int2float_rd(int x);\n __device__ float __int2float_rn(int x);\n __device__ float __int2float_ru(int x);\n __device__ float __int2float_rz(int x);\n __device__ float __int_as_float(int x);\n __device__ double __ll2double_rd(long long int x);\n __device__ double __ll2double_rn(long long int x);\n __device__ double __ll2double_ru(long long int x);\n __device__ double __ll2double_rz(long long int x);\n __device__ float __ll2float_rd(long long int x);\n __device__ float __ll2float_rn(long long int x);\n __device__ float __ll2float_ru(long long int x);\n __device__ float __ll2float_rz(long long int x);\n __device__ double __longlong_as_double(long long int x);\n __device__ double __uint2double_rn(unsigned int x);\n __device__ float __uint2float_rd(unsigned int x);\n __device__ float __uint2float_rn(unsigned int x);\n __device__ float __uint2float_ru(unsigned int x);\n __device__ float __uint2float_rz(unsigned int x);\n __device__ float __uint_as_float(unsigned int x);\n __device__ double __ull2double_rd(unsigned long long int x);\n __device__ double __ull2double_rn(unsigned long long int x);\n __device__ double __ull2double_ru(unsigned long long int x);\n __device__ double __ull2double_rz(unsigned long long int x);\n __device__ float __ull2float_rd(unsigned long long int x);\n __device__ float __ull2float_rn(unsigned long long int x);\n __device__ float __ull2float_ru(unsigned long long int x);\n __device__ float __ull2float_rz(unsigned long long int x);\n</code></pre>"},{"location":"manual/api-math/#12-integer-mathematical-functions","title":"12. Integer Mathematical Functions","text":"<pre><code> __device__ long int abs(long int a);\n __device__ int abs(int a);\n __device__ long long int abs(long long int a);\n __device__ long int labs(long int a);\n __device__ long long int llabs(long long int a);\n __device__ long long int llmax(const long long int a, const long long int b);\n __device__ long long int llmin(const long long int a, const long long int b);\n __device__ unsigned long int max(const long int a, const unsigned long int b);\n __device__ unsigned long long int max(const unsigned long long int a, const unsigned long long int b);\n __device__ unsigned int max(const unsigned int a, const int b);\n __device__ unsigned long long int max(const long long int a, const unsigned long long int b);\n __device__ unsigned long int max(const unsigned long int a, const unsigned long int b);\n __device__ long long int max(const long long int a, const long long int b);\n __device__ unsigned long long int max(const unsigned long long int a, const long long int b);\n __device__ unsigned long int max(const unsigned long int a, const long int b);\n __device__ long int max(const long int a, const long int b);\n __device__ int max(const int a, const int b);\n __device__ unsigned int max(const unsigned int a, const unsigned int b);\n __device__ unsigned int max(const int a, const unsigned int b);\n __device__ unsigned long int min(const long int a, const unsigned long int b);\n __device__ unsigned long long int min(const unsigned long long int a, const unsigned long long int b);\n __device__ unsigned long long int min(const unsigned long long int a, const long long int b);\n __device__ int min(const int a, const int b);\n __device__ unsigned int min(const unsigned int a, const int b);\n __device__ unsigned long long int min(const long long int a, const unsigned long long int b);\n __device__ long long int min(const long long int a, const long long int b);\n __device__ unsigned int min(const int a, const unsigned int b);\n __device__ long int min(const long int a, const long int b);\n __device__ unsigned int min(const unsigned int a, const unsigned int b);\n __device__ unsigned long int min(const unsigned long int a, const long int b);\n __device__ unsigned long int min(const unsigned long int a, const unsigned long int b);\n __device__ unsigned long long int ullmax(const unsigned long long int a, const unsigned long long int b);\n __device__ unsigned long long int ullmin(const unsigned long long int a, const unsigned long long int b);\n __device__ unsigned int umax(const unsigned int a, const unsigned int b);\n __device__ unsigned int umin(const unsigned int a, const unsigned int b);\n</code></pre>"},{"location":"manual/api-math/#13-integer-intrinsics","title":"13. Integer Intrinsics","text":"<pre><code> __device__ unsigned int __brev(unsigned int x);\n __device__ unsigned long long int __brevll(unsigned long long int x);\n __device__ unsigned int __byte_perm(unsigned int x, unsigned int y, unsigned int s);\n __device__ int __clz(int x);\n __device__ int __clzll(long long int x);\n __device__ int __dp2a_hi(int srcA, int srcB, int c);\n __device__ unsigned int __dp2a_hi(unsigned int srcA, unsigned int srcB, unsigned int c);\n __device__ unsigned int __dp2a_hi(ushort2 srcA, uchar4 srcB, unsigned int c);\n __device__ int __dp2a_hi(short2 srcA, char4 srcB, int c);\n __device__ unsigned int __dp2a_lo(ushort2 srcA, uchar4 srcB, unsigned int c);\n __device__ int __dp2a_lo(short2 srcA, char4 srcB, int c);\n __device__ unsigned int __dp2a_lo(unsigned int srcA, unsigned int srcB, unsigned int c);\n __device__ int __dp2a_lo(int srcA, int srcB, int c);\n __device__ unsigned int __dp4a(uchar4 srcA, uchar4 srcB, unsigned int c);\n __device__ unsigned int __dp4a(unsigned int srcA, unsigned int srcB, unsigned int c);\n __device__ int __dp4a(int srcA, int srcB, int c);\n __device__ int __dp4a(char4 srcA, char4 srcB, int c);\n __device__ int __ffs(int x);\n __device__ int __ffsll(long long int x);\n __device__ unsigned __fns(unsigned mask, unsigned base, int offset);\n __device__ unsigned int __funnelshift_l(unsigned int lo, unsigned int hi, unsigned int shift);\n __device__ unsigned int __funnelshift_lc(unsigned int lo, unsigned int hi, unsigned int shift);\n __device__ unsigned int __funnelshift_r(unsigned int lo, unsigned int hi, unsigned int shift);\n __device__ unsigned int __funnelshift_rc(unsigned int lo, unsigned int hi, unsigned int shift);\n __device__ int __hadd(int x, int y);\n __device__ int __mul24(int x, int y);\n __device__ long long int __mul64hi(long long int x, long long int y);\n __device__ int __mulhi(int x, int y);\n unsigned short __nv_bswap16(unsigned short x);\n __device__ unsigned short __nv_bswap16(unsigned short x);\n unsigned int __nv_bswap32(unsigned int x);\n __device__ unsigned int __nv_bswap32(unsigned int x);\n unsigned long long __nv_bswap64(unsigned long long x);\n __device__ unsigned long long __nv_bswap64(unsigned long long x);\n __device__ int __popc(unsigned int x);\n __device__ int __popcll(unsigned long long int x);\n __device__ int __rhadd(int x, int y);\n __device__ unsigned int __sad(int x, int y, unsigned int z);\n __device__ unsigned int __uhadd(unsigned int x, unsigned int y);\n __device__ unsigned int __umul24(unsigned int x, unsigned int y);\n __device__ unsigned long long int __umul64hi(unsigned long long int x, unsigned long long int y);\n __device__ unsigned int __umulhi(unsigned int x, unsigned int y);\n __device__ unsigned int __urhadd(unsigned int x, unsigned int y);\n __device__ unsigned int __usad(unsigned int x, unsigned int y, unsigned int z);\n</code></pre>"},{"location":"manual/api-math/#14-simd-intrinsics","title":"14. SIMD Intrinsics","text":"<pre><code> __device__ unsigned int __vabs2(unsigned int a);\n __device__ unsigned int __vabs4(unsigned int a);\n __device__ unsigned int __vabsdiffs2(unsigned int a, unsigned int b);\n __device__ unsigned int __vabsdiffs4(unsigned int a, unsigned int b);\n __device__ unsigned int __vabsdiffu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vabsdiffu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vabsss2(unsigned int a);\n __device__ unsigned int __vabsss4(unsigned int a);\n __device__ unsigned int __vadd2(unsigned int a, unsigned int b);\n __device__ unsigned int __vadd4(unsigned int a, unsigned int b);\n __device__ unsigned int __vaddss2(unsigned int a, unsigned int b);\n __device__ unsigned int __vaddss4(unsigned int a, unsigned int b);\n __device__ unsigned int __vaddus2(unsigned int a, unsigned int b);\n __device__ unsigned int __vaddus4(unsigned int a, unsigned int b);\n __device__ unsigned int __vavgs2(unsigned int a, unsigned int b);\n __device__ unsigned int __vavgs4(unsigned int a, unsigned int b);\n __device__ unsigned int __vavgu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vavgu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpeq2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpeq4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpges2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpges4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpgeu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpgeu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpgts2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpgts4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpgtu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpgtu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmples2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmples4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpleu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpleu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmplts2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmplts4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpltu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpltu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpne2(unsigned int a, unsigned int b);\n __device__ unsigned int __vcmpne4(unsigned int a, unsigned int b);\n __device__ unsigned int __vhaddu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vhaddu4(unsigned int a, unsigned int b);\n unsigned int __viaddmax_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmax_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __viaddmax_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmax_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n int __viaddmax_s32(const int a, const int b, const int c);\n __device__ int __viaddmax_s32(const int a, const int b, const int c);\n int __viaddmax_s32_relu(const int a, const int b, const int c);\n __device__ int __viaddmax_s32_relu(const int a, const int b, const int c);\n unsigned int __viaddmax_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmax_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __viaddmax_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmax_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __viaddmin_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmin_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __viaddmin_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmin_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n int __viaddmin_s32(const int a, const int b, const int c);\n __device__ int __viaddmin_s32(const int a, const int b, const int c);\n int __viaddmin_s32_relu(const int a, const int b, const int c);\n __device__ int __viaddmin_s32_relu(const int a, const int b, const int c);\n unsigned int __viaddmin_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmin_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __viaddmin_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __viaddmin_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __vibmax_s16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n __device__ unsigned int __vibmax_s16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n int __vibmax_s32(const int a, const int b, bool *const pred);\n __device__ int __vibmax_s32(const int a, const int b, bool *const pred);\n unsigned int __vibmax_u16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n __device__ unsigned int __vibmax_u16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n unsigned int __vibmax_u32(const unsigned int a, const unsigned int b, bool *const pred);\n __device__ unsigned int __vibmax_u32(const unsigned int a, const unsigned int b, bool *const pred);\n unsigned int __vibmin_s16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n __device__ unsigned int __vibmin_s16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n int __vibmin_s32(const int a, const int b, bool *const pred);\n __device__ int __vibmin_s32(const int a, const int b, bool *const pred);\n unsigned int __vibmin_u16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n __device__ unsigned int __vibmin_u16x2(const unsigned int a, const unsigned int b, bool *const pred_hi, bool *const pred_lo);\n unsigned int __vibmin_u32(const unsigned int a, const unsigned int b, bool *const pred);\n __device__ unsigned int __vibmin_u32(const unsigned int a, const unsigned int b, bool *const pred);\n unsigned int __vimax3_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimax3_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __vimax3_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimax3_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n int __vimax3_s32(const int a, const int b, const int c);\n __device__ int __vimax3_s32(const int a, const int b, const int c);\n int __vimax3_s32_relu(const int a, const int b, const int c);\n __device__ int __vimax3_s32_relu(const int a, const int b, const int c);\n unsigned int __vimax3_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimax3_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __vimax3_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimax3_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __vimax_s16x2_relu(const unsigned int a, const unsigned int b);\n __device__ unsigned int __vimax_s16x2_relu(const unsigned int a, const unsigned int b);\n int __vimax_s32_relu(const int a, const int b);\n __device__ int __vimax_s32_relu(const int a, const int b);\n unsigned int __vimin3_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimin3_s16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __vimin3_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimin3_s16x2_relu(const unsigned int a, const unsigned int b, const unsigned int c);\n int __vimin3_s32(const int a, const int b, const int c);\n __device__ int __vimin3_s32(const int a, const int b, const int c);\n int __vimin3_s32_relu(const int a, const int b, const int c);\n __device__ int __vimin3_s32_relu(const int a, const int b, const int c);\n unsigned int __vimin3_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimin3_u16x2(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __vimin3_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n __device__ unsigned int __vimin3_u32(const unsigned int a, const unsigned int b, const unsigned int c);\n unsigned int __vimin_s16x2_relu(const unsigned int a, const unsigned int b);\n __device__ unsigned int __vimin_s16x2_relu(const unsigned int a, const unsigned int b);\n int __vimin_s32_relu(const int a, const int b);\n __device__ int __vimin_s32_relu(const int a, const int b);\n __device__ unsigned int __vmaxs2(unsigned int a, unsigned int b);\n __device__ unsigned int __vmaxs4(unsigned int a, unsigned int b);\n __device__ unsigned int __vmaxu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vmaxu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vmins2(unsigned int a, unsigned int b);\n __device__ unsigned int __vmins4(unsigned int a, unsigned int b);\n __device__ unsigned int __vminu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vminu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vneg2(unsigned int a);\n __device__ unsigned int __vneg4(unsigned int a);\n-__device__ unsigned int __vnegss2(unsigned int a);\n-__device__ unsigned int __vnegss4(unsigned int a);\n-__device__ unsigned int __vsads2(unsigned int a, unsigned int b);\n-__device__ unsigned int __vsads4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsadu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsadu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vseteq2(unsigned int a, unsigned int b);\n __device__ unsigned int __vseteq4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetges2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetges4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetgeu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetgeu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetgts2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetgts4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetgtu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetgtu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetles2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetles4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetleu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetleu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetlts2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetlts4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetltu2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetltu4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetne2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsetne4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsub2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsub4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsubss2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsubss4(unsigned int a, unsigned int b);\n __device__ unsigned int __vsubus2(unsigned int a, unsigned int b);\n __device__ unsigned int __vsubus4(unsigned int a, unsigned int b);\n</code></pre>"},{"location":"manual/api-math/#15-structs","title":"15. Structs","text":"<pre><code> struct __half\n struct __half2\n struct __half2_raw\n struct __half_raw\n struct __nv_bfloat16\n struct __nv_bfloat162\n struct __nv_bfloat162_raw\n struct __nv_bfloat16_raw\n-__nv_fp4_e2m1\n-__nv_fp4x2_e2m1\n-__nv_fp4x4_e2m1\n-__nv_fp6_e2m3\n-__nv_fp6_e3m2\n-__nv_fp6x2_e2m3\n-__nv_fp6x2_e3m2\n-__nv_fp6x4_e2m3\n-__nv_fp6x4_e3m2\n-__nv_fp8_e4m3\n-__nv_fp8_e5m2\n-__nv_fp8_e8m0\n-__nv_fp8x2_e4m3\n-__nv_fp8x2_e5m2\n-__nv_fp8x2_e8m0\n-__nv_fp8x4_e4m3\n-__nv_fp8x4_e5m2\n-__nv_fp8x4_e8m0\n</code></pre>"},{"location":"manual/api-runtime/","title":"Runtime API","text":""},{"location":"manual/api-runtime/#61-device-management","title":"6.1. Device Management","text":"<pre><code> __host__ cudaError_t cudaChooseDevice(int* device, const cudaDeviceProp* prop);\n-__host__ cudaError_t cudaDeviceFlushGPUDirectRDMAWrites(cudaFlushGPUDirectRDMAWritesTarget target, cudaFlushGPUDirectRDMAWritesScope scope);\n __host__  cudaError_t cudaDeviceGetAttribute(int* value, cudaDeviceAttr attr, int device);\n __device__ cudaError_t cudaDeviceGetAttribute(int* value, cudaDeviceAttr attr, int device);\n __host__ cudaError_t cudaDeviceGetByPCIBusId(int* device, const char* pciBusId);\n-__host__  cudaError_t cudaDeviceGetCacheConfig(cudaFuncCache ** pCacheConfig);\n-__device__ cudaError_t cudaDeviceGetCacheConfig(cudaFuncCache ** pCacheConfig);\n-__host__ cudaError_t cudaDeviceGetDefaultMemPool(cudaMemPool_t* memPool, int device);\n-__host__ cudaError_t cudaDeviceGetHostAtomicCapabilities(unsigned int* capabilities, const cudaAtomicOperation ** operations, unsigned int count, int device);\n __host__  cudaError_t cudaDeviceGetLimit(size_t* pValue, cudaLimit limit);\n __device__ cudaError_t cudaDeviceGetLimit(size_t* pValue, cudaLimit limit);\n-__host__ cudaError_t cudaDeviceGetMemPool(cudaMemPool_t* memPool, int device);\n-__host__ cudaError_t cudaDeviceGetNvSciSyncAttributes(void* nvSciSyncAttrList, int device, int flags);\n-__host__ cudaError_t cudaDeviceGetP2PAtomicCapabilities(unsigned int* capabilities, const cudaAtomicOperation ** operations, unsigned int count, int srcDevice, int dstDevice);\n __host__ cudaError_t cudaDeviceGetP2PAttribute(int* value, cudaDeviceP2PAttr attr, int srcDevice, int dstDevice);\n __host__ cudaError_t cudaDeviceGetPCIBusId(char* pciBusId, int len, int device);\n __host__ cudaError_t cudaDeviceGetStreamPriorityRange(int* leastPriority, int* greatestPriority);\n-__host__ cudaError_t cudaDeviceGetTexture1DLinearMaxWidth(size_t* maxWidthInElements, const cudaChannelFormatDesc* fmtDesc, int device);\n-__host__ cudaError_t cudaDeviceRegisterAsyncNotification(int device, cudaAsyncCallback callbackFunc, void* userData, cudaAsyncCallbackHandle_t* callback);\n __host__ cudaError_t cudaDeviceReset(void);\n __host__ cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);\n __host__ cudaError_t cudaDeviceSetLimit(cudaLimit limit, size_t value);\n-__host__ cudaError_t cudaDeviceSetMemPool(int device, cudaMemPool_t memPool);\n __host__  cudaError_t cudaDeviceSynchronize(void);\n-__device__ cudaError_t cudaDeviceSynchronize(void);\n-__host__ cudaError_t cudaDeviceUnregisterAsyncNotification(int device, cudaAsyncCallbackHandle_t callback);\n __host__  cudaError_t cudaGetDevice(int* device);\n __device__ cudaError_t cudaGetDevice(int* device);\n __host__  cudaError_t cudaGetDeviceCount(int* count);\n __device__ cudaError_t cudaGetDeviceCount(int* count);\n __host__ cudaError_t cudaGetDeviceFlags(unsigned int* flags);\n __host__ cudaError_t cudaGetDeviceProperties(cudaDeviceProp* prop, int device);\n __host__ cudaError_t cudaInitDevice(int device, unsigned int deviceFlags, unsigned int flags);\n __host__ cudaError_t cudaIpcCloseMemHandle(void* devPtr);\n __host__ cudaError_t cudaIpcGetEventHandle(cudaIpcEventHandle_t* handle, cudaEvent_t event);\n __host__ cudaError_t cudaIpcGetMemHandle(cudaIpcMemHandle_t* handle, void* devPtr);\n __host__ cudaError_t cudaIpcOpenEventHandle(cudaEvent_t* event, cudaIpcEventHandle_t handle);\n __host__ cudaError_t cudaIpcOpenMemHandle(void** devPtr, cudaIpcMemHandle_t handle, unsigned int flags);\n __host__ cudaError_t cudaSetDevice(int device);\n __host__ cudaError_t cudaSetDeviceFlags(unsigned int flags);\n __host__ cudaError_t cudaSetValidDevices(int* device_arr, int len);\n</code></pre>"},{"location":"manual/api-runtime/#62-device-management-deprecated","title":"6.2. Device Management [DEPRECATED]","text":"<pre><code>-__host__  cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig ** pConfig);\n-__device__ cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig ** pConfig);\n __host__ cudaError_t cudaDeviceSetSharedMemConfig(cudaSharedMemConfig config);\n</code></pre>"},{"location":"manual/api-runtime/#63-thread-management-deprecated","title":"6.3. Thread Management [DEPRECATED]","text":"<pre><code> __host__ cudaError_t cudaThreadExit(void);\n-__host__ cudaError_t cudaThreadGetCacheConfig(cudaFuncCache ** pCacheConfig);\n __host__ cudaError_t cudaThreadGetLimit(size_t* pValue, cudaLimit limit);\n __host__ cudaError_t cudaThreadSetCacheConfig(cudaFuncCache cacheConfig);\n __host__ cudaError_t cudaThreadSetLimit(cudaLimit limit, size_t value);\n __host__ cudaError_t cudaThreadSynchronize(void);\n</code></pre>"},{"location":"manual/api-runtime/#64-error-handling","title":"6.4. Error Handling","text":"<pre><code> __host__  const char* cudaGetErrorName(cudaError_t error);\n __device__ const char* cudaGetErrorName(cudaError_t error);\n __host__  const char* cudaGetErrorString(cudaError_t error);\n __device__ const char* cudaGetErrorString(cudaError_t error);\n __host__  cudaError_t cudaGetLastError(void);\n-__device__ cudaError_t cudaGetLastError(void);\n __host__  cudaError_t cudaPeekAtLastError(void);\n-__device__ cudaError_t cudaPeekAtLastError(void);\n</code></pre>"},{"location":"manual/api-runtime/#64-stream-management","title":"6.4. Stream Management","text":"<pre><code> typedef void(* cudaStreamCallback_t)(cudaStream_t stream, cudaError_t status, void* userData);\n __host__ cudaError_t cudaCtxResetPersistingL2Cache(void);\n __host__ cudaError_t cudaStreamAddCallback(cudaStream_t stream, cudaStreamCallback_t callback, void* userData, unsigned int flags);\n __host__ cudaError_t cudaStreamAttachMemAsync(cudaStream_t stream, void* devPtr, size_t length = 0, unsigned int flags = cudaMemAttachSingle);\n __host__ cudaError_t cudaStreamBeginCapture(cudaStream_t stream, cudaStreamCaptureMode mode);\n-__host__ cudaError_t cudaStreamBeginCaptureToGraph(cudaStream_t stream, cudaGraph_t graph, const cudaGraphNode_t* dependencies, const cudaGraphEdgeData* dependencyData, size_t numDependencies, cudaStreamCaptureMode mode);\n-__host__ cudaError_t cudaStreamCopyAttributes(cudaStream_t dst, cudaStream_t src);\n __host__ cudaError_t cudaStreamCreate(cudaStream_t* pStream);\n __host__  cudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags);\n-__device__ cudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, unsigned int flags);\n __host__ cudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, unsigned int flags, int priority);\n __host__  cudaError_t cudaStreamDestroy(cudaStream_t stream);\n-__device__ cudaError_t cudaStreamDestroy(cudaStream_t stream);\n __host__ cudaError_t cudaStreamEndCapture(cudaStream_t stream, cudaGraph_t* pGraph);\n-__host__ cudaError_t cudaStreamGetAttribute(cudaStream_t hStream, cudaStreamAttrID attr, cudaStreamAttrValue* value_out);\n-__host__ cudaError_t cudaStreamGetCaptureInfo(cudaStream_t stream, cudaStreamCaptureStatus ** captureStatus_out, unsigned long long* id_out = 0, cudaGraph_t* graph_out = 0, const cudaGraphNode_t** dependencies_out = 0, const cudaGraphEdgeData** edgeData_out = 0, size_t* numDependencies_out = 0);\n __host__ cudaError_t cudaStreamGetDevice(cudaStream_t hStream, int* device);\n __host__ cudaError_t cudaStreamGetFlags(cudaStream_t hStream, unsigned int* flags);\n __host__ cudaError_t cudaStreamGetId(cudaStream_t hStream, unsigned long long* streamId);\n __host__ cudaError_t cudaStreamGetPriority(cudaStream_t hStream, int* priority);\n-__host__ cudaError_t cudaStreamIsCapturing(cudaStream_t stream, cudaStreamCaptureStatus ** pCaptureStatus);\n __host__ cudaError_t cudaStreamQuery(cudaStream_t stream);\n-__host__ cudaError_t cudaStreamSetAttribute(cudaStream_t hStream, cudaStreamAttrID attr, const cudaStreamAttrValue* value);\n __host__ cudaError_t cudaStreamSynchronize(cudaStream_t stream);\n-__host__ cudaError_t cudaStreamUpdateCaptureDependencies(cudaStream_t stream, cudaGraphNode_t* dependencies, const cudaGraphEdgeData* dependencyData, size_t numDependencies, unsigned int flags = 0);\n __host__  cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event, unsigned int flags = 0);\n-__device__ cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event, unsigned int flags = 0);\n-__host__ cudaError_t cudaThreadExchangeStreamCaptureMode(cudaStreamCaptureMode ** mode);\n</code></pre>"},{"location":"manual/api-runtime/#65-event-management","title":"6.5. Event Management","text":"<pre><code> __host__ cudaError_t cudaEventCreate(cudaEvent_t* event);\n __host__  cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags);\n-__device__ cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, unsigned int flags);\n __host__  cudaError_t cudaEventDestroy(cudaEvent_t event);\n-__device__ cudaError_t cudaEventDestroy(cudaEvent_t event);\n __host__ cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t end);\n __host__ cudaError_t cudaEventQuery(cudaEvent_t event);\n __host__  cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0);\n-__device__ cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0);\n __host__ cudaError_t cudaEventRecordWithFlags(cudaEvent_t event, cudaStream_t stream = 0, unsigned int flags = 0);\n __host__ cudaError_t cudaEventSynchronize(cudaEvent_t event);\n</code></pre>"},{"location":"manual/api-runtime/#66-external-resource-interoperability","title":"6.6. External Resource Interoperability","text":"<pre><code>-__host__ cudaError_t cudaDestroyExternalMemory(cudaExternalMemory_t extMem);\n-__host__ cudaError_t cudaDestroyExternalSemaphore(cudaExternalSemaphore_t extSem);\n-__host__ cudaError_t cudaExternalMemoryGetMappedBuffer(void** devPtr, cudaExternalMemory_t extMem, const cudaExternalMemoryBufferDesc* bufferDesc);\n-__host__ cudaError_t cudaExternalMemoryGetMappedMipmappedArray(cudaMipmappedArray_t* mipmap, cudaExternalMemory_t extMem, const cudaExternalMemoryMipmappedArrayDesc* mipmapDesc);\n-__host__ cudaError_t cudaImportExternalMemory(cudaExternalMemory_t* extMem_out, const cudaExternalMemoryHandleDesc* memHandleDesc);\n-__host__ cudaError_t cudaImportExternalSemaphore(cudaExternalSemaphore_t* extSem_out, const cudaExternalSemaphoreHandleDesc* semHandleDesc);\n-__host__ cudaError_t cudaSignalExternalSemaphoresAsync(const cudaExternalSemaphore_t* extSemArray, const cudaExternalSemaphoreSignalParams* paramsArray, unsigned int numExtSems, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaWaitExternalSemaphoresAsync(const cudaExternalSemaphore_t* extSemArray, const cudaExternalSemaphoreWaitParams* paramsArray, unsigned int numExtSems, cudaStream_t stream = 0);\n</code></pre>"},{"location":"manual/api-runtime/#67-execution-control","title":"6.7. Execution Control","text":"<pre><code> __host__  cudaError_t cudaFuncGetAttributes(cudaFuncAttributes* attr, const void* func);\n-__device__ cudaError_t cudaFuncGetAttributes(cudaFuncAttributes* attr, const void* func);\n __host__ cudaError_t cudaFuncGetName(const char** name, const void* func);\n __host__ cudaError_t cudaFuncGetParamInfo(const void* func, size_t paramIndex, size_t* paramOffset, size_t* paramSize);\n __host__ cudaError_t cudaFuncSetAttribute(const void* func, cudaFuncAttribute attr, int value);\n __host__ cudaError_t cudaFuncSetCacheConfig(const void* func, cudaFuncCache cacheConfig);\n __device__ void* cudaGetParameterBuffer(size_t alignment, size_t size);\n __device__ void cudaGridDependencySynchronize(void);\n __host__ cudaError_t cudaLaunchCooperativeKernel(const void* func, dim3 gridDim, dim3 blockDim, void** args, size_t sharedMem, cudaStream_t stream);\n __device__ cudaError_t cudaLaunchDevice(void* func, void* parameterBuffer, dim3 gridDimension, dim3 blockDimension, unsigned int sharedMemSize, cudaStream_t stream);\n __host__ cudaError_t cudaLaunchHostFunc(cudaStream_t stream, cudaHostFn_t fn, void* userData);\n __host__ cudaError_t cudaLaunchKernel(const void* func, dim3 gridDim, dim3 blockDim, void** args, size_t sharedMem, cudaStream_t stream);\n __host__ cudaError_t cudaLaunchKernelExC(const cudaLaunchConfig_t* config, const void* func, void** args);\n __device__ void cudaTriggerProgrammaticLaunchCompletion(void);\n</code></pre>"},{"location":"manual/api-runtime/#68-execution-control-deprecated","title":"6.8. Execution Control [DEPRECATED]","text":"<pre><code> __host__ cudaError_t cudaFuncSetSharedMemConfig(const void* func, cudaSharedMemConfig config);\n</code></pre>"},{"location":"manual/api-runtime/#69-occupancy","title":"6.9. Occupancy","text":"<pre><code> __host__ cudaError_t cudaOccupancyAvailableDynamicSMemPerBlock(size_t* dynamicSmemSize, const void* func, int numBlocks, int blockSize);\n __host__  cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessor(int* numBlocks, const void* func, int blockSize, size_t dynamicSMemSize);\n-__device__ cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessor(int* numBlocks, const void* func, int blockSize, size_t dynamicSMemSize);\n __host__ cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int* numBlocks, const void* func, int blockSize, size_t dynamicSMemSize, unsigned int flags);\n-__host__ cudaError_t cudaOccupancyMaxActiveClusters(int* numClusters, const void* func, const cudaLaunchConfig_t* launchConfig);\n-__host__ cudaError_t cudaOccupancyMaxPotentialClusterSize(int* clusterSize, const void* func, const cudaLaunchConfig_t* launchConfig);\n</code></pre>"},{"location":"manual/api-runtime/#610-memory-management","title":"6.10. Memory Management","text":"<pre><code> __host__ cudaError_t cudaArrayGetInfo(cudaChannelFormatDesc* desc, cudaExtent* extent, unsigned int* flags, cudaArray_t array);\n-__host__ cudaError_t cudaArrayGetMemoryRequirements(cudaArrayMemoryRequirements* memoryRequirements, cudaArray_t array, int device);\n-__host__ cudaError_t cudaArrayGetPlane(cudaArray_t* pPlaneArray, cudaArray_t hArray, unsigned int planeIdx);\n-__host__ cudaError_t cudaArrayGetSparseProperties(cudaArraySparseProperties* sparseProperties, cudaArray_t array);\n __host__  cudaError_t cudaFree(void* devPtr);\n __device__ cudaError_t cudaFree(void* devPtr);\n __host__ cudaError_t cudaFreeArray(cudaArray_t array);\n __host__ cudaError_t cudaFreeHost(void* ptr);\n-__host__ cudaError_t cudaFreeMipmappedArray(cudaMipmappedArray_t mipmappedArray);\n-__host__ cudaError_t cudaGetMipmappedArrayLevel(cudaArray_t* levelArray, cudaMipmappedArray_const_t mipmappedArray, unsigned int level);\n __host__ cudaError_t cudaGetSymbolAddress(void** devPtr, const void* symbol);\n __host__ cudaError_t cudaGetSymbolSize(size_t* size, const void* symbol);\n __host__ cudaError_t cudaHostAlloc(void** pHost, size_t size, unsigned int flags);\n __host__ cudaError_t cudaHostGetDevicePointer(void** pDevice, void* pHost, unsigned int flags);\n __host__ cudaError_t cudaHostGetFlags(unsigned int* pFlags, void* pHost);\n __host__ cudaError_t cudaHostRegister(void* ptr, size_t size, unsigned int flags);\n __host__ cudaError_t cudaHostUnregister(void* ptr);\n __host__  cudaError_t cudaMalloc(void** devPtr, size_t size);\n __device__ cudaError_t cudaMalloc(void** devPtr, size_t size);\n-__host__ cudaError_t cudaMalloc3D(cudaPitchedPtr* pitchedDevPtr, cudaExtent extent);\n __host__ cudaError_t cudaMalloc3DArray(cudaArray_t* array, const cudaChannelFormatDesc* desc, cudaExtent extent, unsigned int flags = 0);\n __host__ cudaError_t cudaMallocArray(cudaArray_t* array, const cudaChannelFormatDesc* desc, size_t width, size_t height = 0, unsigned int flags = 0);\n __host__ cudaError_t cudaMallocHost(void** ptr, size_t size);\n __host__ cudaError_t cudaMallocManaged(void** devPtr, size_t size, unsigned int flags = cudaMemAttachGlobal);\n-__host__ cudaError_t cudaMallocMipmappedArray(cudaMipmappedArray_t* mipmappedArray, const cudaChannelFormatDesc* desc, cudaExtent extent, unsigned int numLevels, unsigned int flags = 0);\n __host__ cudaError_t cudaMallocPitch(void** devPtr, size_t* pitch, size_t width, size_t height);\n-__host__ cudaError_t cudaMemAdvise(const void* devPtr, size_t count, cudaMemoryAdvise advice, cudaMemLocation location);\n-__host__ cudaError_t cudaMemDiscardAndPrefetchBatchAsync(void** dptrs, size_t* sizes, size_t count, cudaMemLocation* prefetchLocs, size_t* prefetchLocIdxs, size_t numPrefetchLocs, unsigned long long flags, cudaStream_t stream);\n-__host__ cudaError_t cudaMemDiscardBatchAsync(void** dptrs, size_t* sizes, size_t count, unsigned long long flags, cudaStream_t stream);\n __host__ cudaError_t cudaMemGetInfo(size_t* free, size_t* total);\n-__host__ cudaError_t cudaMemPrefetchAsync(const void* devPtr, size_t count, cudaMemLocation location, unsigned int flags, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaMemPrefetchBatchAsync(void** dptrs, size_t* sizes, size_t count, cudaMemLocation* prefetchLocs, size_t* prefetchLocIdxs, size_t numPrefetchLocs, unsigned long long flags, cudaStream_t stream);\n-__host__ cudaError_t cudaMemRangeGetAttribute(void* data, size_t dataSize, cudaMemRangeAttribute attribute, const void* devPtr, size_t count);\n-__host__ cudaError_t cudaMemRangeGetAttributes(void** data, size_t* dataSizes, cudaMemRangeAttribute ** attributes, size_t numAttributes, const void* devPtr, size_t count);\n __host__ cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind);\n __host__ cudaError_t cudaMemcpy2D(void* dst, size_t dpitch, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind);\n-__host__ cudaError_t cudaMemcpy2DArrayToArray(cudaArray_t dst, size_t wOffsetDst, size_t hOffsetDst, cudaArray_const_t src, size_t wOffsetSrc, size_t hOffsetSrc, size_t width, size_t height, cudaMemcpyKind kind = cudaMemcpyDeviceToDevice);\n __host__  cudaError_t cudaMemcpy2DAsync(void* dst, size_t dpitch, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind, cudaStream_t stream = 0);\n-__device__ cudaError_t cudaMemcpy2DAsync(void* dst, size_t dpitch, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaMemcpy2DFromArray(void* dst, size_t dpitch, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t width, size_t height, cudaMemcpyKind kind);\n-__host__ cudaError_t cudaMemcpy2DFromArrayAsync(void* dst, size_t dpitch, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t width, size_t height, cudaMemcpyKind kind, cudaStream_t stream = 0);\n __host__ cudaError_t cudaMemcpy2DToArray(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind);\n __host__ cudaError_t cudaMemcpy2DToArrayAsync(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind, cudaStream_t stream = 0);\n __host__ cudaError_t cudaMemcpy3D(const cudaMemcpy3DParms* p);\n __host__  cudaError_t cudaMemcpy3DAsync(const cudaMemcpy3DParms* p, cudaStream_t stream = 0);\n-__device__ cudaError_t cudaMemcpy3DAsync(const cudaMemcpy3DParms* p, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaMemcpy3DBatchAsync(size_t numOps, cudaMemcpy3DBatchOp* opList, unsigned long long flags, cudaStream_t stream);\n __host__ cudaError_t cudaMemcpy3DPeer(const cudaMemcpy3DPeerParms* p);\n __host__ cudaError_t cudaMemcpy3DPeerAsync(const cudaMemcpy3DPeerParms* p, cudaStream_t stream = 0);\n __host__  cudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t count, cudaMemcpyKind kind, cudaStream_t stream = 0);\n-__device__ cudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t count, cudaMemcpyKind kind, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaMemcpyBatchAsync(const void** dsts, const void** srcs, const size_t* sizes, size_t count, cudaMemcpyAttributes* attrs, size_t* attrsIdxs, size_t numAttrs, cudaStream_t stream);\n __host__ cudaError_t cudaMemcpyFromSymbol(void* dst, const void* symbol, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyDeviceToHost);\n __host__ cudaError_t cudaMemcpyFromSymbolAsync(void* dst, const void* symbol, size_t count, size_t offset, cudaMemcpyKind kind, cudaStream_t stream = 0);\n __host__ cudaError_t cudaMemcpyPeer(void* dst, int dstDevice, const void* src, int srcDevice, size_t count);\n __host__ cudaError_t cudaMemcpyPeerAsync(void* dst, int dstDevice, const void* src, int srcDevice, size_t count, cudaStream_t stream = 0);\n __host__ cudaError_t cudaMemcpyToSymbol(const void* symbol, const void* src, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyHostToDevice);\n __host__ cudaError_t cudaMemcpyToSymbolAsync(const void* symbol, const void* src, size_t count, size_t offset, cudaMemcpyKind kind, cudaStream_t stream = 0);\n __host__ cudaError_t cudaMemset(void* devPtr, int value, size_t count);\n __host__ cudaError_t cudaMemset2D(void* devPtr, size_t pitch, int value, size_t width, size_t height);\n __host__  cudaError_t cudaMemset2DAsync(void* devPtr, size_t pitch, int value, size_t width, size_t height, cudaStream_t stream = 0);\n-__device__ cudaError_t cudaMemset2DAsync(void* devPtr, size_t pitch, int value, size_t width, size_t height, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaMemset3D(cudaPitchedPtr pitchedDevPtr, int value, cudaExtent extent);\n-__host__  cudaError_t cudaMemset3DAsync(cudaPitchedPtr pitchedDevPtr, int value, cudaExtent extent, cudaStream_t stream = 0);\n-__device__ cudaError_t cudaMemset3DAsync(cudaPitchedPtr pitchedDevPtr, int value, cudaExtent extent, cudaStream_t stream = 0);\n __host__  cudaError_t cudaMemsetAsync(void* devPtr, int value, size_t count, cudaStream_t stream = 0);\n-__device__ cudaError_t cudaMemsetAsync(void* devPtr, int value, size_t count, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaMipmappedArrayGetMemoryRequirements(cudaArrayMemoryRequirements* memoryRequirements, cudaMipmappedArray_t mipmap, int device);\n-__host__ cudaError_t cudaMipmappedArrayGetSparseProperties(cudaArraySparseProperties* sparseProperties, cudaMipmappedArray_t mipmap);\n __host__ cudaExtent make_cudaExtent(size_t w, size_t h, size_t d);\n __host__ cudaPitchedPtr make_cudaPitchedPtr(void* d, size_t p, size_t xsz, size_t ysz);\n __host__ cudaPos make_cudaPos(size_t x, size_t y, size_t z);\n</code></pre>"},{"location":"manual/api-runtime/#611-memory-management-deprecated","title":"6.11. Memory Management [DEPRECATED]","text":"<pre><code>-__host__ cudaError_t cudaMemcpyArrayToArray(cudaArray_t dst, size_t wOffsetDst, size_t hOffsetDst, cudaArray_const_t src, size_t wOffsetSrc, size_t hOffsetSrc, size_t count, cudaMemcpyKind kind = cudaMemcpyDeviceToDevice);\n-__host__ cudaError_t cudaMemcpyFromArray(void* dst, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t count, cudaMemcpyKind kind);\n-__host__ cudaError_t cudaMemcpyFromArrayAsync(void* dst, cudaArray_const_t src, size_t wOffset, size_t hOffset, size_t count, cudaMemcpyKind kind, cudaStream_t stream = 0);\n __host__ cudaError_t cudaMemcpyToArray(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t count, cudaMemcpyKind kind);\n __host__ cudaError_t cudaMemcpyToArrayAsync(cudaArray_t dst, size_t wOffset, size_t hOffset, const void* src, size_t count, cudaMemcpyKind kind, cudaStream_t stream = 0);\n</code></pre>"},{"location":"manual/api-runtime/#612-stream-ordered-memory-allocator","title":"6.12. Stream Ordered Memory Allocator","text":"<pre><code> __host__ cudaError_t cudaFreeAsync(void* devPtr, cudaStream_t hStream);\n __host__ cudaError_t cudaMallocAsync(void** devPtr, size_t size, cudaStream_t hStream);\n-__host__ cudaError_t cudaMallocFromPoolAsync(void** ptr, size_t size, cudaMemPool_t memPool, cudaStream_t stream);\n-__host__ cudaError_t cudaMemGetDefaultMemPool(cudaMemPool_t* memPool, cudaMemLocation* location, cudaMemAllocationType type);\n-__host__ cudaError_t cudaMemGetMemPool(cudaMemPool_t* memPool, cudaMemLocation* location, cudaMemAllocationType type);\n-__host__ cudaError_t cudaMemPoolCreate(cudaMemPool_t* memPool, const cudaMemPoolProps* poolProps);\n-__host__ cudaError_t cudaMemPoolDestroy(cudaMemPool_t memPool);\n-__host__ cudaError_t cudaMemPoolExportPointer(cudaMemPoolPtrExportData* exportData, void* ptr);\n-__host__ cudaError_t cudaMemPoolExportToShareableHandle(void* shareableHandle, cudaMemPool_t memPool, cudaMemAllocationHandleType handleType, unsigned int flags);\n-__host__ cudaError_t cudaMemPoolGetAccess(cudaMemAccessFlags ** flags, cudaMemPool_t memPool, cudaMemLocation* location);\n-__host__ cudaError_t cudaMemPoolGetAttribute(cudaMemPool_t memPool, cudaMemPoolAttr attr, void* value);\n-__host__ cudaError_t cudaMemPoolImportFromShareableHandle(cudaMemPool_t* memPool, void* shareableHandle, cudaMemAllocationHandleType handleType, unsigned int flags);\n-__host__ cudaError_t cudaMemPoolImportPointer(void** ptr, cudaMemPool_t memPool, cudaMemPoolPtrExportData* exportData);\n-__host__ cudaError_t cudaMemPoolSetAccess(cudaMemPool_t memPool, const cudaMemAccessDesc* descList, size_t count);\n-__host__ cudaError_t cudaMemPoolSetAttribute(cudaMemPool_t memPool, cudaMemPoolAttr attr, void* value);\n-__host__ cudaError_t cudaMemPoolTrimTo(cudaMemPool_t memPool, size_t minBytesToKeep);\n-__host__ cudaError_t cudaMemSetMemPool(cudaMemLocation* location, cudaMemAllocationType type, cudaMemPool_t memPool);\n</code></pre>"},{"location":"manual/api-runtime/#613-unified-addressing","title":"6.13. Unified Addressing","text":"<pre><code> __host__ cudaError_t cudaPointerGetAttributes(cudaPointerAttributes* attributes, const void* ptr);\n</code></pre>"},{"location":"manual/api-runtime/#614-peer-device-memory-access","title":"6.14. Peer Device Memory Access","text":"<pre><code> __host__ cudaError_t cudaDeviceCanAccessPeer(int* canAccessPeer, int device, int peerDevice);\n __host__ cudaError_t cudaDeviceDisablePeerAccess(int peerDevice);\n __host__ cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int flags);\n</code></pre>"},{"location":"manual/api-runtime/#615-opengl-interoperability","title":"6.15. OpenGL Interoperability","text":"<pre><code> enum cudaGLDeviceList\n-__host__ cudaError_t cudaGLGetDevices(unsigned int* pCudaDeviceCount, int* pCudaDevices, unsigned int cudaDeviceCount, cudaGLDeviceList deviceList);\n __host__ cudaError_t cudaGraphicsGLRegisterBuffer(cudaGraphicsResource** resource, GLuint buffer, unsigned int flags);\n-__host__ cudaError_t cudaGraphicsGLRegisterImage(cudaGraphicsResource** resource, GLuint image, GLenum target, unsigned int flags);\n-__host__ cudaError_t cudaWGLGetDevice(int* device, HGPUNV hGpu);\n</code></pre>"},{"location":"manual/api-runtime/#616-opengl-interoperability-deprecated","title":"6.16. OpenGL Interoperability [DEPRECATED]","text":"<pre><code>-enum cudaGLMapFlags\n-__host__ cudaError_t cudaGLMapBufferObject(void** devPtr, GLuint bufObj);\n-__host__ cudaError_t cudaGLMapBufferObjectAsync(void** devPtr, GLuint bufObj, cudaStream_t stream);\n-__host__ cudaError_t cudaGLRegisterBufferObject(GLuint bufObj);\n-__host__ cudaError_t cudaGLSetBufferObjectMapFlags(GLuint bufObj, unsigned int flags);\n-__host__ cudaError_t cudaGLSetGLDevice(int device);\n-__host__ cudaError_t cudaGLUnmapBufferObject(GLuint bufObj);\n-__host__ cudaError_t cudaGLUnmapBufferObjectAsync(GLuint bufObj, cudaStream_t stream);\n-__host__ cudaError_t cudaGLUnregisterBufferObject(GLuint bufObj);\n</code></pre>"},{"location":"manual/api-runtime/#617-direct3d-9-interoperability","title":"6.17. Direct3D 9 Interoperability","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#618-direct3d-9-interoperability-deprecated","title":"6.18. Direct3D 9 Interoperability [DEPRECATED]","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#619-direct3d-10-interoperability","title":"6.19. Direct3D 10 Interoperability","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#620-direct3d-10-interoperability-deprecated","title":"6.20. Direct3D 10 Interoperability [DEPRECATED]","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#621-direct3d-11-interoperability","title":"6.21. Direct3D 11 Interoperability","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#622-direct3d-11-interoperability-deprecated","title":"6.22. Direct3D 11 Interoperability [DEPRECATED]","text":"<pre><code>-Windows APIs are currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#623-vdpau-interoperability","title":"6.23. VDPAU Interoperability","text":"<pre><code>-VDPAU is currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#624-egl-interoperability","title":"6.24. EGL Interoperability","text":"<pre><code>-EGL is currently unsupported\n</code></pre>"},{"location":"manual/api-runtime/#625-graphics-interoperability","title":"6.25. Graphics Interoperability","text":"<pre><code> __host__ cudaError_t cudaGraphicsMapResources(int count, cudaGraphicsResource_t* resources, cudaStream_t stream = 0);\n-__host__ cudaError_t cudaGraphicsResourceGetMappedMipmappedArray(cudaMipmappedArray_t* mipmappedArray, cudaGraphicsResource_t resource);\n __host__ cudaError_t cudaGraphicsResourceGetMappedPointer(void** devPtr, size_t* size, cudaGraphicsResource_t resource);\n-__host__ cudaError_t cudaGraphicsResourceSetMapFlags(cudaGraphicsResource_t resource, unsigned int flags);\n-__host__ cudaError_t cudaGraphicsSubResourceGetMappedArray(cudaArray_t* array, cudaGraphicsResource_t resource, unsigned int arrayIndex, unsigned int mipLevel);\n __host__ cudaError_t cudaGraphicsUnmapResources(int count, cudaGraphicsResource_t* resources, cudaStream_t stream = 0);\n __host__ cudaError_t cudaGraphicsUnregisterResource(cudaGraphicsResource_t resource);\n</code></pre>"},{"location":"manual/api-runtime/#626-texture-object-management","title":"6.26. Texture Object Management","text":"<pre><code> __host__ cudaChannelFormatDesc cudaCreateChannelDesc(int x, int y, int z, int w, cudaChannelFormatKind f);\n-__host__ cudaError_t cudaCreateTextureObject(cudaTextureObject_t* pTexObject, const cudaResourceDesc* pResDesc, const cudaTextureDesc* pTexDesc, const cudaResourceViewDesc* pResViewDesc);\n-__host__ cudaError_t cudaDestroyTextureObject(cudaTextureObject_t texObject);\n-__host__ cudaError_t cudaGetChannelDesc(cudaChannelFormatDesc* desc, cudaArray_const_t array);\n-__host__ cudaError_t cudaGetTextureObjectResourceDesc(cudaResourceDesc* pResDesc, cudaTextureObject_t texObject);\n-__host__ cudaError_t cudaGetTextureObjectResourceViewDesc(cudaResourceViewDesc* pResViewDesc, cudaTextureObject_t texObject);\n-__host__ cudaError_t cudaGetTextureObjectTextureDesc(cudaTextureDesc* pTexDesc, cudaTextureObject_t texObject);\n</code></pre>"},{"location":"manual/api-runtime/#627-surface-object-management","title":"6.27. Surface Object Management","text":"<pre><code>-__host__ cudaError_t cudaCreateSurfaceObject(cudaSurfaceObject_t* pSurfObject, const cudaResourceDesc* pResDesc);\n-__host__ cudaError_t cudaDestroySurfaceObject(cudaSurfaceObject_t surfObject);\n-__host__ cudaError_t cudaGetSurfaceObjectResourceDesc(cudaResourceDesc* pResDesc, cudaSurfaceObject_t surfObject);\n</code></pre>"},{"location":"manual/api-runtime/#628-version-management","title":"6.28. Version Management","text":"<pre><code> __host__ cudaError_t cudaDriverGetVersion(int* driverVersion);\n __host__  cudaError_t cudaRuntimeGetVersion(int* runtimeVersion);\n __device__ cudaError_t cudaRuntimeGetVersion(int* runtimeVersion);\n</code></pre>"},{"location":"manual/api-runtime/#629-error-log-management-functions","title":"6.29. Error Log Management Functions","text":"<pre><code>-typedef void(* cudaLogsCallback_t)(void* data, cudaLogLevel logLevel, char* message, size_t length);\n-__host__ cudaError_t cudaLogsCurrent(cudaLogIterator* iterator_out, unsigned int flags);\n-__host__ cudaError_t cudaLogsDumpToFile(cudaLogIterator* iterator, const char* pathToFile, unsigned int flags);\n-__host__ cudaError_t cudaLogsDumpToMemory(cudaLogIterator* iterator, char* buffer, size_t* size, unsigned int flags);\n-__host__ cudaError_t cudaLogsRegisterCallback(cudaLogsCallback_t callbackFunc, void* userData, cudaLogsCallbackHandle* callback_out);\n-__host__ cudaError_t cudaLogsUnregisterCallback(cudaLogsCallbackHandle callback);\n</code></pre>"},{"location":"manual/api-runtime/#630-graph-management","title":"6.30. Graph Management","text":"<pre><code>-__host__ cudaError_t cudaDeviceGetGraphMemAttribute(int device, cudaGraphMemAttributeType attr, void* value);\n-__host__ cudaError_t cudaDeviceGraphMemTrim(int device);\n-__host__ cudaError_t cudaDeviceSetGraphMemAttribute(int device, cudaGraphMemAttributeType attr, void* value);\n-__device__ cudaGraphExec_t cudaGetCurrentGraphExec(void);\n-__host__ cudaError_t cudaGraphAddChildGraphNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, cudaGraph_t childGraph);\n-__host__ cudaError_t cudaGraphAddDependencies(cudaGraph_t graph, const cudaGraphNode_t* from, const cudaGraphNode_t* to, const cudaGraphEdgeData* edgeData, size_t numDependencies);\n-__host__ cudaError_t cudaGraphAddEmptyNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies);\n-__host__ cudaError_t cudaGraphAddEventRecordNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, cudaEvent_t event);\n-__host__ cudaError_t cudaGraphAddEventWaitNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, cudaEvent_t event);\n-__host__ cudaError_t cudaGraphAddExternalSemaphoresSignalNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const cudaExternalSemaphoreSignalNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphAddExternalSemaphoresWaitNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const cudaExternalSemaphoreWaitNodeParams* nodeParams);\n __host__ cudaError_t cudaGraphAddHostNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const cudaHostNodeParams* pNodeParams);\n __host__ cudaError_t cudaGraphAddKernelNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const cudaKernelNodeParams* pNodeParams);\n-__host__ cudaError_t cudaGraphAddMemAllocNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, cudaMemAllocNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphAddMemFreeNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, void* dptr);\n __host__ cudaError_t cudaGraphAddMemcpyNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const cudaMemcpy3DParms* pCopyParams);\n __host__ cudaError_t cudaGraphAddMemcpyNode1D(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, void* dst, const void* src, size_t count, cudaMemcpyKind kind);\n __host__ cudaError_t cudaGraphAddMemcpyNodeFromSymbol(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, void* dst, const void* symbol, size_t count, size_t offset, cudaMemcpyKind kind);\n __host__ cudaError_t cudaGraphAddMemcpyNodeToSymbol(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const void* symbol, const void* src, size_t count, size_t offset, cudaMemcpyKind kind);\n __host__ cudaError_t cudaGraphAddMemsetNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const cudaMemsetParams* pMemsetParams);\n-__host__ cudaError_t cudaGraphAddNode(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, const cudaGraphEdgeData* dependencyData, size_t numDependencies, cudaGraphNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphChildGraphNodeGetGraph(cudaGraphNode_t node, cudaGraph_t* pGraph);\n __host__ cudaError_t cudaGraphClone(cudaGraph_t* pGraphClone, cudaGraph_t originalGraph);\n-__host__ cudaError_t cudaGraphConditionalHandleCreate(cudaGraphConditionalHandle* pHandle_out, cudaGraph_t graph, unsigned int defaultLaunchValue = 0, unsigned int flags = 0);\n __host__ cudaError_t cudaGraphCreate(cudaGraph_t* pGraph, unsigned int flags);\n-__host__ cudaError_t cudaGraphDebugDotPrint(cudaGraph_t graph, const char* path, unsigned int flags);\n __host__ cudaError_t cudaGraphDestroy(cudaGraph_t graph);\n __host__ cudaError_t cudaGraphDestroyNode(cudaGraphNode_t node);\n-__host__ cudaError_t cudaGraphEventRecordNodeGetEvent(cudaGraphNode_t node, cudaEvent_t* event_out);\n-__host__ cudaError_t cudaGraphEventRecordNodeSetEvent(cudaGraphNode_t node, cudaEvent_t event);\n-__host__ cudaError_t cudaGraphEventWaitNodeGetEvent(cudaGraphNode_t node, cudaEvent_t* event_out);\n-__host__ cudaError_t cudaGraphEventWaitNodeSetEvent(cudaGraphNode_t node, cudaEvent_t event);\n-__host__ cudaError_t cudaGraphExecChildGraphNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, cudaGraph_t childGraph);\n __host__ cudaError_t cudaGraphExecDestroy(cudaGraphExec_t graphExec);\n-__host__ cudaError_t cudaGraphExecEventRecordNodeSetEvent(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, cudaEvent_t event);\n-__host__ cudaError_t cudaGraphExecEventWaitNodeSetEvent(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, cudaEvent_t event);\n-__host__ cudaError_t cudaGraphExecExternalSemaphoresSignalNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, const cudaExternalSemaphoreSignalNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphExecExternalSemaphoresWaitNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, const cudaExternalSemaphoreWaitNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphExecGetFlags(cudaGraphExec_t graphExec, unsigned long long* flags);\n-__host__ cudaError_t cudaGraphExecHostNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const cudaHostNodeParams* pNodeParams);\n-__host__ cudaError_t cudaGraphExecKernelNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const cudaKernelNodeParams* pNodeParams);\n-__host__ cudaError_t cudaGraphExecMemcpyNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const cudaMemcpy3DParms* pNodeParams);\n-__host__ cudaError_t cudaGraphExecMemcpyNodeSetParams1D(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, void* dst, const void* src, size_t count, cudaMemcpyKind kind);\n-__host__ cudaError_t cudaGraphExecMemcpyNodeSetParamsFromSymbol(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, void* dst, const void* symbol, size_t count, size_t offset, cudaMemcpyKind kind);\n-__host__ cudaError_t cudaGraphExecMemcpyNodeSetParamsToSymbol(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const void* symbol, const void* src, size_t count, size_t offset, cudaMemcpyKind kind);\n-__host__ cudaError_t cudaGraphExecMemsetNodeSetParams(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const cudaMemsetParams* pNodeParams);\n-__host__ cudaError_t cudaGraphExecNodeSetParams(cudaGraphExec_t graphExec, cudaGraphNode_t node, cudaGraphNodeParams* nodeParams);\n __host__ cudaError_t cudaGraphExecUpdate(cudaGraphExec_t hGraphExec, cudaGraph_t hGraph, cudaGraphExecUpdateResultInfo* resultInfo);\n-__host__ cudaError_t cudaGraphExternalSemaphoresSignalNodeGetParams(cudaGraphNode_t hNode, cudaExternalSemaphoreSignalNodeParams* params_out);\n-__host__ cudaError_t cudaGraphExternalSemaphoresSignalNodeSetParams(cudaGraphNode_t hNode, const cudaExternalSemaphoreSignalNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphExternalSemaphoresWaitNodeGetParams(cudaGraphNode_t hNode, cudaExternalSemaphoreWaitNodeParams* params_out);\n-__host__ cudaError_t cudaGraphExternalSemaphoresWaitNodeSetParams(cudaGraphNode_t hNode, const cudaExternalSemaphoreWaitNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphGetEdges(cudaGraph_t graph, cudaGraphNode_t* from, cudaGraphNode_t* to, cudaGraphEdgeData* edgeData, size_t* numEdges);\n __host__ cudaError_t cudaGraphGetNodes(cudaGraph_t graph, cudaGraphNode_t* nodes, size_t* numNodes);\n-__host__ cudaError_t cudaGraphGetRootNodes(cudaGraph_t graph, cudaGraphNode_t* pRootNodes, size_t* pNumRootNodes);\n __host__ cudaError_t cudaGraphHostNodeGetParams(cudaGraphNode_t node, cudaHostNodeParams* pNodeParams);\n __host__ cudaError_t cudaGraphHostNodeSetParams(cudaGraphNode_t node, const cudaHostNodeParams* pNodeParams);\n __host__ cudaError_t cudaGraphInstantiate(cudaGraphExec_t* pGraphExec, cudaGraph_t graph, unsigned long long flags = 0);\n __host__ cudaError_t cudaGraphInstantiateWithFlags(cudaGraphExec_t* pGraphExec, cudaGraph_t graph, unsigned long long flags = 0);\n-__host__ cudaError_t cudaGraphInstantiateWithParams(cudaGraphExec_t* pGraphExec, cudaGraph_t graph, cudaGraphInstantiateParams* instantiateParams);\n-__host__ cudaError_t cudaGraphKernelNodeCopyAttributes(cudaGraphNode_t hDst, cudaGraphNode_t hSrc);\n-__host__ cudaError_t cudaGraphKernelNodeGetAttribute(cudaGraphNode_t hNode, cudaKernelNodeAttrID attr, cudaKernelNodeAttrValue* value_out);\n __host__ cudaError_t cudaGraphKernelNodeGetParams(cudaGraphNode_t node, cudaKernelNodeParams* pNodeParams);\n-__host__ cudaError_t cudaGraphKernelNodeSetAttribute(cudaGraphNode_t hNode, cudaKernelNodeAttrID attr, const cudaKernelNodeAttrValue* value);\n-__device__ cudaError_t cudaGraphKernelNodeSetEnabled(cudaGraphDeviceNode_t node, bool enable);\n-__device__ cudaError_t cudaGraphKernelNodeSetGridDim(cudaGraphDeviceNode_t node, dim3 gridDim);\n-template &lt; typename T &gt;__device__ cudaError_t cudaGraphKernelNodeSetParam(cudaGraphDeviceNode_t node, size_t offset, const T&amp; value);\n-__device__ cudaError_t cudaGraphKernelNodeSetParam(cudaGraphDeviceNode_t node, size_t offset, const void* value, size_t size);\n __host__ cudaError_t cudaGraphKernelNodeSetParams(cudaGraphNode_t node, const cudaKernelNodeParams* pNodeParams);\n-__device__ cudaError_t cudaGraphKernelNodeUpdatesApply(const cudaGraphKernelNodeUpdate* updates, size_t updateCount);\n __host__  cudaError_t cudaGraphLaunch(cudaGraphExec_t graphExec, cudaStream_t stream);\n-__device__ cudaError_t cudaGraphLaunch(cudaGraphExec_t graphExec, cudaStream_t stream);\n-__host__ cudaError_t cudaGraphMemAllocNodeGetParams(cudaGraphNode_t node, cudaMemAllocNodeParams* params_out);\n-__host__ cudaError_t cudaGraphMemFreeNodeGetParams(cudaGraphNode_t node, void* dptr_out);\n __host__ cudaError_t cudaGraphMemcpyNodeGetParams(cudaGraphNode_t node, cudaMemcpy3DParms* pNodeParams);\n __host__ cudaError_t cudaGraphMemcpyNodeSetParams(cudaGraphNode_t node, const cudaMemcpy3DParms* pNodeParams);\n __host__ cudaError_t cudaGraphMemcpyNodeSetParams1D(cudaGraphNode_t node, void* dst, const void* src, size_t count, cudaMemcpyKind kind);\n __host__ cudaError_t cudaGraphMemcpyNodeSetParamsFromSymbol(cudaGraphNode_t node, void* dst, const void* symbol, size_t count, size_t offset, cudaMemcpyKind kind);\n __host__ cudaError_t cudaGraphMemcpyNodeSetParamsToSymbol(cudaGraphNode_t node, const void* symbol, const void* src, size_t count, size_t offset, cudaMemcpyKind kind);\n __host__ cudaError_t cudaGraphMemsetNodeGetParams(cudaGraphNode_t node, cudaMemsetParams* pNodeParams);\n __host__ cudaError_t cudaGraphMemsetNodeSetParams(cudaGraphNode_t node, const cudaMemsetParams* pNodeParams);\n-__host__ cudaError_t cudaGraphNodeFindInClone(cudaGraphNode_t* pNode, cudaGraphNode_t originalNode, cudaGraph_t clonedGraph);\n-__host__ cudaError_t cudaGraphNodeGetDependencies(cudaGraphNode_t node, cudaGraphNode_t* pDependencies, cudaGraphEdgeData* edgeData, size_t* pNumDependencies);\n-__host__ cudaError_t cudaGraphNodeGetDependentNodes(cudaGraphNode_t node, cudaGraphNode_t* pDependentNodes, cudaGraphEdgeData* edgeData, size_t* pNumDependentNodes);\n-__host__ cudaError_t cudaGraphNodeGetEnabled(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, unsigned int* isEnabled);\n-__host__ cudaError_t cudaGraphNodeGetType(cudaGraphNode_t node, cudaGraphNodeType ** pType);\n-__host__ cudaError_t cudaGraphNodeSetEnabled(cudaGraphExec_t hGraphExec, cudaGraphNode_t hNode, unsigned int isEnabled);\n-__host__ cudaError_t cudaGraphNodeSetParams(cudaGraphNode_t node, cudaGraphNodeParams* nodeParams);\n-__host__ cudaError_t cudaGraphReleaseUserObject(cudaGraph_t graph, cudaUserObject_t object, unsigned int count = 1);\n-__host__ cudaError_t cudaGraphRemoveDependencies(cudaGraph_t graph, const cudaGraphNode_t* from, const cudaGraphNode_t* to, const cudaGraphEdgeData* edgeData, size_t numDependencies);\n-__host__ cudaError_t cudaGraphRetainUserObject(cudaGraph_t graph, cudaUserObject_t object, unsigned int count = 1, unsigned int flags = 0);\n-__device__ void cudaGraphSetConditional(cudaGraphConditionalHandle handle, unsigned int value);\n __host__ cudaError_t cudaGraphUpload(cudaGraphExec_t graphExec, cudaStream_t stream);\n-__host__ cudaError_t cudaUserObjectCreate(cudaUserObject_t* object_out, void* ptr, cudaHostFn_t destroy, unsigned int initialRefcount, unsigned int flags);\n-__host__ cudaError_t cudaUserObjectRelease(cudaUserObject_t object, unsigned int count = 1);\n-__host__ cudaError_t cudaUserObjectRetain(cudaUserObject_t object, unsigned int count = 1);\n</code></pre>"},{"location":"manual/api-runtime/#631-driver-entry-point-access","title":"6.31. Driver Entry Point Access","text":"<pre><code>-__host__ cudaError_t cudaGetDriverEntryPoint(const char* symbol, void** funcPtr, unsigned long long flags, cudaDriverEntryPointQueryResult ** driverStatus = NULL);\n-__host__ cudaError_t cudaGetDriverEntryPointByVersion(const char* symbol, void** funcPtr, unsigned int cudaVersion, unsigned long long flags, cudaDriverEntryPointQueryResult ** driverStatus = NULL);\n</code></pre>"},{"location":"manual/api-runtime/#632-library-management","title":"6.32. Library Management","text":"<pre><code>-__host__ cudaError_t cudaKernelSetAttributeForDevice(cudaKernel_t kernel, cudaFuncAttribute attr, int value, int device);\n-__host__ cudaError_t cudaLibraryEnumerateKernels(cudaKernel_t* kernels, unsigned int numKernels, cudaLibrary_t lib);\n-__host__ cudaError_t cudaLibraryGetGlobal(void** dptr, size_t* bytes, cudaLibrary_t library, const char* name);\n-__host__ cudaError_t cudaLibraryGetKernel(cudaKernel_t* pKernel, cudaLibrary_t library, const char* name);\n-__host__ cudaError_t cudaLibraryGetKernelCount(unsigned int* count, cudaLibrary_t lib);\n-__host__ cudaError_t cudaLibraryGetManaged(void** dptr, size_t* bytes, cudaLibrary_t library, const char* name);\n-__host__ cudaError_t cudaLibraryGetUnifiedFunction(void** fptr, cudaLibrary_t library, const char* symbol);\n-__host__ cudaError_t cudaLibraryLoadData(cudaLibrary_t* library, const void* code, cudaJitOption ** jitOptions, void** jitOptionsValues, unsigned int numJitOptions, cudaLibraryOption ** libraryOptions, void** libraryOptionValues, unsigned int numLibraryOptions);\n-__host__ cudaError_t cudaLibraryLoadFromFile(cudaLibrary_t* library, const char* fileName, cudaJitOption ** jitOptions, void** jitOptionsValues, unsigned int numJitOptions, cudaLibraryOption ** libraryOptions, void** libraryOptionValues, unsigned int numLibraryOptions);\n-__host__ cudaError_t cudaLibraryUnload(cudaLibrary_t library);\n</code></pre>"},{"location":"manual/api-runtime/#633-c-api-routines","title":"6.33. C++ API Routines","text":"<pre><code>-class __cudaOccupancyB2DHelper\n template &lt; class T &gt;__host__ cudaChannelFormatDesc cudaCreateChannelDesc(void);\n __host__ cudaError_t cudaEventCreate(cudaEvent_t* event, unsigned int flags);\n template &lt; class T &gt;__host__ cudaError_t cudaFuncGetAttributes(cudaFuncAttributes* attr, T* entry);\n template &lt; class T &gt;__host__ cudaError_t cudaFuncGetName(const char** name, T* func);\n template &lt; class T &gt;__host__ cudaError_t cudaFuncSetAttribute(T* func, cudaFuncAttribute attr, int value);\n template &lt; class T &gt;__host__ cudaError_t cudaFuncSetCacheConfig(T* func, cudaFuncCache cacheConfig);\n-template &lt; class T &gt;__host__ cudaError_t cudaGetKernel(cudaKernel_t* kernelPtr, T* func);\n template &lt; class T &gt;__host__ cudaError_t cudaGetSymbolAddress(void** devPtr, const T&amp; symbol);\n template &lt; class T &gt;__host__ cudaError_t cudaGetSymbolSize(size_t* size, const T&amp; symbol);\n template &lt; class T &gt;__host__ cudaError_t cudaGraphAddMemcpyNodeFromSymbol(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, void* dst, const T&amp; symbol, size_t count, size_t offset, cudaMemcpyKind kind);\n template &lt; class T &gt;__host__ cudaError_t cudaGraphAddMemcpyNodeToSymbol(cudaGraphNode_t* pGraphNode, cudaGraph_t graph, const cudaGraphNode_t* pDependencies, size_t numDependencies, const T&amp; symbol, const void* src, size_t count, size_t offset, cudaMemcpyKind kind);\n-template &lt; class T &gt;__host__ cudaError_t cudaGraphExecMemcpyNodeSetParamsFromSymbol(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, void* dst, const T&amp; symbol, size_t count, size_t offset, cudaMemcpyKind kind);\n-template &lt; class T &gt;__host__ cudaError_t cudaGraphExecMemcpyNodeSetParamsToSymbol(cudaGraphExec_t hGraphExec, cudaGraphNode_t node, const T&amp; symbol, const void* src, size_t count, size_t offset, cudaMemcpyKind kind);\n __host__ cudaError_t cudaGraphInstantiate(cudaGraphExec_t* pGraphExec, cudaGraph_t graph, cudaGraphNode_t* pErrorNode, char* pLogBuffer, size_t bufferSize);\n template &lt; class T &gt;__host__ cudaError_t cudaGraphMemcpyNodeSetParamsFromSymbol(cudaGraphNode_t node, void* dst, const T&amp; symbol, size_t count, size_t offset, cudaMemcpyKind kind);\n template &lt; class T &gt;__host__ cudaError_t cudaGraphMemcpyNodeSetParamsToSymbol(cudaGraphNode_t node, const T&amp; symbol, const void* src, size_t count, size_t offset, cudaMemcpyKind kind);\n template &lt; class T &gt;__host__ cudaError_t cudaLaunchCooperativeKernel(T* func, dim3 gridDim, dim3 blockDim, void** args, size_t sharedMem = 0, cudaStream_t stream = 0);\n template &lt; class T &gt;__host__ cudaError_t cudaLaunchKernel(T* func, dim3 gridDim, dim3 blockDim, void** args, size_t sharedMem = 0, cudaStream_t stream = 0);\n-template &lt; typename... ActTypes &gt;__host__ cudaError_t cudaLaunchKernelEx(const cudaLaunchConfig_t* config, const cudaKernel_t kernel, ActTypes &amp;&amp;... args);\n-template &lt; typename... ExpTypes, typename... ActTypes &gt;__host__ cudaError_t cudaLaunchKernelEx(const cudaLaunchConfig_t* config, void (*kernel)(ExpTypes...), ActTypes &amp;&amp;... args);\n-template &lt; class T &gt;__host__ cudaError_t cudaLibraryGetGlobal(T** dptr, size_t* bytes, cudaLibrary_t library, const char* name);\n-template &lt; class T &gt;__host__ cudaError_t cudaLibraryGetManaged(T** dptr, size_t* bytes, cudaLibrary_t library, const char* name);\n-template &lt; class T &gt;__host__ cudaError_t cudaLibraryGetUnifiedFunction(T** fptr, cudaLibrary_t library, const char* symbol);\n-__host__ cudaError_t cudaMallocAsync(void** ptr, size_t size, cudaMemPool_t memPool, cudaStream_t stream);\n __host__ cudaError_t cudaMallocHost(void** ptr, size_t size, unsigned int flags);\n template &lt; class T &gt;__host__ cudaError_t cudaMallocManaged(T** devPtr, size_t size, unsigned int flags = cudaMemAttachGlobal);\n-template &lt; typename T &gt;__host__ cudaError_t cudaMemDiscardAndPrefetchBatchAsync(T** dptrs, size_t* sizes, size_t count, cudaMemLocation prefetchLocs, unsigned long long flags, cudaStream_t stream);\n-template &lt; typename T &gt;__host__ cudaError_t cudaMemDiscardAndPrefetchBatchAsync(T** dptrs, size_t* sizes, size_t count, cudaMemLocation* prefetchLocs, size_t* prefetchLocIdxs, size_t numPrefetchLocs, unsigned long long flags, cudaStream_t stream);\n-template &lt; typename T &gt;__host__ cudaError_t cudaMemPrefetchBatchAsync(T** dptrs, size_t* sizes, size_t count, cudaMemLocation prefetchLocs, unsigned long long flags, cudaStream_t stream);\n-template &lt; typename T &gt;__host__ cudaError_t cudaMemPrefetchBatchAsync(T** dptrs, size_t* sizes, size_t count, cudaMemLocation* prefetchLocs, size_t* prefetchLocIdxs, size_t numPrefetchLocs, unsigned long long flags, cudaStream_t stream);\n-template &lt; typename T, typename U &gt;__host__ cudaError_t cudaMemcpyBatchAsync(const T** dsts, const U** srcs, const size_t* sizes, size_t count, cudaMemcpyAttributes attr, cudaStream_t hStream);\n-template &lt; typename T, typename U &gt;__host__ cudaError_t cudaMemcpyBatchAsync(const T** dsts, const U** srcs, const size_t* sizes, size_t count, cudaMemcpyAttributes* attrs, size_t* attrsIdxs, size_t numAttrs, cudaStream_t hStream);\n template &lt; class T &gt;__host__ cudaError_t cudaMemcpyFromSymbol(void* dst, const T&amp; symbol, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyDeviceToHost);\n template &lt; class T &gt;__host__ cudaError_t cudaMemcpyFromSymbolAsync(void* dst, const T&amp; symbol, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyDeviceToHost, cudaStream_t stream = 0);\n template &lt; class T &gt;__host__ cudaError_t cudaMemcpyToSymbol(const T&amp; symbol, const void* src, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyHostToDevice);\n template &lt; class T &gt;__host__ cudaError_t cudaMemcpyToSymbolAsync(const T&amp; symbol, const void* src, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyHostToDevice, cudaStream_t stream = 0);\n template &lt; class T &gt;__host__ cudaError_t cudaOccupancyAvailableDynamicSMemPerBlock(size_t* dynamicSmemSize, T* func, int numBlocks, int blockSize);\n template &lt; class T &gt;__host__ cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessor(int* numBlocks, T func, int blockSize, size_t dynamicSMemSize);\n template &lt; class T &gt;__host__ cudaError_t cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(int* numBlocks, T func, int blockSize, size_t dynamicSMemSize, unsigned int flags);\n-template &lt; class T &gt;__host__ cudaError_t cudaOccupancyMaxActiveClusters(int* numClusters, T* func, const cudaLaunchConfig_t* config);\n template &lt; class T &gt;__host__ cudaError_t cudaOccupancyMaxPotentialBlockSize(int* minGridSize, int* blockSize, T func, size_t dynamicSMemSize = 0, int blockSizeLimit = 0);\n template &lt; typename UnaryFunction, class T &gt;__host__ cudaError_t cudaOccupancyMaxPotentialBlockSizeVariableSMem(int* minGridSize, int* blockSize, T func, UnaryFunction blockSizeToDynamicSMemSize, int blockSizeLimit = 0);\n template &lt; typename UnaryFunction, class T &gt;__host__ cudaError_t cudaOccupancyMaxPotentialBlockSizeVariableSMemWithFlags(int* minGridSize, int* blockSize, T func, UnaryFunction blockSizeToDynamicSMemSize, int blockSizeLimit = 0, unsigned int flags = 0);\n template &lt; class T &gt;__host__ cudaError_t cudaOccupancyMaxPotentialBlockSizeWithFlags(int* minGridSize, int* blockSize, T func, size_t dynamicSMemSize = 0, int blockSizeLimit = 0, unsigned int flags = 0);\n-template &lt; class T &gt;__host__ cudaError_t cudaOccupancyMaxPotentialClusterSize(int* clusterSize, T* func, const cudaLaunchConfig_t* config);\n template &lt; class T &gt;__host__ cudaError_t cudaStreamAttachMemAsync(cudaStream_t stream, T* devPtr, size_t length = 0, unsigned int flags = cudaMemAttachSingle);\n</code></pre>"},{"location":"manual/api-runtime/#634-interactions-with-the-cuda-driver-api","title":"6.34. Interactions with the CUDA Driver API","text":"<pre><code>-__host__ cudaError_t cudaGetFuncBySymbol(cudaFunction_t* functionPtr, const void* symbolPtr);\n-__host__ cudaError_t cudaGetKernel(cudaKernel_t* kernelPtr, const void* entryFuncAddr);\n</code></pre>"},{"location":"manual/api-runtime/#635-profiler-control","title":"6.35. Profiler Control","text":"<pre><code> __host__ cudaError_t cudaProfilerStart(void);\n __host__ cudaError_t cudaProfilerStop(void);\n</code></pre>"},{"location":"manual/api-runtime/#636-data-types-used-by-cuda-runtime","title":"6.36. Data types used by CUDA Runtime","text":"<pre><code>-#define CUDA_EGL_MAX_PLANES\n #define CUDA_IPC_HANDLE_SIZE\n #define cudaArrayColorAttachment\n #define cudaArrayCubemap\n #define cudaArrayDefault\n #define cudaArrayDeferredMapping\n #define cudaArrayLayered\n #define cudaArraySparse\n #define cudaArraySparsePropertiesSingleMipTail\n #define cudaArraySurfaceLoadStore\n #define cudaArrayTextureGather\n #define cudaCpuDeviceId\n #define cudaDeviceBlockingSync\n #define cudaDeviceLmemResizeToMax\n #define cudaDeviceMapHost\n #define cudaDeviceMask\n #define cudaDeviceScheduleAuto\n #define cudaDeviceScheduleBlockingSync\n #define cudaDeviceScheduleMask\n #define cudaDeviceScheduleSpin\n #define cudaDeviceScheduleYield\n-#define cudaDeviceSyncMemops\n #define cudaEventBlockingSync\n #define cudaEventDefault\n #define cudaEventDisableTiming\n #define cudaEventInterprocess\n #define cudaEventRecordDefault\n #define cudaEventRecordExternal\n #define cudaEventWaitDefault\n #define cudaEventWaitExternal\n-#define cudaExternalMemoryDedicated\n-#define cudaExternalSemaphoreSignalSkipNvSciBufMemSync\n-#define cudaExternalSemaphoreWaitSkipNvSciBufMemSync\n #define cudaGraphKernelNodePortDefault\n #define cudaGraphKernelNodePortLaunchCompletion\n #define cudaGraphKernelNodePortProgrammatic\n #define cudaHostAllocDefault\n #define cudaHostAllocMapped\n #define cudaHostAllocPortable\n #define cudaHostAllocWriteCombined\n #define cudaHostRegisterDefault\n #define cudaHostRegisterIoMemory\n #define cudaHostRegisterMapped\n #define cudaHostRegisterPortable\n #define cudaHostRegisterReadOnly\n #define cudaInitDeviceFlagsAreValid\n #define cudaInvalidDeviceId\n #define cudaIpcMemLazyEnablePeerAccess\n #define cudaMemAttachGlobal\n #define cudaMemAttachHost\n #define cudaMemAttachSingle\n-#define cudaMemPoolCreateUsageHwDecompress\n-#define cudaNvSciSyncAttrSignal\n-#define cudaNvSciSyncAttrWait\n #define cudaOccupancyDefault\n #define cudaOccupancyDisableCachingOverride\n #define cudaPeerAccessDefault\n #define cudaStreamDefault\n #define cudaStreamLegacy\n #define cudaStreamNonBlocking\n #define cudaStreamPerThread\n enum cudaAccessProperty\n-enum cudaAsyncNotificationType\n-enum cudaAtomicOperation\n-enum cudaAtomicOperationCapability\n-enum cudaCGScope\n enum cudaChannelFormatKind\n enum cudaClusterSchedulingPolicy\n enum cudaComputeMode\n-enum cudaDeviceAttr\n-enum cudaDeviceNumaConfig\n-enum cudaDeviceP2PAttr\n enum cudaDriverEntryPointQueryResult\n-enum cudaEglColorFormat\n-enum cudaEglFrameType\n-enum cudaEglResourceLocationFlags\n-enum cudaError\n-enum cudaExternalMemoryHandleType\n-enum cudaExternalSemaphoreHandleType\n-enum cudaFlushGPUDirectRDMAWritesOptions\n-enum cudaFlushGPUDirectRDMAWritesScope\n-enum cudaFlushGPUDirectRDMAWritesTarget\n enum cudaFuncAttribute\n enum cudaFuncCache\n-enum cudaGPUDirectRDMAWritesOrdering\n enum cudaGetDriverEntryPointFlags\n-enum cudaGraphChildGraphNodeOwnership\n-enum cudaGraphConditionalNodeType\n-enum cudaGraphDebugDotFlags\n-enum cudaGraphDependencyType\n enum cudaGraphExecUpdateResult\n-enum cudaGraphInstantiateFlags\n-enum cudaGraphInstantiateResult\n-enum cudaGraphKernelNodeField\n-enum cudaGraphMemAttributeType\n enum cudaGraphNodeType\n enum cudaGraphicsCubeFace\n enum cudaGraphicsMapFlags\n enum cudaGraphicsRegisterFlags\n-enum cudaJitOption\n-enum cudaJit_CacheMode\n-enum cudaJit_Fallback\n-enum cudaLaunchAttributeID\n enum cudaLaunchMemSyncDomain\n-enum cudaLibraryOption\n enum cudaLimit\n enum cudaMemAccessFlags\n enum cudaMemAllocationHandleType\n-enum cudaMemAllocationType\n-enum cudaMemLocationType\n enum cudaMemPoolAttr\n enum cudaMemRangeAttribute\n-enum cudaMemcpy3DOperandType\n-enum cudaMemcpyFlags\n enum cudaMemcpyKind\n enum cudaMemoryAdvise\n enum cudaMemoryType\n enum cudaResourceType\n enum cudaResourceViewFormat\n enum cudaSharedCarveout\n enum cudaSharedMemConfig\n enum cudaStreamCaptureMode\n enum cudaStreamCaptureStatus\n-enum cudaStreamUpdateCaptureDependenciesFlags\n-enum cudaSurfaceBoundaryMode\n-enum cudaSurfaceFormatMode\n enum cudaTextureAddressMode\n enum cudaTextureFilterMode\n enum cudaTextureReadMode\n-enum cudaUserObjectFlags\n-enum cudaUserObjectRetainFlags\n struct CUuuid_st\n struct cudaAccessPolicyWindow\n struct cudaArrayMemoryRequirements\n struct cudaArraySparseProperties\n-struct cudaAsyncNotificationInfo_t\n struct cudaChannelFormatDesc\n-struct cudaChildGraphNodeParams\n-struct cudaConditionalNodeParams\n struct cudaDeviceProp\n-struct cudaEglFrame\n-struct cudaEglPlaneDesc\n-struct cudaEventRecordNodeParams\n-struct cudaEventWaitNodeParams\n struct cudaExtent\n-struct cudaExternalMemoryBufferDesc\n-struct cudaExternalMemoryHandleDesc\n-struct cudaExternalMemoryMipmappedArrayDesc\n-struct cudaExternalSemaphoreHandleDesc\n-struct cudaExternalSemaphoreSignalNodeParams\n-struct cudaExternalSemaphoreSignalNodeParamsV2\n-struct cudaExternalSemaphoreSignalParams\n-struct cudaExternalSemaphoreWaitNodeParams\n-struct cudaExternalSemaphoreWaitNodeParamsV2\n-struct cudaExternalSemaphoreWaitParams\n struct cudaFuncAttributes\n-struct cudaGraphEdgeData\n struct cudaGraphExecUpdateResultInfo\n-struct cudaGraphInstantiateParams\n-struct cudaGraphKernelNodeUpdate\n-struct cudaGraphNodeParams\n struct cudaHostNodeParams\n-struct cudaHostNodeParamsV2\n struct cudaIpcEventHandle_t\n struct cudaIpcMemHandle_t\n struct cudaKernelNodeParams\n-struct cudaKernelNodeParamsV2\n-struct cudaLaunchAttribute\n union cudaLaunchAttributeValue\n struct cudaLaunchConfig_t\n-struct cudaLaunchMemSyncDomainMap\n struct cudaMemAccessDesc\n-struct cudaMemAllocNodeParams\n-struct cudaMemAllocNodeParamsV2\n-struct cudaMemFreeNodeParams\n struct cudaMemLocation\n struct cudaMemPoolProps\n-struct cudaMemPoolPtrExportData\n-struct cudaMemcpy3DOperand\n struct cudaMemcpy3DParms\n struct cudaMemcpy3DPeerParms\n-struct cudaMemcpyAttributes\n-struct cudaMemcpyNodeParams\n struct cudaMemsetParams\n-struct cudaMemsetParamsV2\n-struct cudaOffset3D\n struct cudaPitchedPtr\n struct cudaPointerAttributes\n struct cudaPos\n struct cudaResourceDesc\n struct cudaResourceViewDesc\n struct cudaTextureDesc\n-typedef cudaArray * cudaArray_const_t;\n typedef cudaArray * cudaArray_t;\n-typedef cudaAsyncCallbackEntry * cudaAsyncCallbackHandle_t;\n-typedef CUeglStreamConnection_st * cudaEglStreamConnection;\n typedef enum cudaError cudaError_t;\n typedef CUevent_st * cudaEvent_t;\n-typedef CUexternalMemory_st * cudaExternalMemory_t;\n-typedef CUexternalSemaphore_st * cudaExternalSemaphore_t;\n-typedef CUfunc_st * cudaFunction_t;\n-typedef unsigned long long cudaGraphConditionalHandle;\n typedef CUgraphDeviceUpdatableNode_st * cudaGraphDeviceNode_t;\n typedef CUgraphExec_st * cudaGraphExec_t;\n typedef CUgraphNode_st * cudaGraphNode_t;\n typedef CUgraph_st * cudaGraph_t;\n typedef cudaGraphicsResource * cudaGraphicsResource_t;\n typedef void(* cudaHostFn_t)(void* userData);\n-typedef CUkern_st * cudaKernel_t;\n-typedef CUlib_st * cudaLibrary_t;\n typedef CUmemPoolHandle_st * cudaMemPool_t;\n-typedef cudaMipmappedArray * cudaMipmappedArray_const_t;\n typedef cudaMipmappedArray * cudaMipmappedArray_t;\n typedef CUstream_st * cudaStream_t;\n-typedef unsigned long long cudaSurfaceObject_t;\n-typedef unsigned long long cudaTextureObject_t;\n-typedef CUuserObject_st * cudaUserObject_t;\n</code></pre>"},{"location":"manual/apis/","title":"API Coverage Report","text":"<p>These pages provide a diff between SCALE's headers and the NVIDIA documentation, describing which APIs are supported by SCALE.</p> <ul> <li>Driver API</li> <li>Math API</li> <li>Runtime API</li> </ul> <p>The lists are based on the official Nvidia documentation and use the same layout.</p>"},{"location":"manual/apis/#presentation","title":"Presentation","text":"<p>The lists are presented using <code>diff</code> syntax highlighting of code blocks. This allows seeing which entries are available and which may be missing. Missing entries are prefixed with <code>-</code> (a minus) which paints them red in the list.</p> <p>By default, <code>__host__</code> qualifier is assumed for functions, it is removed if present. Functions that are qualified as <code>__host__ __device__</code> are split into two separate entries.</p> <p>Here is an example:</p> <pre><code>const char * cudaGetErrorName(cudaError_t);\n__device__ const char * cudaGetErrorName(cudaError_t);\nconst char * cudaGetErrorString(cudaError_t);\n__device__ const char * cudaGetErrorString(cudaError_t);\ncudaError_t cudaGetLastError();\n-__device__ cudaError_t cudaGetLastError();\ncudaError_t cudaPeekAtLastError();\n-__device__ cudaError_t cudaPeekAtLastError();\n</code></pre> <p>In this example, functions <code>cudaGetErrorName</code> and <code>cudaGetErrorString</code> are available on both host and device. Functions <code>cudaGetLastError</code> and <code>cudaPeekAtLastError</code> are available on host, and are not available on device.</p>"},{"location":"manual/apis/#correctness","title":"Correctness","text":"<p>The lists may say that something is unavailable when it's not the case. This may happen for a few reasons.</p> <p>NVIDIA documentation may differ from what CUDA provides in reality. An example of that is differences in <code>const</code>-ness of some function arguments. In such cases SCALE may be forced to maintain \"bug compatibility\" and the functions stop matching what NVIDIA documentation promises.</p> <p>Many functions are called conditionally and may never get used in certain scenarios. For some of those functions, SCALE may provide an empty implementation. By doing this, SCALE allows more projects pass compilation and linking. We don't want to list such empty functions as available, so we manually mark them as missing to avoid confusion.</p> <p>SCALE retains support for some old APIs NVIDIA have since deleted.</p> <p>Found a mistake?</p> <p>Let us know, or file a pull request</p>"},{"location":"manual/comparison/","title":"Comparison to other solutions","text":""},{"location":"manual/comparison/#hip","title":"HIP","text":"<p>HIP is AMD's answer to CUDA. It is superficially similar to CUDA, providing a similar programming language and similar APIs. An automatic <code>hipify</code> tool exists to partially automate the process of rewriting your code from CUDA to HIP.</p> <p>We believe HIP does not solve the \"CUDA compatibility problem\" because:</p> <ul> <li>HIP does not address the  CUDA dialect problem.   HIP's language is almost identical to LLVM-dialect CUDA, which is   quite different from the dialect   of CUDA accepted by NVIDIA <code>nvcc</code>. Consequently, many CUDA programs fail in   strange ways after porting, if they compile at all.</li> <li>HIP has no support for inline PTX <code>asm</code> blocks in CUDA code. These must be   manually removed or guarded by macros. SCALE simply accepts them and   compiles them for AMD.</li> <li>HIP's support for NVIDIA is via wrapper APIs rather than simply using   NVIDIA's tools directly as a SCALE-based solution does.</li> <li><code>hipify</code> is unable to handle many CUDA code constructs, such as complex   macros.</li> <li>HIP runtime APIs have subtly different semantics than the corresponding   CUDA APIs. <code>hipify</code> operates under the incorrect assumption that all   <code>cudaFoo()</code> can be mapped to <code>hipFoo()</code> to achieve the same effect, which   is not the case. SCALE aims to carefully reproduce the behaviour of   NVIDIA's CUDA APIs.</li> </ul> <p>Most projects that use HIP mitigate these issues by maintaining separate HIP and  CUDA codebases, or one codebase that converts to HIP or CUDA via complex preprocessor macros. This significantly increases maintainence costs.</p> <p>We relatively often encounter projects with a significant performance or correctness discrepancy between their CUDA and HIP editions, because one or the other gets more attention, or gets changes merged more promptly. Such source fragmentation is bad for everyone.</p>"},{"location":"manual/comparison/#zluda","title":"ZLUDA","text":"<p>ZLUDA is a PTX JIT for AMD GPUs. On program startup, ZLUDA grabs the PTX from the CUDA binary and compiles it for your AMD GPU.</p> <p>ZLUDA is a useful tool for end-users to run CUDA programs on otherwise-unsupported GPUs, without the involvement of the authors of the program (or even access to the source code!).</p> <p>There are some downsides:</p> <ul> <li>JIT on startup can lead to startup-time delays.</li> <li>Reliance on dll-injection is a bit \"hacky\", and tends to make antivirus   software angry.</li> <li>ZLUDA's approach to providing AMD support inherently depends on tools   provided by NVIDIA. NVIDIA controls the design of the PTX language and the   compilers that produce it, and manipulate both to optimise outcomes for   their hardware.</li> <li>Compiling source code directly to AMDGPU machine code should   offer greater opportunities for optimisation than working backwards from   PTX that has already been optimised for a specific NVIDIA target. ZLUDA   has to deal with many of the same problems that a decompiler does, which   are intractible in general.</li> <li>ZLUDA does not support AMD GPUs with a wave size of 64, which includes   many exciting datacenter devices such as the MI300.</li> </ul> <p>We believe that ZLUDA fills a useful niche, but that software distributors should have the power to compile their CUDA source code directly to the machine code of multiple GPU vendors, without reliance on tools maintained by NVIDIA.</p>"},{"location":"manual/compute-capabilities/","title":"Compute Capability Mapping","text":"<p>\"Compute capability\" is a numbering system used by NVIDIA's CUDA tools to represent different GPU targets. The value of the <code>__CUDA_ARCH__</code> macro is derived from this, and it's how you communicate with <code>nvcc</code> to request the target to build for.</p> <p>GPUs from other vendors have their own numbering scheme, such as AMD's <code>gfx1234</code> format.</p> <p>CUDA projects sometimes do numeric comparisons on the compute capability value to enable/disable features using the preprocessor. This is a problem, since those comparisons are inherently meaningless when targeting non-NVIDIA hardware.</p> <p>There is no meaningful mapping between compute capability numbers and the hardware of other vendors.</p> <p>SCALE addresses this problem by providing a \"CUDA installation directory\" for each supported GPU target. By default, the <code>nvcc</code> in each of these directories maps every compute capability number to the corresponding AMD GPU target.</p> <p>That works fine for single-architecture builds and most simple cases, but has two obvious flaws: - Building for multiple architectures is unrepresentable. - nvRTC code may use arbitrary logic to choose a different target.</p> <p>SCALE provides two tools to address this issue: - The compiler and RTC APIs accept AMD architectures eg. <code>-arch gfx1100</code>, so   you can modify your build scripts or nvRTC code to explicitly ask for the   correct architecture, entirely opting-out of the mapping tricks. - ccmap configuration, which allows you to use config files or environment   variables to create an arbitrary mapping between compute capability numbers   and real GPU architectures.</p>"},{"location":"manual/compute-capabilities/#configuration-file-format","title":"Configuration file format","text":"<p>The configuration file provides the answer to two questions:</p> <ul> <li>For a given compute capability, which GPU should we compile for?</li> <li>Given a device binary, which compute capability should we say it has?</li> </ul> <p>Each line consists of a GPU architecture, a space, and a compute capbility number. Entries are tried in order, and the first applicable one is used, so it's possible to unambiguously map more than one ISA and compute capability to each other.</p> <p>A line consisting of just a GPU architecture name is a wildcard which maps all remaining compute capilibite sto that GPU architecture.</p> <p>Lines starting with <code>#</code> are comments.</p>"},{"location":"manual/compute-capabilities/#example","title":"Example","text":"<pre><code># The library will report compute capability 6.1 for gfx900 devices. The compiler will use gfx900 for `sm_61` or\n# `compute_61`.\ngfx900 61\n\n# The library will report compute capability 8.6 for gfx1030 devices. The compiler will use gfx1030 for any of `sm_80`,\n# `compute_80`, `sm_86`, or `compute_86`.\ngfx1030 86\ngfx1030 80\n\n# The compiler will use gfx1100 for any compute capability other than 6.1, 8.0, or 8.6.\ngfx1100\n</code></pre>"},{"location":"manual/compute-capabilities/#search-locations-for-the-library","title":"Search locations for the library","text":"<p>The library searches for a compute capability map in the following order:</p> <ul> <li>The file pointed the <code>SCALE_CCMAP</code> environment variable.</li> <li><code>../share/scale/ccmap.conf</code> relative to the directory containing <code>libredscale.so</code>.</li> <li><code>${HOME}/.scale/ccmap.conf</code></li> <li><code>/etc/scale/ccmap.conf</code></li> </ul>"},{"location":"manual/compute-capabilities/#search-locations-for-the-compiler","title":"Search locations for the compiler","text":"<p>The compiler searches for a compute capability map in the following order:</p> <ul> <li>The file pointed to by the <code>--cuda-ccmap</code> flag.</li> <li>The file pointed the <code>SCALE_CCMAP</code> environment variable.</li> <li><code>../share/scale/ccmap.conf</code> relative to the directory containing the   compiler executable.</li> </ul>"},{"location":"manual/diagnostic-flags/","title":"Compiler warnings","text":"<p>There are some differences in how NVIDIA's <code>nvcc</code> and the SCALE compiler in \"nvcc mode\" interpret compiler options relatig to warnings.</p>"},{"location":"manual/diagnostic-flags/#clang-flags","title":"<code>clang++</code> flags","text":"<p>The SCALE compiler accepts all of <code>clang++</code>'s usual flags in addition to those provided by nvcc, except where doing so would create an ambiguity.</p>"},{"location":"manual/diagnostic-flags/#compiler-warnings_1","title":"Compiler warnings","text":"<p>The SCALE compiler has the same default warning behaviour as <code>clang</code>, which is somewhat more strict than <code>nvcc</code>. Warnings may be disabled with the usual <code>-Wno-</code> flags documented in the clang diagnostics reference.</p> <p>There may be value in enabling even more warnings to find further issues and improve your code.</p> <p>Note that the end of every compiler warning message tells you the name of the warning flag it is associated with, such as:</p> <pre><code>warning: implicit conversion from 'int' to 'float' changes value from\n2147483647 to 2147483648 [-Wimplicit-const-int-float-conversion]\n</code></pre> <p>By changing <code>-W</code> to <code>-Wno-</code>, you obtain the flag required to disable that warning.</p> <p>The SCALE implementation of the CUDA runtime/driver APIs uses <code>[[nodiscard]]</code> for the error return codes, meaning you'll get a warning from code that ignores potential errors from CUDA APIs. This warning can be disabled via <code>-Wno-unused-value</code>.</p>"},{"location":"manual/diagnostic-flags/#-werror","title":"<code>-Werror</code>","text":"<p><code>nvcc</code>'s <code>-Werror</code> takes an argument specifying the types of warnings that should be errors, such as:</p> <pre><code>-Werror reorder,default-stream-launch\n</code></pre> <p>This differs from clang's syntax, which consists of either a lone <code>-Werror</code> to make all warnings into errors, or a set of <code>-Werror=name</code> flags to make specific things into errors.</p> <p>In <code>nvcc</code> mode, the SCALE compiler accepts only the <code>nvcc</code> syntax, but allows the same set of diagnostic names accepted by <code>clang</code> (as well as the special names supported by NVIDIA's <code>nvcc</code>). For example:</p> <pre><code>nvcc -Werror=documentation,implicit-int-conversion foo.cu\nclang++ -Werror=documentation -Werror=implicit-int-conversion foo.cu\n</code></pre> <p>Since SCALE enables more warnings than nvcc does by default, many projects using <code>-Werror</code> with nvcc will not compile without one of:</p> <ul> <li>Disabling <code>-Werror</code>.</li> <li>Disabling the corresponding warnings.</li> <li>Using diagnostic pragmas to disable the corresponding warnings in a region   of code.</li> </ul>"},{"location":"manual/diagnostic-flags/#diagnostic-control-pragmas","title":"Diagnostic control pragmas","text":"<p>The SCALE compiler does not currently support <code>#pragma nv_diag_suppress</code> or <code>#pragma diag_suppress</code> because the set of integers accepted by these pragmas does not appear to be documented, so we do not know which diagnostics should be controlled by which pragmas. Using these pragmas in your program will produce an \"unrecognised pragma ignored\" warning, which can itself be disabled with <code>-Wno-unknown-pragmas</code>.</p> <p>SCALE supports clang-style diagnostic pragmas, as documented here. This can be combined with preprocessor macros to achieve the desired effect:</p> <pre><code>#if defined(__clang__) // All clang-like compilers, including SCALE.\n#pragma clang diagnostic ignored \"-Wunused-result\"\n#elif defined(__NVCC__) // NVCC, but not clang. AKA: nvidia's one.\n#pragma nv_diag_suppress ...\n#endif\n</code></pre>"},{"location":"manual/diagnostic-reference/","title":"Compiler Diagnostics Reference","text":"<p>This page documents the meaning of various compiler diagnostics provided by SCALE which are not found in other compilers. Diagnostics not listed here are provided by clang, and may be found in the Clang Compiler Diagnostics Reference</p> <p>As with all diagnostics, these can be enabled or disabled both globally and in a particular region of code. See diagnostic flags reference</p> <p>Since many of them represent undefined behaviour even on NVIDIA platforms, fixing the underlying problem is recommended.</p>"},{"location":"manual/diagnostic-reference/#-wptx-binding-as-address","title":"<code>-Wptx-binding-as-address</code>","text":"<p>Any pointer argument to a PTX <code>asm()</code> statement is passed as a generic address. It is therefore invlaid to directly use an <code>asm()</code> input as an address operand to any PTX instruction that doesn't use generic addressing. Such code will work correctly any time the generic address space has identical layout to the target address space of the instruction (as is relatively often the case for <code>global</code>, for example), but will fail randomly on some targets.</p> <p>To achieve correct behaviour across all GPUs, use the <code>cvta</code> PTX instruction to convert the incoming pointer to the desired address space before passing it to the PTX memory instruction. In cases where this conversion is a no-op, the optimiser will remove this extra step (including with NVIDIA's compiler!).</p> <p>When the conversion is not a no-op, both SCALE and NVIDIA <code>nvcc</code> have compiler optimisations that attempt to deduce the address space of the pointer and rewrite it into the target address space for its entire lifetime.</p>"},{"location":"manual/diagnostic-reference/#-wptx-unused-local-variable","title":"<code>-Wptx-unused-local-variable</code>","text":"<p>Identifies unused local variables (<code>.reg</code> declarations) in  PTX<code>asm</code>.</p>"},{"location":"manual/diagnostic-reference/#-wptx-local-variable-leak","title":"<code>-Wptx-local-variable-leak</code>","text":"<p>Identifies PTX variable declarations that may lead to <code>ptxas</code> failures when compiling for NVIDIA.</p> <p>When a device function contains a PTX variable declaration, repeated inlining of calls to it may lead to duplicate variable declarations in the generated PTX.</p> <pre><code>__device__ void suffering() {\n    asm(\".reg .b32 pain;\");\n}\n\n__global__ void explode() {\n    // The function body will inline twice, causing this kernel's final PTX to\n    // contain two declarations of the same PTX variable. This produces a\n    // confusing ptxas error.\n    suffering();\n    suffering();\n}\n</code></pre> <p>To resolve this, all device functions that make PTX <code>.reg</code> declarations should enclose them in PTX <code>{}</code>. This limits the scope of the inlined variable declarartion to the inlined functoin body, allowing multiple copies to coexist.</p> <p>This issue can never cause a problem when building for AMD targets.</p>"},{"location":"manual/diagnostic-reference/#-wptx-wave64","title":"<code>-Wptx-wave64</code>","text":"<p>Detects hardcoded lanemask operands that have all zeros in the top 32 bits when compiling for native wave64 mode. Such code is likely a mistake, such as hardcoding <code>0xFFFFFFFF</code> for a ballot's mask argument instead of the more portable <code>-1</code>. On a wave64 target, <code>0xFFFFFFFF</code> is really <code>0x00000000FFFFFFFF</code>, turning off half the warp, when the intent was likely to turn every thread on.</p> <p>Note that SCALE's default compilation mode is to emulate a warp size of 32 on all targets, so you can usually ignore this class of problems initially. Most programs don't suffer a measurable performance degredation from this emulation process, but certain patterns (such as sending alternating warps down different control flow paths) would be pathological. It is desirable to migrate your code to be truly warp-size portable.</p>"},{"location":"manual/diagnostic-reference/#errors-relating-to-the-ptx-carry-bit","title":"Errors relating to the PTX carry bit","text":"<p>PTX offers extended-precision integr math instructions, with implicit carry-out and carry-in. However, the PTX manual notes:</p> <p>The condition code register is not preserved across calls and is mainly intended for use in straight-line code sequences for computing extended-precision integer addition, subtraction, and multiplication.</p> <p>It is therefore undefined behaviour to end one device function with a PTX <code>asm()</code> statement that writes the carry-bit, and then try to read that stored carry-bit using an <code>asm()</code> statement at the start of another device function. If you need to write that kind of code, you can either:</p> <ul> <li>Use compiler intrinsics to access add-with-carry operations more directly.</li> <li>Use int128 types (where possible) to avoid having to have this kind of asm   entirely.</li> <li>Use macros instead of functions for affected regions of code.</li> <li>Refactor so both the reader and writer of the carry bit are in the same   device function.</li> </ul> <p>When compiling for NVIDIA, such code will usually work if the functions inline and the asm blocks end up adjacent (so there is no actual function call to discard the carry bit). This is optimiser-dependent behaviour, and will fail if the compiler decides to reorder code or not perform the inlining.</p> <p>When compiling for AMD with SCALE, we cannot create that behaviour, so it is simply an error to attempt to return the carry-bit.</p>"},{"location":"manual/dialects/","title":"CUDA Dialects","text":"<p>The CUDA programming language is not formally specified. The \"standard\" is  therefore approximately \"whatever <code>nvcc</code> does\". Although <code>clang</code> supports  compiling CUDA, it supports a somewhat different dialect compared to <code>nvcc</code>.</p> <p>You can read more about (some of) the specific differences in the LLVM manual page about it.</p> <p>This leads to a problem: most CUDA code is written with <code>nvcc</code> in mind, but  the only open-source compiler available with a CUDA frontend is <code>clang</code>. Many  real-world CUDA programs cannot be successfully compiled with <code>clang</code>  because they depend on <code>nvcc</code>'s behaviour.</p> <p>HIP experiences the same problem: the HIP compiler is based on LLVM, so HIP  is closer to \"LLVM-dialect CUDA\" than it is to \"nvcc-dialect CUDA\". This  causes some CUDA programs to fail in \"interesting\" ways when ported to HIP. It's not really the case that you can remap all the API calls to the HIP ones and expect it to work: nvcc-CUDA, and LLVM-CUDA/HIP have quite different C++  semantics.</p> <p>SCALE resolves this issue by offering two compilers:</p> <ul> <li><code>\"nvcc\"</code>: a clang frontend that replicates the behaviour of NVIDIA's <code>nvcc</code>,    allowing existing CUDA programs to compile directly. This is similar to how   LLVM achieves MSVC compatibility by providing <code>clang-cl</code>.</li> <li><code>clang</code>: providing clang's usual clang-dialect-CUDA support, with our    opt-in language extensions.</li> </ul> <p>Existing projects can be compiled without modification using the  <code>nvcc</code>-equivalent compiler. Users of clang-dialect CUDA may use the provided  clang compiler to compile for either platform.</p>"},{"location":"manual/differences/","title":"Differences from NVIDIA CUDA","text":"<p>There are some areas where SCALE's implementation of a certain feature also found in NVIDIA CUDA has different behaviour. This document does not enumerate missing CUDA APIs/features.</p>"},{"location":"manual/differences/#defects","title":"Defects","text":""},{"location":"manual/differences/#nvrtc-differences","title":"NVRTC differences","text":"<p>SCALE's current implementation of the nvrtc API works by calling the compiler as a subprocess instead of a library. This differs from how NVIDIA's implementation works, and means that the library must be able to locate the compiler to invoke it.</p> <p>If your program uses the rtc APIs and fails with errors that relate to being  unable to locate the compiler, ensure that SCALE's <code>nvcc</code> is first in PATH.</p>"},{"location":"manual/differences/#stream-synchronization","title":"Stream synchronization","text":"<p>SCALE does not yet support per-thread default stream behaviour.</p> <p>Instead, the default stream is used in place of the per-thread default stream. This will not break programs, but is likely to reduce performance.</p> <p>A workaround which will also slightly improve the performance of your program when run on NVIDIA GPUs is to use nonblocking CUDA streams explicitly, rather than relying on the implicit CUDA stream.</p>"},{"location":"manual/differences/#host-side-__half-support","title":"Host-side <code>__half</code> support","text":"<p>The CUDA API allows many <code>__half</code> math functions to be used on both host and  device.</p> <p>When compiling non-CUDA translation units, you can include <code>&lt;cuda_fp16.h&gt;</code>  and use the <code>__half</code> math APIs in host code. When you do this, NVIDIA's CUDA  implementation converts the <code>__half</code> to 32-bit <code>float</code>, does the calculation, and converts back.</p> <p>SCALE only allows these functions to be used on the host when the host compiler  supports compiling fp16 code directly (via the <code>_Float16</code> type). Current versions of gcc and clang both support this.</p> <p>This difference only applies to non-CUDA translation units using compilers at least 2 years old.</p> <p>This means:</p> <ul> <li>All <code>__half</code> APIs work in both host and device code in <code>.cu</code> files.</li> <li><code>__half</code> APIs that perform floating point math will not compile in host    code in non-CUDA translation units if an old host compiler is used.</li> <li>The outcome of <code>__half</code> calculations on host/device will always be the same.</li> <li>APIs for using <code>__half</code> as a storage type are always supported.</li> </ul> <p>SCALE bundles a modern host compiler at <code>&lt;SCALE_DIR&gt;/targets/gfxXXX/bin/clang++</code>  you can use as a workaround if this edgecase becomes a problem.</p>"},{"location":"manual/differences/#enhancements","title":"Enhancements","text":""},{"location":"manual/differences/#contexts-where-cuda-apis-are-forbidden","title":"Contexts where CUDA APIs are forbidden","text":"<p>NVIDIA's implementation forbids CUDA APIs in various contexts, such as from host-side functions enqueued onto streams.</p> <p>This implementation allows CUDA API calls in such cases.</p>"},{"location":"manual/differences/#static-initialization-and-deinitialization","title":"Static initialization and deinitialization","text":"<p>This implementation permits the use of CUDA API functions during global static initialization and <code>thread_local</code> static initialization.</p> <p>It is not permitted to use CUDA API functions during static deinitialization.</p> <p>This is more permissive than what is allowed by NVIDIA's implementation.</p>"},{"location":"manual/differences/#device-printf","title":"Device <code>printf</code>","text":"<p>SCALE's device <code>printf</code> accepts an unlimited number of arguments if you compile with at least C++11.</p> <p>If you target an older version of C++ then it is limited to 32, like NVIDIA's implementation.</p>"},{"location":"manual/differences/#contexts","title":"Contexts","text":"<p>If <code>cuCtxDestroy()</code> is used to destroy the context that is current to a different CPU thread, and that CPU thread then issues an API call that depends on the context without first setting a different context to be current, the behaviour is undefined.</p> <p>In NVIDIA's implementation, this condition returns <code>CUDA_ERROR_CONTEXT_IS_DESTROYED</code>.</p> <p>Matching NVIDIA's behaviour would have incurred a small performance penalty on many operations to handle an edgecase that is not permitted.</p>"},{"location":"manual/differences/#kernel-argument-size","title":"Kernel argument size","text":"<p>SCALE accepts kernel arguments up to 2GB, whereas NVIDIA CUDA allows only  32kb (and 4kb before version 12.1).</p> <p>This is more an implementation quirk than a feature, since huge kernel  arguments are unlikely to perform well compared to achieving the same effect  with async copies, memory mapping, etc.</p>"},{"location":"manual/faq/","title":"Frequently asked questions","text":""},{"location":"manual/faq/#how-do-i-report-a-problem","title":"How do I report a problem?","text":"<p>Go ahead and contact us if facing problems.</p> <p>Bug reports - no matter how small - accelerate the SCALE project.</p> <p>Let's work together to democratise the GPU market!</p>"},{"location":"manual/faq/#is-scale-free","title":"Is SCALE free?","text":"<p>SCALE is free for non-commercial use including research and academia. </p> <p>For commercial use, a license agreement is required. </p> <p>To learn more about licencing SCALE, we invite you to read this or contact us.</p>"},{"location":"manual/faq/#does-scale-increase-performance","title":"Does SCALE increase performance?","text":"<p>In many cases, yes, it does. </p> <p>Seeking to 'reduce compute cost' and/or 'increase performance' are good reasons to explore SCALE. That said, these depend on the specific compute workload in question and benchmarks will be different from one CUDA project to the other.</p> <p>For the latest performance benchmarks, see this section of our website.</p>"},{"location":"manual/faq/#when-will-some-gpu-be-supported","title":"When will <code>&lt;some GPU&gt;</code> be supported?","text":"<p>Expanding the set of supported GPUs is an ongoing process. We already support some GPUs that vendors has dropped support for (eg. AMD MI25 / gfx900), and are working to expand the set further. If there's a GPU you want to bring to our attention, please get in touch.</p>"},{"location":"manual/faq/#when-will-some-cuda-api-be-supported","title":"When will <code>&lt;some CUDA API&gt;</code> be supported?","text":"<p>We prioritise CUDA APIs based on the number (and popularity) of third-party projects requiring the missing API.</p> <p>If you'd like to bring a missing API to our attention, contact us.</p>"},{"location":"manual/faq/#does-scale-infringe-nvidias-copyright","title":"Does SCALE infringe NVIDIA\u2019s copyright?","text":"<p>By design, SCALE does not infringe NVIDIA\u2019s EULAs or copyright. </p> <p>We think CUDA is amazing and we follow the guidelines set by NVIDIA. </p> <p>Check out this post for an elaborate explanation.</p>"},{"location":"manual/faq/#cant-nvidia-just-change-cuda-and-break-scale","title":"Can't NVIDIA just change CUDA and break SCALE?","text":"<p>Of course, we have no control over what NVIDIA does with the CUDA toolkit.</p> <p>Although it is possible for NVIDIA to change/remove APIs in CUDA or PTX, doing so would break every CUDA program that uses these functions. Those programs would then be broken on both SCALE and NVIDIA's platform. It seems unlikely that NVIDIA would do something that breaks existing programs.</p> <p>NVIDIA can add new things to CUDA which we don't support. Projects are free to choose whether or not to use any new features that are added in the future, and may choose to use feature detection macros to conditionalise dependence on non-essential new features. Projects face a similar choice when deciding whether or not to use SCALE's steadily growing set of features that go beyond NVIDIA's CUDA.</p>"},{"location":"manual/faq/#does-scale-depend-on-nvidias-compilerassembleretc","title":"Does SCALE depend on NVIDIA's compiler/assembler/etc.?","text":"<p>No.</p> <p>Although much of this manual talks about \"nvcc\", it is important to understand the distinction between the two things this can refer to:</p> <ul> <li>The SCALE compiler, which is named \"nvcc\" for compatibility. This is the   name build scripts expect, so if we named it anything else then nothing   would work!</li> <li>NVIDIA's proprietary CUDA compiler, <code>nvcc</code>.</li> </ul> <p>SCALE provides a thing called nvcc, which is in fact absolutely nothing to do with NVIDIA's <code>nvcc</code>. Our \"nvcc\" is built on top of the open-source clang/llvm compiler, and has no dependency on NVIDIA's compiler.</p> <p>SCALE does not make use of \"nvvm\", either.</p>"},{"location":"manual/faq/#isnt-cuda-inherently-optimised-for-nvidia-hardware","title":"Isn't CUDA inherently optimised for NVIDIA hardware?","text":"<p>Not really.</p> <p>CUDA is already a cross-platform language/toolkit. NVIDIA have been making CUDA-capable GPUs for more than 20 years, with many drastic changes in the hardware during that period. You can nevertheless compile CUDA programs and run them on NVIDIA cards spanning many years and multiple hardware architecture redesigns, because CUDA is a cross-platform toolkit.</p> <p>The only problem is that, of course, NVIDIA didn't extend this cross-platform compatibility to other vendors.</p> <p>Claiming \"CUDA is optimised for NVIDIA GPUs\" is a bit like saying \"C++ is optimised for Intel CPUs\". If compilers can be taught to build the same C++ program for Intel/ARM/AMD CPUs with good performance, why can't we do that for CUDA? It's a different and challenging compiler research problem, but it is possible.</p>"},{"location":"manual/faq/#does-it-work-on-windows","title":"Does it work on Windows?","text":"<p>Not yet, but windows support is a challenge that we have ideas about.</p>"},{"location":"manual/how-to-install/","title":"Install SCALE","text":"<p>Select your operating system and version below to see installation instructions.</p> UbuntuRocky LinuxOther Distros 22.0424.04 <p>Set up the SCALE repository, if you haven't already:</p> <pre><code># Add the scale deb repos.\nwget https://pkgs.scale-lang.com/deb/dists/jammy/main/binary-all/scale-repos.deb\n\nsudo apt install ./scale-repos.deb\n\n# Update package list\nsudo apt update\n</code></pre> <p>Then, install the SCALE package:</p> <pre><code>sudo apt install scale\n\n# Add your user to the `video` group:\nsudo usermod -a -G video $(whoami)\n</code></pre> <p>If you did not already have the <code>amdgpu-dkms</code> kernel driver installed prior to installing SCALE, you should now reboot.</p> <p>Set up the SCALE repository, if you haven't already:</p> <pre><code># Add the scale deb repos.\nwget https://pkgs.scale-lang.com/deb/dists/noble/main/binary-all/scale-repos.deb\n\nsudo apt install ./scale-repos.deb\n\n# Update package list\nsudo apt update\n</code></pre> <p>Then, install the SCALE package:</p> <pre><code>sudo apt install scale\n\n# Add your user to the `video` group:\nsudo usermod -a -G video $(whoami)\n</code></pre> <p>If you did not already have the <code>amdgpu-dkms</code> kernel driver installed prior to installing SCALE, you should now reboot.</p> 9 <p>Set up the SCALE repository, if you haven't already:</p> <pre><code># Add the scale rpm repos.\nsudo dnf install https://pkgs.scale-lang.com/rpm/el9/main/scale-repos.rpm\n</code></pre> <p>Then, install the SCALE package:</p> <pre><code># Install SCALE\nsudo dnf install scale\n</code></pre> <p>If you did not already have the <code>amdgpu-dkms</code> kernel driver installed prior to installing SCALE, you should now reboot.</p> <p>Download and extract the SCALE tarball:</p> <pre><code># Download the tarball\nwget https://pkgs.scale-lang.com/tar/scale-latest-amd64.tar.xz\n\n# Extract it to the current directory\ntar xf scale-latest-amd64.tar.xz\n</code></pre> <p>The tarball is significantly larger than other options since it includes many dependent libraries directly instead of asking the system     package manager to install them.</p>"},{"location":"manual/how-to-install/#troubleshooting","title":"Troubleshooting","text":"<p>These issues relate to installation specifically. For more general troubleshooting steps, see here.</p>"},{"location":"manual/how-to-install/#im-not-able-to-set-up-the-scale-repository","title":"I'm not able to set up the SCALE repository","text":"<ul> <li> <p>To follow the installation instructions above, you will need to install <code>wget</code> and <code>tar</code>.     This is installed by default on many systems, and usually available in your system package manager otherwise.</p> </li> <li> <p>If you're still unable to download the repository setup package (<code>scale-repos.deb</code> / <code>scale-repos.rpm</code>), check your internet connection.</p> </li> <li>If you're unable to install the repository setup package, you may have manually added the repository previously. If so, these files can be safely overwritten when prompted by your package manager.</li> </ul>"},{"location":"manual/how-to-install/#i-get-an-error-related-to-amdgpu-dkms","title":"I get an error related to <code>amdgpu-dkms</code>","text":"<p>AMD's kernel modules are only supported on certain kernels. If your system uses a very out of date kernel, you may need to upgrade it in order to build it correctly.</p>"},{"location":"manual/how-to-install/#i-get-file-conflicts-when-installing-the-scale-package","title":"I get file conflicts when installing the SCALE package","text":"<p>This is usually caused by a previous manual installation of SCALE, or a different version of SCALE installed on the same system. Currently, only one version of SCALE and its associated ROCM version can be installed at once.</p>"},{"location":"manual/how-to-install/#i-previously-had-your-repositories-set-up-but-it-broke-mysteriously","title":"I previously had your repositories set up, but it broke mysteriously.","text":"<p>We recently merged our stable and unstable repos. This shouldn't require any action for our users, but if you're having troubles then we suggest completely removing our repositories and adding them again:</p> <pre><code># On Rocky\nsudo dnf remove 'scale-repos*'\nsudo rm -f /etc/yum.repos.d/scale.repo\n\n# On Ubuntu\nsudo apt-get remove 'scale-repos*'\nsudo rm -f /etc/apt/sources.list.d/scale.list /etc/apt/auth.conf.d/scale.conf\n\n# Then follow the instructions above again.\n</code></pre>"},{"location":"manual/how-to-use/","title":"Compile CUDA with SCALE","text":"<p>This guide covers the steps required to compile an existing CUDA project for an AMD GPU using SCALE.</p> <p>SCALE makes this as easy as possible by convincingly impersonating the  NVIDIA CUDA Toolkit (from the point of view of your build system).</p> <p>To use SCALE, we must simply cause your build system to use the \"CUDA  installation\" offered by SCALE.</p> <p>Install SCALE, if you haven't already.</p>"},{"location":"manual/how-to-use/#identifying-gpu-target","title":"Identifying GPU Target","text":"<p>If you don't already know which AMD GPU target you need to compile for, you can use the <code>scaleinfo</code> command provided by SCALE to find out:</p> <pre><code>scaleinfo | grep gfx\n</code></pre> <p>Example output:</p> <pre><code>Device 0 (00:23:00.0): AMD Radeon Pro W6800 - gfx1030 (AMD) &lt;amdgcn-amd-amdhsa--gfx1030&gt;\n</code></pre> <p>In this example, the GPU target ID is <code>gfx1030</code>.</p> <p>If your GPU is not listed in the output of this command, it is not currently supported by SCALE.</p> <p>If the <code>scaleinfo</code> command is not found, ensure that <code>&lt;SCALE install path&gt;/bin</code> is in <code>PATH</code>.</p>"},{"location":"manual/how-to-use/#the-easy-way-scaleenv","title":"The easy way: <code>scaleenv</code>","text":"<p>SCALE offers a \"<code>venv</code>-flavoured\" environment management script to allow  \"magically\" building CUDA projects.</p> <p>The concept is simple:</p> <ol> <li>Activate the <code>scaleenv</code> for the AMD GPU target you want to build for.</li> <li>Run the commands you normally use to build the project for an NVIDIA GPU.</li> <li>AMD binaries are sneakily produced instead of NVIDIA ones.</li> </ol> <p>To activate a scaleenv:</p> <pre><code>source /opt/scale/bin/scaleenv gfx1030\n</code></pre> <p>You can exit a <code>scaleenv</code> by typing <code>deactivate</code> or closing your terminal.</p> <p>While the environment is active: simply run the usual <code>cmake</code>/<code>make</code>/etc. commands needed to build the project, and it will build for whatever AMD  target you handed to <code>scaleenv</code>.</p>"},{"location":"manual/how-to-use/#how-it-really-works","title":"How it really works","text":"<p>To allow compilation without build system changes, SCALE provides a series of directories that are recognised by build systems as being CUDA Toolkit installations. One such directory is provided for each supported AMD GPU target. These directories can be found at <code>&lt;SCALE install  path&gt;/targets/gfxXXXX</code>, where <code>gfxXXXX</code> is the name of an AMD GPU target, such as <code>gfx1030</code>.</p> <p>To achieve the desired effect, we need the build system to use the \"CUDA  toolkit\" corresponding to the desired AMD GPU target.</p> <p>For example: to build for <code>gfx1030</code> you would tell your build system that CUDA is installed at <code>&lt;SCALE install path&gt;/targets/gfx1030</code>.</p> <p>All <code>scaleenv</code> is actually doing is setting various environment variables up  to make this happen. It's just a shell script: open it to see the variables  it is manipulating.</p>"},{"location":"manual/how-to-use/#finding-the-libraries-at-runtime","title":"Finding the libraries at runtime","text":"<p>For maximum compatibility with projects that depend on NVIDIA's \"compute  capability\" numbering scheme, SCALE provides one \"cuda mimic directory\" per  supported GPU target that maps the new target to \"sm_86\" in NVIDIA's  numbering scheme.</p> <p>This means that each of the <code>target</code> subdirectories contains  identically-named libraries, so SCALE cannot meaningfully add them to the  system's library search path when it is installed. The built executable/library therefore needs to be told how to find the libraries via another mechanism,  such as:</p> <ul> <li>rpath. With CMake, the simplest    thing that \"usually just works\" is to add    <code>-DCMAKE_INSTALL_RPATH_USE_LINK_PATH=ON</code> to your cmake incantation.</li> <li>Set <code>LD_LIBRARY_PATH</code> to include <code>${SCALE_DIR}/lib</code> at runtime. <code>scaleenv</code>    does this, so if you keep that enabled when running your programs things    will just work.</li> </ul> <p>Support for multiple GPU architectures in a single binary (\"Fat binaries\")  is in development.</p>"},{"location":"manual/how-to-use/#next-steps","title":"Next steps","text":"<ul> <li>Learn about CUDA dialects and SCALE language extensions</li> <li>Report a bug</li> </ul>"},{"location":"manual/inline-ptx/","title":"Inline PTX support","text":"<p>SCALE accepts inline PTX <code>asm</code> blocks in CUDA programs and will attempt to  compile it for AMD along with the rest of your program.</p>"},{"location":"manual/inline-ptx/#wave64-considerations","title":"Wave64 considerations","text":"<p>A small number of PTX instructions depend on the warp size of the GPU being  used. Since all NVIDIA GPUs and many AMD ones have a warp size of 32, much  code implicitly relies on this. As a result, issues can appear when  targeting wave64 devices.</p> <p>SCALE provides several tools and compiler warnings to help you write  portable PTX code. In most cases only small tweaks are required to get things working. Since so little PTX actually depends on the warp size, most  projects are unaffected by the issues documented in this section.  Nevertheless, it is useful to adjust your code to be warp-size-agnostic,  since doing so can be done with no downsides.</p>"},{"location":"manual/inline-ptx/#querying-warp-size","title":"Querying warp size","text":"<p>PTX defines the <code>WARP_SZ</code> global constant which can be used to access the warp size directly. It's a compile-time constant in nvidia's implementation  as well as in SCALE, so there is no cost to using this and doing arithmetic  with it (like with <code>warpSize</code> in CUDA code).</p>"},{"location":"manual/inline-ptx/#lanemask-inputs","title":"Lanemask inputs","text":"<p>The length of lanemask operands on instructions will always have a number of  bits equal to the warp size on the target GPU. For  example, when compiling for a wave64 GPU, the lanemask argument to <code>shfl.sync</code> is a <code>b64</code>, not <code>b32</code>.</p> <p>The following rules are applied to help detect problems with such operands:</p> <ul> <li>If a non-constant lanemask operand is used, and its bit-length is &lt;= the    warp size, an error is raised.</li> <li>If a constant lanemask operand is used with no 1-bits in the high 32 bits,    while compiling for a wave64 architecture, a warning is raised (which can    be disabled). This catches the common case of hardcoded lanemasks like    <code>0xFFFFFFFF</code> which will typecheck as <code>b64</code>, but will probably not do what    you want.</li> </ul> <p>In the common case where you want an all-ones lanemask, the most convenient  thing to do is write <code>-1</code> instead of <code>0xFFFFFFFF</code>: this will give you the  correct number of 1-bits in all cases, including on nvidia platforms.</p>"},{"location":"manual/inline-ptx/#the-c-argument-to-shfl-instructions","title":"The <code>c</code> argument to <code>shfl</code> instructions","text":"<p>The <code>shfl</code> PTX instruction has a funky operand, <code>c</code>, used for clamping etc. See the documentation.</p> <p>The <code>c</code> operand is really two operands packed together: <code>cval</code> in bits 0-4, and <code>segmask</code> in bits 8-12. For wave64, an extra bit is needed. Since there is space for an extra bit in each of these values, we simply add it in the obvious place.</p> <p>A portable way of reasoning about this is to assume that <code>cval</code> is in bits 0-7 and <code>segmask</code> in bits 8-15.</p> <p>Here's a concrete example of a reverse cumsum that works on either warp size:</p> <pre><code>__global__ void shuffleRevCumsumKernel(float *dst)\n{\n    float out;\n    const int C = warpSize - 1;\n    asm(\n    \".reg .f32 Rx;\"\n    \".reg .f32 Ry;\"\n    \".reg .pred p;\"\n    \"mov.b32 Rx, %1;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x1,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x2,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x4,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x8,  %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x10, %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n\n    // One extra shuffle is needed for the larger warp size.\n    #if __SCALE_WARP_SIZE__ &gt; 32\n    \"shfl.sync.down.b32  Ry|p, Rx, 0x20, %2, -1;\"\n    \"@p  add.f32        Rx, Ry, Rx;\"\n    #endif // __SCALE_WARP_SIZE__\n    \"mov.b32 %0, Rx;\"\n    : \"=f\"(out) : \"f\"(1.0f), \"n\"(C)\n    );\n\n    dst[threadIdx.x] = out;\n}\n</code></pre> <p>And here's how to do a portable butterfly shuffle reduction:</p> <pre><code>__global__ void shuffleBflyKernel(float *dst)\n{\n    const int C = warpSize - 1;\n\n    float out;\n    asm(\n    \".reg .f32 Rx;\"\n    \".reg .f32 Ry;\"\n    \".reg .pred p;\"\n    \"mov.b32 Rx, %1;\"\n    #if __SCALE_WARP_SIZE__ &gt; 32\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x20, %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    #endif // __SCALE_WARP_SIZE__\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x10, %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x8,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x4,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x2,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"shfl.sync.bfly.b32  Ry, Rx, 0x1,  %2, -1;\"\n    \"add.f32        Rx, Ry, Rx;\"\n    \"mov.b32 %0, Rx;\"\n    : \"=f\"(out) : \"f\"((float) threadIdx.x), \"n\"(C)\n    );\n\n    dst[threadIdx.x] = out;\n}\n</code></pre>"},{"location":"manual/inline-ptx/#dialect-differences","title":"Dialect differences","text":"<p>The SCALE compiler accepts a more permissive dialect of PTX than NVIDIA's  implementation does. </p>"},{"location":"manual/inline-ptx/#integer-lengths","title":"Integer lengths","text":"<p>Most PTX instructions are defined to work only for a specific, arbitrary set  of integer types. We didn't bother to implement such restrictions except in  cases where they are needed for correctness, so many PTX instructions accept  a wider selection of types than nvcc accepts.</p> <p>One amusing consequence of this is that most of the simple instructions work  for any bit-length: <code>add.s17</code> is allowed (but will of course lead to  extra sext/trunc instructions, so isn't necessarily a good idea).</p>"},{"location":"manual/inline-ptx/#divergent-exit","title":"Divergent <code>exit</code>","text":"<p>AMD hardware does not seem to have a mechanism by which individual threads  can terminate early (only entire warps). As a result, the <code>exit</code>  instruction may be used only in converged contexts. We transform it into approximately:</p> <pre><code>if (__activemask() == -1) {\n    exit_entire_warp();\n} else {\n    // This situation is unrepresentable\n    __trap();\n}\n</code></pre> <p>Code that uses <code>exit</code> as a performance optimisation for nvidia hardware may  benefit from being adjusted for AMD.</p>"},{"location":"manual/inline-ptx/#empty-asm-volatile-blocks","title":"Empty <code>asm volatile</code> blocks","text":"<p>To cater to \"interesting\" user code, the SCALE compiler will not touch <code>asm volatile</code> blocks containing no instructions. We've seen real-world CUDA code that uses these as a kind of ad-hoc optimisation barrier to prevent the compiler breaking programs that contain undefined behaviour. This pragmatic choice should reduce how often such broken programs fail to function, but such code is broken by definition.</p> <p>Note that the <code>volatile</code> on non-empty <code>volatile asm</code> blocks has no effect on the behaviour of the SCALE compiler. <code>volatile</code> asm is a conservative feature that allows the compiler to model \"unknowable\" implicit dependencies of the actions takenby the inline asm. Since we're compiling the asm to IR, the actual dependencies and properties of everything it does are known and modelled. This can improve optimisation, but may break programs that have undefined behaviour that was being hidden by the optimisation barrier effect of the volatile asm  block.</p>"},{"location":"manual/inline-ptx/#returning-the-carry-bit","title":"Returning the carry-bit","text":"<p>The PTX carry-bit may not be implicitly returned from functions.</p> <p>Some PTX instructions have a carry flag bit, used to perform extended-precision integer arithmetic across multiple instructions. The PTX manual notes:</p> <p>The condition code register is not preserved across calls and is mainly intended for use in straight-line code sequences for computing extended-precision integer addition, subtraction, and multiplication.</p> <p>It is, therefore, undefined behaviour to write code that writes to the  carry-bit in one function and then attempts to read it from another. The  correct execution of such code is optimiser-dependent on NVIDIA's platform  (depending on the function to inline), so will likely fail in <code>-O0</code> builds. </p> <p>For example:</p> <pre><code>void createCarryBit(int x, int y) {\n    // Add with carry-out.\n    asm(\"add.cc.u32 %0, %0, %1\": \"+r\"(x) : \"r\"(y));\n}\n\nvoid useCarryBit(int x, int y) {\n    createCarryBit(x, y);\n\n    // Add with carry-in. Not allowed, since it's trying to read the\n    // carry-bit across a function boundary.\n    int z;\n    asm(\"add.cc.u32 %0, %1, %2\" : \"=r\"(z) : \"r\"(x), \"r\"(y));\n}\n</code></pre> <p>Due to how SCALE's PTX support works, it can't support this pattern, so this situation is a compiler error instead. To write code like this in a portable  way, you can refactor it to use macros instead.</p> <p>It may also be worth considering if you truly still need this inline asm: both NVIDIA nvcc and SCALE have good support for <code>int128_t</code> now, which makes many  common uses of these asm constructs redundant.</p>"},{"location":"manual/inline-ptx/#asm-inputoutput-types","title":"<code>asm</code> input/output types","text":"<p><code>nvcc</code> doesn't appear to consistently follow its own tying rules for PTX asm  inputs/outputs. It allows the following invalid things to occur in some cases  (and real programs depend on this):</p> <ul> <li>Writes to read-only asm bindings are permitted (such as writing to an \"r\")    constraint. The result of the write is not visible to the caller: it's    effectively a temporary inside the scope of the asm block.</li> <li><code>=r</code> (write-only) constraints can be used in a read-write fashion (as if    they were <code>+r</code>).</li> <li>Values passed to the asm block are sometimes, but not always, type checked,   implicitly widened, or implicitly truncated.</li> </ul> <p>To avoid having to characterise and define the perimeter of this buggy  behaviour, SCALE's implementation defines the following consistent rules  which are intended to maximise compatibility (and minimise \"weirdness\"):</p> <ul> <li>All read-only inputs may be written to. The results of these writes are    visible only within the scope of the asm block (as if they were local    variables being passed by value into a function).</li> <li>All write-only outputs are implicitly read-write. ie.: there is no    difference between <code>+r</code> and <code>=r</code>.</li> <li>The type of an input or output binding is governed by the type of the    expression, not the constraint letter. Once \"in PTX\", the usual PTX rules    about implicit truncation/widening/etc. apply. This nuance won't change    the behaviour of programs unless they rely on using a \"too short\" PTX    constraint type to truncate a value, and then implicitly widen it within    PTX (hence zeroing out some of the bits). Since such truncations are    inconsistently applied even with nvidia nvcc mode, they are probably best    achieved with an explicit cast.</li> </ul>"},{"location":"manual/inline-ptx/#performance-considerations","title":"Performance considerations","text":"<p>In most cases, there isn't a performance penalty from using PTX asm in CUDA  code: it will usually convert to the same IR as the C++ you could have  written instead, and may actually be faster due to not needing to be as conservative about optimisation compared to the usual rules of asm blocks.</p> <p>Since the compiler effectively converts it to the CUDA code you could have  written to achieve the same effect without the use of the PTX asm, it  doesn't come with the optimisation-hindering downsides asm blocks  normally imply. The compiler will respect the ordering/synchronisation/etc.  requirements of each operation individually, rather than having to regard an  entire <code>asm volatile</code> block as an opaque, immutable unit.</p> <p>Programs that have already added support for HIP might have multiple  codepaths: one for CUDA that uses inline PTX, and one for AMD which doesn't.  In such cases, it is worth testing both to see which is fastest.</p>"},{"location":"manual/inline-ptx/#supported-constraints","title":"Supported constraints","text":"<p>The following PTX constraint letters are supported. See above commentary on  nuances regarding how they are interpreted.</p> <p><code>h</code>: u16 <code>r</code>: u32 <code>l</code>: u64 <code>f</code>: f32 <code>d</code>: f64 <code>n</code>: constants <code>C</code>: dynamic asm strings</p>"},{"location":"manual/inline-ptx/#supported-instructions","title":"Supported instructions","text":"<p>The following instructions are currently supported.</p> <p>Caveat: since the <code>bf16</code>, <code>fp8</code> and <code>tf32</code> floating point formats are not  currently supported in SCALE, they are also not supported here.</p> Instruction Notes abs activemask add addc and atom bfe bfi bfind bfind.shiftamt bmsk bra brev brkpt Currently a no-op clz cnot copysign cos.approx cvt cvt.pack discard Currently a no-op div dp2a dp4a elect ex2.approx exit Only from convergent code fence Memory ranges unsupported fma fns griddepcontrol.launch_dependents Currently a no-op griddepcontrol.wait Currently a no-op isspacep ld ld.nc ldmatrix ldu lg2.approx lop3 mad mad24 madc match.all match.any max max.xorsign.abs membar min min.xorsign.abs mma <code>wmma.mma</code> likely faster mov mul mul24 nanosleep neg not or pmevent Currently a no-op popc prefetch prefetchu prmt prmt.b4e prmt.ecl prmt.ecr prmt.f4e prmt.rc16 prmt.rc8 rcp red redux rem sad sin.approx selp set setp shf.l shfl.bfly shfl.down shfl.idx shfl.up shf.r shl shr slct st stmatrix sub subc szext testp.finite testp.infinite testp.normal testp.notanumber testp.number testp.subnormal trap vabsdiff vadd vmax vmin vote.all vote.any vote.ballot vote.uni vshl vshr vsub wmma.load wmma.store wmma.mma xor"},{"location":"manual/language-extensions/","title":"Language Extensions","text":"<p>SCALE has various opt-in language extensions that aim to improve the experience of writing GPU code. More language extensions are in development.</p> <p>Unless otherwise noted, SCALE accepts these language extensions in both <code>clang</code> and <code>nvcc</code> modes. Note that there are dialect differences between the two modes.</p> <p>Since NVIDIA's compiler does not support SCALE language extensions, if you want to retain the ability to compile for NVIDIA GPUs you must do one of two things:</p> <ul> <li>Guard use of language extensions behind the <code>__REDSCALE__</code> macro, hiding   it from NVIDIA's <code>nvcc</code>.</li> <li>Use SCALE's <code>clang</code> compiler to compile for both NVIDIA and AMD targets.   This will require changes to your build system.</li> </ul>"},{"location":"manual/language-extensions/#clangloop_unroll","title":"<code>[[clang::loop_unroll]]</code>","text":"<p>GPU code frequently contains loops that need to be partially unrolled, and which have the property that the degree of unrolling is a tradeoff between ILP and register usage.</p> <p>Finding the optimal amount to unroll is not usually possible in the compiler because the number of threads to be used is a runtime value. Programmers therefore usually want to set unroll depth by hand.</p> <p>The existing <code>#pragma unroll N</code> allows this to be set at the preprocessor level. The new <code>[[clang::loop_unroll N]]</code> allows doing this in a template-dependent way:</p> <pre><code>template&lt;int UnrollAmount&gt;\n__device__ void example(int n) {\n    [[clang::loop_unroll UnrollAmount]]\n    for (int i = 0; i &lt; n; i++) {\n        // ...\n    }\n}\n</code></pre>"},{"location":"manual/language-extensions/#clanggetter","title":"<code>[[clang::getter]]</code>","text":"<p>Introduced almost entirely to implement <code>threadIdx</code> and friends, this language extension is available in all modes of the SCALE compiler.</p> <p>Given:</p> <pre><code>[[clang::getter(someFunction)]] int example;\n</code></pre> <p>All references to <code>example</code> will be replaced by calls to <code>someFunction()</code>. Taking the address of a variable so declared is undefined behaviour.</p>"},{"location":"manual/language-extensions/#__builtin_provablebool-x","title":"<code>__builtin_provable(bool X)</code>","text":"<p><code>__builtin_provable(X)</code> accepts a boolean, <code>X</code>, and:</p> <ul> <li>If the compiler is able to prove, during optimisation, that <code>X</code> is a   compile-time constant true, the entire expression evaluates to a   compile-time constant true.</li> <li>Otherwise (if <code>X</code> is unknown, or provably false), the entire expression   evaluates to a compile-time constant false.</li> </ul> <p>This allows you to write code that opportunistically optimises for a special case, without the risk of runtime branching overhead or the inconvenience of propagating this information through your entire program using templates. For example:</p> <pre><code>__device__ int myCleverFunction(int input) {\n    if (__builtin_provable(input % 2 == 0)) {\n        // Special fast code for the case where `input` is divisible\n        // by 2 goes here.\n    } else {\n        // Slow, general case goes here.\n    }\n}\n</code></pre> <p>During optimisation, as calls to <code>myCleverFunction</code> get inlined, the compiler may be able to prove that <code>input % 2 == 0</code> for specific calls to this function. Those cases will be compiled with the \"fast path\", while all others will be compiled to the \"slow path\". The <code>if</code> statement will never compile to an actual conditional.</p> <p>Since there are no guarantees that the optimiser is able to prove the condition, the program must produce identical outputs from either path, or the behaviour is undefined.</p> <p>This feature differs from the standard c++17 <code>if constexpr</code> in that it is not required that the input boolean be <code>constexpr</code>. <code>__builtin_provable()</code> communicates with the optimiser, not the template system. Consequently:</p> <ul> <li>You don't need to use templates to propagate \"optimisation knowledge\"   throughout the program.</li> <li>Compilation may be faster, as a result of not having to template everything.</li> <li>Some cases may be missed where optimisation fails. Such cases are probably   independently worth investigating (Why did optimisation fail? That's a   source of additional slowness).</li> </ul>"},{"location":"manual/language-extensions/#improved-support-for-non-32-warpsize","title":"Improved support for non-32 warpSize","text":"<p>Not all AMD GPUs have a warp size of 32. To mitigate this, we offer a variety of compiler and API features:</p> <ul> <li><code>cudaLaneMask_t</code>: A type that is an integer with the number of bits as a CUDA   warp. This should be used when using functions such as <code>__ballot()</code> to avoid   discarding half the bits.</li> <li>Use of <code>cudaLaneMask_t</code> in appropriate places in the CUDA APIs (such as   the return value of <code>__ballot()</code>)</li> <li>Diagnostics to catch implicit casts from <code>cudaLaneMask_t</code> to narrower types.</li> </ul> <p>In practice, this means the compiler detects the majority of cases where code is written in a way that will break on a device with a warp size of 64.</p> <p>Programmers should modify their CUDA code to be agnostic to warp size. NVIDIA's documentation recommends this practice, but a lot of real-world CUDA code does it incorrectly because no current NVIDIA hardware has a warp size other than 32.</p> <p>Since NVIDIA's <code>nvcc</code> does not have <code>cudaLaneMask_t</code>, programmers should use <code>auto</code> to declare the return types of functions such as <code>__ballot()</code> that return it. This will compile correctly on all platforms.</p>"},{"location":"manual/optimisation-flags/","title":"Compiler Optimisation Flags","text":"<p>When using the <code>nvcc</code> frontend to SCALE, it matches the behaviour of NVIDIA's compiler as closely as possible.</p> <p>This means disabling some optimisations that are enabled by default by clang, since those break certain programs which rely on the behaviour of NVIDIA's compiler. In some cases, such reliance represents undefined behaviour in the affected program.</p> <p>This page documents some of these differences, and how to opt-in to these optimisations on a case-by-case basis. In general, all <code>clang++</code> flags are also accepted by SCALE in <code>nvcc</code> mode.</p> <p>You may be able to simply switch some of these features on and immediately gain performance.</p> <p>tl;dr: try <code>-ffast-math -fstrict-aliasing</code> and see if your program explodes.</p>"},{"location":"manual/optimisation-flags/#floating-point-optimisation","title":"Floating Point Optimisation","text":"<p>NVIDIA's compiler provides a <code>-use_fast_math</code> flag that relaxes some floating point rules to improve optimisation. It's documented to do exactly these things:</p> <ul> <li>Use some less precise math functions (eg. <code>__sinf()</code> instead of <code>sinf()</code>).</li> <li>Enables less precise sqrt/division</li> <li>Flushes denorms to zero.</li> <li>Fuse multiply-adds into FMA operations.</li> </ul> <p>SCALE mirrors this behaviour when you use this flag in our <code>nvcc</code> emulation mode, aiming to produce the same results as NVIDIA's compiler.</p> <p>SCALE also provides all of <code>clang</code>'s flags, allowing access to its more aggressive floating point optimisations, such as:</p> <ul> <li>Assume infinities and NaNs never happen</li> <li>Allow algebraic rearrangement of floating point calculations</li> </ul> <p>Full details about these flags are available in the clang user manal.</p> <p>These optimisations can be controlled per-line using the fp control pragmas.</p> <p>This allows you to either:</p> <ul> <li>Specify the compiler flag (to enable an optimisation by default) and then   switch it off for special code regions (ie. opt-out mode).</li> <li>Opt-in to the optimisation in regions of code where you know it to be safe.</li> </ul> <p>These flags will affect the performance of functions in the SCALE implementation of the CUDA Math API.</p> <p>These flags do not affect the accuracy of the results of the Math API, but do apply assumptions about the range of possible inputs. For example: if you enable \"assume no infinities\", all infinity-handling logic will be removed from the Math API functions, making them slightly more efficient. Only the inf, nan, and ignore-signed-zero flags will affect the output of the math functions. Flags like reassoc will not degrade the accuracy of these routines.</p> <p>Each call to a math function will be optimised separately, using the set of fp optimisation flags in effect at that point in that file. You can use pragmas to mix different optimisation flags at different points within the same file. It is OK to compile different source files with different fp optimisation flags and then link them together.</p>"},{"location":"manual/optimisation-flags/#strict-aliasing","title":"Strict aliasing","text":"<p>By default, in C++, the compiler assumes that pointers to unrelated types (eg <code>float</code> and <code>int</code>) never point to the same place. This can significantly improve optimisation by improving instruction reordering and ILP.</p> <p>Unfortunately, NVIDIA's <code>nvcc</code> does not do strict-aliasing optimisations, and enabling it breaks some CUDA programs. SCALE-nvcc therefore disables this by default.</p> <p>You can explicitly enable this class of optimisations in SCALE by adding <code>-fstrict-aliasing</code>. This may break your program if it contains TBAA violations. We recommend you find and fix such violations, since they are undefined behaviour. This would mean your code is only working correctly because NVIDIA's compiler doesn't currently exploit this type of optimisation: something which may change in the future!</p>"},{"location":"manual/runtime-extensions/","title":"API Extensions","text":"<p>SCALE has some runtime/library features not found in NVIDIA's CUDA Toolkit.</p>"},{"location":"manual/runtime-extensions/#environment-variables","title":"Environment variables","text":"<p>Some extra features can be enabled by environment variables.</p>"},{"location":"manual/runtime-extensions/#scale_exceptions","title":"<code>SCALE_EXCEPTIONS</code>","text":"<p>Errors from the CUDA API can be hard to debug, since they simply return an error code that the host program has to do something with.</p> <p>SCALE provides an environment variable to make any error from the CUDA API produce a observable result.</p> <p>Setting <code>SCALE_EXCEPTIONS=1</code> will cause all CUDA APIs to throw descriptive  exceptions instead of returning C-style error codes.</p> <p>Setting <code>SCALE_EXCEPTIONS=2</code> will print the error messages to stderr, but not throw them. This is helpful for programs that deliberately create CUDA errors as part of their processing.</p> <p>In cases where CUDA APIs are expected to return a value other than  <code>cudaSuccess</code> during normal operation (such as <code>cudaStreamQuery()</code>, an  exception will not be thrown except if an exceptional case arises.</p>"},{"location":"manual/runtime-extensions/#api-extensions_1","title":"API Extensions","text":"<p>Some of SCALE's API extensions require the <code>scale.h</code> header to be included. </p>"},{"location":"manual/runtime-extensions/#programmatic-exception-enablement","title":"Programmatic Exception Enablement","text":"<p>SCALE's exception mode may also be controlled programmatically:</p> <pre><code>scale::Exception::setMode(scale::ExceptionMode::THROW); // Throw exceptions\nscale::Exception::setMode(scale::ExceptionMode::PRINT); // Print errors\nscale::Exception::setMode(scale::ExceptionMode::OFF); // Match CUDA behaviour\n</code></pre> <p>Even when exceptions are disabled, you can access a <code>scale::Exception</code> object containing the descriptive error message from the most recent failure using <code>scale::Exception::last()</code>:</p> <pre><code>cudaError_t e = cudaSomething();\nif (e != cudaSuccess) {\n    const scale::Exception &amp;ex = scale::Exception::last();\n    std::cerr &lt;&lt; \"CUDA error: \" &lt;&lt; ex.what() &lt;&lt; '\\n';\n}\n</code></pre> <p>The error accessed by this API is the same one you'd get from using the CUDA API <code>cudaGetLastError()</code>, just more descriptive.</p>"},{"location":"manual/troubleshooting/","title":"Troubleshooting","text":"<p>This page provides tips for solving common problems encountered when trying to compile or run CUDA programs with SCALE.</p>"},{"location":"manual/troubleshooting/#crashes","title":"Crashes","text":"<p>Please report a bug.</p>"},{"location":"manual/troubleshooting/#no-such-function-cublascufftcusolversomethingsomething","title":"\"No such function: cuBlas/cuFFt/cuSolverSomethingSomething()\"","text":"<p>If your project needs a missing \"CUDA-X\" API (cuBLAS, cuFFT, cuSOLVER and friends), this is most likely something you can fix yourself by submitting a patch to the open-source library wrapper project. So long as an equivalent function is available in a ROCm library, the wrapper code is trivial.</p>"},{"location":"manual/troubleshooting/#cuda-api-errors","title":"CUDA API errors","text":"<p>The <code>SCALE_EXCEPTIONS</code> feature can be helpful for getting more information about many failures.</p>"},{"location":"manual/troubleshooting/#wave64-issues","title":"wave64 issues","text":"<p>All current NVIDIA GPUs have a warp size of 32, so many CUDA programs are written in a way that assumes this is always the case.</p> <p>Some AMD GPUs have a warp size of 64, which can cause problems for CUDA code written in this way.</p> <p>SCALE offers tools to address this problem:</p> <ul> <li>APIs that operate on warp masks accept and return a new type:   <code>cudaWarpSize_t</code>. This is an integer with as many bits as there are   threads in a warp on the target GPU.</li> <li>Some APIs (such as <code>__ffs()</code>) have extra overloads for <code>cudaWarpSize_t</code>, so   common patterns (such as <code>__ffs(__ballot(...))</code>) just work.</li> <li>The SCALE compiler will emit compiler warnings when values that represent   warp masks are implicitly truncated to 32 bits.</li> </ul> <p>To write code that works correctly on both platforms:</p> <ul> <li>Use <code>auto</code> instead of <code>uint32_t</code> when declaring a variable that is   intended to contain a warp mask. With NVIDIA <code>nvcc</code> this will map to   <code>uint32_t</code>, and with SCALE this will map to <code>cudaWarpSize_t</code>, producing   correct behaviour on both platforms.</li> <li>Avoid hardcoding the constant \"32\" to represent warp size, instead using   the global <code>warpSize</code> available on all platforms.</li> </ul>"},{"location":"manual/troubleshooting/#initialization-errors-or-no-devices-found","title":"Initialization errors or no devices found","text":"<p>The SCALE runtime can fail to initialise if:</p> <ul> <li>The AMD kernel module is out of date.</li> <li><code>/dev/kfd</code> is not writable by the user running the program.</li> <li>There are no supported GPUs attached.</li> </ul> <p>This situation produces error messages such as:</p> <pre><code>$ SCALE_EXCEPTIONS=1 ./myProgram\nterminate called after throwing an instance of 'redscale::SimpleException'\n  what():  cudaDeviceSynchronize: No usable CUDA devices found., CUDA error: \"no device\"\nAborted (core dumped)\n</code></pre> <pre><code>$ /opt/scale/bin/scaleinfo\nError getting device count: initialization error\n</code></pre> <pre><code>$ /opt/scale/bin/hsakmtsysinfo\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  HSAKMT Error 20: Could not open KFD\nAborted (core dumped)\n</code></pre>"},{"location":"manual/troubleshooting/#verify-you-have-a-supported-gpu","title":"Verify you have a supported gpu","text":"<p>Run <code>/opt/scale/bin/hsasysinfo | grep 'Name: gfx</code> to determine the architecture of your GPU, and determine if it is one of the supported architectures listed here.</p>"},{"location":"manual/troubleshooting/#ensure-devkfd-is-writable","title":"Ensure <code>/dev/kfd</code> is writable","text":"<p>Ensure your user is in the group that grants access to <code>/dev/kfd</code>. On Ubuntu, this is via membership of the <code>render</code> group: <code>sudo usermod -a -G render USERNAME</code>.</p> <p>You could temporarily make <code>/dev/kfd</code> world-writable via: <code>sudo chmod 666 /dev/kfd</code>.</p>"},{"location":"manual/troubleshooting/#cannot-find-shared-object","title":"Cannot find shared object","text":"<p>The correct library search path for a SCALE binary can be target dependent due to compute capability mapping. This can lead to runtime errors where the SCALE libraries cannot be found, such as:</p> <pre><code>error while loading shared libraries: libredscale.so: cannot open shared object file: No such file or directory\n</code></pre> <p>Two ways to solve this problem are:</p> <ul> <li>Set <code>LD_LIBRARY_PATH</code> to the SCALE target library directory, such as:    <code>LD_LIBRARY_PATH=/opt/scale/targets/gfx1030/lib:$LD_LIBRARY_PATH</code> for <code>gfx1030</code>.    <code>scaleenv</code> does this for you.</li> <li>Compile your program is compiled with that directory in RPATH:    rpath.</li> </ul>"},{"location":"manual/troubleshooting/#cannot-compile-using-the-nvrtc-api-or-reported-compute-capabilities-are-huge","title":"Cannot compile using the nvrtc API or reported compute capabilities are huge","text":"<p>Both of these problems are caused by using a <code>libredscale.so</code> that is not located in the correct place relative to its support files when running a program. In the case of the nvrtc API, it's because the compiler cannot be found. In the case of reported huge compute capabilities, it's because the compute capability map cannot be found.</p> <p>The solution is to make sure to use the <code>lib</code> subdirectories for one of the targets, rather than the <code>lib</code> directory of the SCALE installation directory. For example, <code>/opt/scale/targets/gfx1030/lib</code> rather than <code>/opt/scale/lib</code>. The <code>gfxany</code> target is suitable for using the nvrtc API, but it does not contain a compute capability map so it will not report small compute capabilities.</p> <p>As with being unable to find the shared object at all, this can be solved either by setting <code>LD_LIBRARY_PATH</code> or by setting the binary's rpath.</p>"},{"location":"manual/troubleshooting/#example-error","title":"Example error:","text":"<pre><code>$ SCALE_EXCEPTIONS=1 ./rtc\nterminate called after throwing an instance of 'redscale::RtcException'\n  what():  nvrtcCompileProgram: Could not find clang-nvcc or nvcc., CUDA error: \"JIT compiler not found\", NVRTC error: \"Compilation\"\nAborted (core dumped)\n</code></pre>"},{"location":"manual/troubleshooting/#nvcc-cannot-find-libdevice-for-sm_52-and-cannot-find-cuda-installation","title":"nvcc: cannot find libdevice for sm_52 and cannot find CUDA installation","text":"<p>If <code>targets/gfxany</code> rather than a specific target like <code>targets/gfx1030</code> is used, then there is no default GPU to target. This leads to an error like the example below. The solution is to either use a target-specific directory like <code>targets/gfx1030</code>, or to specify a specific target such as with <code>-arch gfx1030</code>.</p>"},{"location":"manual/troubleshooting/#example-error_1","title":"Example error","text":"<pre><code>nvcc: error: cannot find libdevice for sm_52; provide path to different CUDA installation via '--cuda-path', or pass '-nocudalib' to build without linking with libdevice\nnvcc: error: cannot find CUDA installation; provide its path via '--cuda-path', or pass '-nocudainc' to build without CUDA includes\n</code></pre>"},{"location":"manual/troubleshooting/#cannot-find-c-standard-library-include","title":"Cannot find C++ standard library include","text":"<p>Some distributions, such as Ubuntu, permit multiple versions of <code>gcc</code> and <code>g++</code> to be installed separately. It is possible to have a version of <code>gcc</code> installed without the corresponding version of <code>g++</code>. This can cause our compiler to be unable to find the C++ standard library headers.</p> <p>The solution is to ensure the corresponding version of <code>g++</code> is installed. For example: if the latest version of <code>gcc</code> you have installed is <code>gcc-12</code>, but you do not have <code>g++-12</code> installed, run: <code>sudo apt-get install g++-12</code>.</p>"},{"location":"manual/troubleshooting/#example-error_2","title":"Example error","text":"<pre><code>  In file included from &lt;built-in&gt;:1:\n\n  In file included from\n  /opt/scale/targets/gfx1100/include/redscale_impl/device.h:6:\n\n  In file included from\n  /opt/scale/targets/gfx1100/include/redscale_impl/common.h:40:\n\n  /opt/scale/targets/gfx1100/include/redscale_impl/../cuda.h:15:10: fatal\n  error: 'cstddef' file not found\n\n  #include &lt;cstddef&gt;\n\n           ^~~~~~~~~\n\n  1 error generated when compiling for gfx1100.\n</code></pre>"},{"location":"manual/troubleshooting/#cmake-error-running-link-command-no-such-file-or-directory","title":"CMake: Error running link command: no such file or directory","text":"<p>CMake tries to detect the linker to use based on the compiler. For SCALE's <code>nvcc</code>, it uses <code>clang++</code> as the linker. If this does not exist in your <code>PATH</code>, the result is an error like the one in the example below.</p> <p>A good solution is to make sure SCALE's <code>nvcc</code> is at the start of your <code>PATH</code>. This will place our <code>clang++</code> on your path too, avoiding the problem.</p> <pre><code># Adjust for the target you want to use.\nexport PATH=/opt/scale/targets/gfx1030/bin:$PATH\n</code></pre>"},{"location":"manual/troubleshooting/#example-error_3","title":"Example error","text":"<pre><code>-- The CUDA compiler identification is NVIDIA 12.5.999\n-- Detecting CUDA compiler ABI info\n-- Detecting CUDA compiler ABI info - failed\n-- Check for working CUDA compiler: /opt/scale/targets/gfx1030/bin/nvcc\n-- Check for working CUDA compiler: /opt/scale/targets/gfx1030/bin/nvcc - broken\nCMake Error at /usr/local/share/cmake-3.29/Modules/CMakeTestCUDACompiler.cmake:59 (message):\n  The CUDA compiler\n\n    \"/opt/scale/targets/gfx1030/bin/nvcc\"\n\n  is not able to compile a simple test program.\n\n  It fails with the following output:\n\n    Change Dir: '/home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV'\n\n    Run Build Command(s): /usr/local/bin/cmake -E env VERBOSE=1 /usr/bin/gmake -f Makefile cmTC_185e7/fast\n    /usr/bin/gmake  -f CMakeFiles/cmTC_185e7.dir/build.make CMakeFiles/cmTC_185e7.dir/build\n    gmake[1]: Entering directory '/home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV'\n    Building CUDA object CMakeFiles/cmTC_185e7.dir/main.cu.o\n    /opt/scale/targets/gfx1030/bin/nvcc -forward-unknown-to-host-compiler   \"--generate-code=arch=compute_86,code=[compute_86,sm_86]\" -MD -MT CMakeFiles/cmTC_185e7.dir/main.cu.o -MF CMakeFiles/cmTC_185e7.dir/main.cu.o.d -x cu -c /home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV/main.cu -o CMakeFiles/cmTC_185e7.dir/main.cu.o\n    Linking CUDA executable cmTC_185e7\n    /usr/local/bin/cmake -E cmake_link_script CMakeFiles/cmTC_185e7.dir/link.txt --verbose=1\n    clang++ @CMakeFiles/cmTC_185e7.dir/objects1.rsp -o cmTC_185e7 @CMakeFiles/cmTC_185e7.dir/linkLibs.rsp -L\"/opt/scale/targets/gfx1030/lib\"\n    Error running link command: no such file or directorygmake[1]: *** [CMakeFiles/cmTC_185e7.dir/build.make:102: cmTC_185e7] Error 2\n    gmake[1]: Leaving directory '/home/user/test/cmake/build/CMakeFiles/CMakeScratch/TryCompile-vLZLYV'\n    gmake: *** [Makefile:127: cmTC_185e7/fast] Error 2\n\n\n\n\n\n  CMake will not be able to correctly generate this project.\nCall Stack (most recent call first):\n  CMakeLists.txt:2 (project)\n\n\n-- Configuring incomplete, errors occurred!\n</code></pre>"},{"location":"manual/troubleshooting/#half-precision-intrinsics-not-defined-in-c","title":"Half precision intrinsics not defined in C++","text":"<p>If you're using <code>__half</code> in host code in a non-CUDA translation unit, you might get an error claiming the function you want does not exist:</p> <pre><code>error: \u2018__half2float\u2019 was not declared in this scope\n</code></pre> <p>This problem can be resolved by using newer C++ compiler.</p> <p>This issue is discussed in more detail in the Differences from NVIDIA CUDA section.</p>"}]}